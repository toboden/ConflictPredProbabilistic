{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google drive runtime connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ctrl + F9 alle Zellen ausfÃ¼hren\n",
    "\n",
    "!pip install PyDrive\n",
    "!pip install CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth) \n",
    "\n",
    "data_folder_id = '1RigGnEyyNGnO_SPBSc_RwO9jjdbnPTAV'\n",
    "result_folder_id = '1CNBTHtBOTFXh01WUpP2EF1aEmDi0rSyg'  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "## functions for the distribtion models\n",
    "# truncated negative binomial---------------------------------------------------\n",
    "def truncNegBin_CDF(y, n, p):\n",
    "    f_zero = nbinom.pmf(0, n, p)\n",
    "    if y > 0:\n",
    "        return (nbinom.cdf(y, n, p) - nbinom.cdf(0, n, p)) / (1 - f_zero)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def truncNegBin_PPF(x, n, p, epsilon=1e-6, max_iterations=100):\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if (1 - nbinom.pmf(0, n, p)) == 1:\n",
    "        return nbinom.ppf(x, n, p)\n",
    "    else:\n",
    "        # Define the range of y where the solution might exist\n",
    "        lower_bound = 0\n",
    "        upper_bound = 1000000000  # Adjust this based on the expected range of y\n",
    "\n",
    "        # Bisection method\n",
    "        for _ in range(max_iterations):\n",
    "            y = (lower_bound + upper_bound) / 2\n",
    "            cdf_value = truncNegBin_CDF(y, n, p)\n",
    "\n",
    "            if abs(cdf_value - x) < epsilon:\n",
    "                return np.ceil(y)  # Found a good approximation\n",
    "\n",
    "            if cdf_value < x:\n",
    "                lower_bound = y\n",
    "            else:\n",
    "                upper_bound = y\n",
    "\n",
    "        # Return the best approximation if max_iterations is reached\n",
    "        return np.ceil(y)\n",
    "\n",
    "def calculate_trunc_nbinom_quantile(quantile, n, p):\n",
    "    return truncNegBin_PPF(quantile, n, p)\n",
    "\n",
    "# truncated poisson---------------------------------------------------\n",
    "def truncPois_CDF(y, mu):\n",
    "    f_zero = poisson.pmf(0, mu)\n",
    "    if y > 0:\n",
    "        return (poisson.cdf(y, mu) - poisson.cdf(0, mu)) / (1 - f_zero)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def truncPois_PPF(x, mu, epsilon=1e-6, max_iterations=100):\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if (1 - poisson.pmf(0, mu)) == 1:\n",
    "        return poisson.ppf(x, mu)\n",
    "    else:\n",
    "        # Define the range of y where the solution might exist\n",
    "        lower_bound = 0\n",
    "        upper_bound = 1000000000  # Adjust this based on the expected range of y\n",
    "\n",
    "        # Bisection method\n",
    "        for _ in range(max_iterations):\n",
    "            y = (lower_bound + upper_bound) / 2\n",
    "            cdf_value = truncPois_CDF(y, mu)\n",
    "\n",
    "            if abs(cdf_value - x) < epsilon:\n",
    "                return np.ceil(y)  # Found a good approximation\n",
    "\n",
    "            if cdf_value < x:\n",
    "                lower_bound = y\n",
    "            else:\n",
    "                upper_bound = y\n",
    "\n",
    "        # Return the best approximation if max_iterations is reached\n",
    "        return np.ceil(y)\n",
    "\n",
    "def calculate_trunc_pois_quantile(quantile, mu):\n",
    "    return truncPois_PPF(quantile, mu)\n",
    "\n",
    "### function to compute distribution-------------------------------------------------\n",
    "def baseFatalModel_quantiles(featureSeries, quantiles, w=None, model='hurdle'):\n",
    "    # list to store quantiles \n",
    "    dummy_fatalities_list = []\n",
    "    # string to store model distribution\n",
    "    dist_string = ''\n",
    "\n",
    "    mean = None\n",
    "    var = None\n",
    "\n",
    "    numberQuantiles = len(quantiles)\n",
    "\n",
    "    # hurdle model\n",
    "    if model == 'hurdle':\n",
    "        if w == None:\n",
    "            \n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (featureSeries.value_counts().get(0, 0) / featureSeries.count())\n",
    "            # calculate n (r) and p via average/variance without the zero values\n",
    "            mean = pd.Series.mean(featureSeries[featureSeries != 0])\n",
    "            var = pd.Series.var(featureSeries[featureSeries != 0])\n",
    "\n",
    "        elif w <= 0:\n",
    "            return 'w has to be > 0'\n",
    "        \n",
    "        else:\n",
    "            features = featureSeries.tail(w).loc[:,'ged_sb']\n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (features.value_counts().get(0, 0) / features.count())\n",
    "            # calculate n (r) and p via average/variance without the zero values\n",
    "            mean = pd.Series.mean(features[features != 0])\n",
    "            var = pd.Series.var(features[features != 0])\n",
    "\n",
    "        # pd.Series.var or mean returns Nan in case of a passed series of length 1\n",
    "        if np.isnan(mean):\n",
    "            mean = 0\n",
    "        if np.isnan(var):\n",
    "            var = 0\n",
    "\n",
    "        # check if there are values above zero, otherwise no second component (trunc dist.) needed\n",
    "        if p_t > 0:\n",
    "            # component 1, y=0: Bernoulli\n",
    "            comp2_quantiles = [q for q in quantiles if q > p_t] #quantiles for the second component\n",
    "            removed_elements_length = numberQuantiles-len(comp2_quantiles)\n",
    "            zeros_array = np.zeros(removed_elements_length) #zero values that originate from the bernoulli dist\n",
    "\n",
    "            # component 2, y>0\n",
    "            if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                trunc_nbinom_quantiles = Parallel(n_jobs=-1)(delayed(calculate_trunc_nbinom_quantile)(quantile, n, p) for quantile in comp2_quantiles) #fast way\n",
    "                trunc_nbinom_quantiles = np.array(trunc_nbinom_quantiles)\n",
    "\n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_nbinom_quantiles)).tolist()\n",
    "                dist_string = 'BernoulliTruncNBinom'\n",
    "\n",
    "            elif mean == 0 and var == 0: # due to faster calculation\n",
    "                dummy_fatalities_list = [0] * numberQuantiles\n",
    "                dist_string = 'BernoulliTruncPois'\n",
    "\n",
    "            else:  # equivalent to all means and 0 < var <= mean\n",
    "                trunc_pois_quantiles = Parallel(n_jobs=-1)(delayed(calculate_trunc_pois_quantile)(quantile, mean) for quantile in comp2_quantiles) #fast way\n",
    "                trunc_pois_quantiles = np.array(trunc_pois_quantiles)\n",
    "                \n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_pois_quantiles)).tolist()\n",
    "                dist_string = 'BernoulliTruncPois'\n",
    "            \n",
    "        # p_t = 0 so no second component is needed    \n",
    "        else:\n",
    "            dummy_fatalities_list = [0] * numberQuantiles\n",
    "            dist_string = 'BernoulliHurdle'\n",
    "\n",
    "    # nbinom model\n",
    "    elif model == 'nbinom':\n",
    "        if w == None:\n",
    "             # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries)\n",
    "            var = pd.Series.var(featureSeries)\n",
    "        elif w <= 0:\n",
    "            return 'w has to be > 0'\n",
    "        else:\n",
    "            # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "        if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "        elif mean == 0 and var == 0: # due to faster calculation\n",
    "                dummy_fatalities_list = [0] * numberQuantiles\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "        else: # equivalent to all means and 0 < var <= mean\n",
    "                dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "    return {'fatalities': dummy_fatalities_list, 'dist': dist_string, 'mean': mean, 'var': var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_years\n",
    "feature_years\n",
    "estimModel\n",
    "s_prediction_list\n",
    "window_list\n",
    "country_list\n",
    "number_countries\n",
    "number_dataframes\n",
    "number_w\n",
    "baseline_estimate_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the average over all indiviual moving windows per w and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store all crps values\n",
    "baseline_crps_list_to_oct20 = [\n",
    "    {\n",
    "        'country_id': country,\n",
    "        'baseline': [\n",
    "            {'s': s, 'w': [], 'CRPS': []}\n",
    "            for s in s_prediction_list\n",
    "        ]\n",
    "    }\n",
    "    for country in country_list\n",
    "]\n",
    "baseline_crps_list_to_oct19 = copy.deepcopy(baseline_crps_list_to_oct20)\n",
    "baseline_crps_list_to_oct18 = copy.deepcopy(baseline_crps_list_to_oct20)\n",
    "baseline_crps_list_to_oct17 = copy.deepcopy(baseline_crps_list_to_oct20)\n",
    "\n",
    "# number of prediction windows\n",
    "number_s = len(s_prediction_list)\n",
    "\n",
    "# fill lists with crps calculations\n",
    "for s in tqdm(s_prediction_list):\n",
    "    sleep(3)\n",
    "    #print('                  prediction window ' + str(s-2) + '/' + str(number_s), end='\\r')\n",
    "\n",
    "    for index in range(number_countries):\n",
    "        country = country_list[index]\n",
    "        #print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "            \n",
    "        for i in range(number_w):\n",
    "            w = window_list[i]\n",
    "            dummy_crps_list = [] \n",
    "\n",
    "            # loop over all subset windows of the country and w \n",
    "            for j in range(len(baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'])):\n",
    "\n",
    "                distribution = baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'][j][0]['fatalities']\n",
    "                actual = baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'][j][1]['unreal_actuals'][s-3]\n",
    "\n",
    "                crps = pscore(np.array(distribution),actual).compute()[0]\n",
    "                dummy_crps_list.append(crps)\n",
    "\n",
    "            # dataframe to_oct17\n",
    "            baseline_crps_list_to_oct17[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct17[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[:-(3*12)]))\n",
    "\n",
    "            # dataframe to_oct18\n",
    "            baseline_crps_list_to_oct18[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct18[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[12:-(2*12)]))\n",
    "\n",
    "            # dataframe to_oct19\n",
    "            baseline_crps_list_to_oct19[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct19[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[(2*12):-12]))\n",
    "\n",
    "            # dataframe to_oct20\n",
    "            baseline_crps_list_to_oct20[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct20[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[(3*12):]))\n",
    "\n",
    "task2_baseline_list = [baseline_crps_list_to_oct17, baseline_crps_list_to_oct18,\n",
    "                       baseline_crps_list_to_oct19, baseline_crps_list_to_oct20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimization\n",
    "'w_minimization_list' contains the minimal w's for the different baselines for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the results of the minimal w's\n",
    "w_minimization_list = [{'predictionYear':year, 'minWData':[]} for year in actual_years]\n",
    "\n",
    "# list to store the list to compute the minimal w's\n",
    "w_compute_list = [{'predictionYear':year, 'data':[]} for year in actual_years]\n",
    "\n",
    "# loop over the four different datasets to predict (18-21)\n",
    "for task2_index in range(len(task2_baseline_list)):\n",
    "    v1_baseline_crps_dict = {'w':[],'CRPS':[]}\n",
    "    v2_baseline_crps_list = [{'country_id': country, 'baseline': {'w':[],'CRPS':[]}} for country in country_list]\n",
    "    v3_baseline_crps_list = [{'s':s,'w':[],'CRPS':[]} for s in s_prediction_list]\n",
    "\n",
    "    ## baseline v1---------------------------------------------------------------------------\n",
    "    # loop over w\n",
    "    for j in range(number_w):\n",
    "        w = window_list[j]\n",
    "        dummy_crps_v1_list = []\n",
    "        # loop over countries\n",
    "        for i in range(number_countries):\n",
    "            # loop over prediction windows s\n",
    "            for k in range(number_s):\n",
    "                dummy_crps_v1_list.append(task2_baseline_list[task2_index][i]['baseline'][k]['CRPS'][j])\n",
    "        v1_baseline_crps_dict['w'].append(w)\n",
    "        v1_baseline_crps_dict['CRPS'].append(np.mean(dummy_crps_v1_list))\n",
    "\n",
    "    v1_baseline_crps = pd.DataFrame(v1_baseline_crps_dict)\n",
    "\n",
    "    w_compute_list[task2_index]['data'].append(v1_baseline_crps)\n",
    "\n",
    "    v1_baseline_crps = v1_baseline_crps[v1_baseline_crps.CRPS == v1_baseline_crps.loc[:,'CRPS'].min()]\n",
    "    v1_baseline_crps.set_index(pd.Index(range(len(v1_baseline_crps))), inplace=True)\n",
    "        \n",
    "    w_minimization_list[task2_index]['minWData'].append(v1_baseline_crps)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    ## baseline v2----------------------------------------------------------------------------\n",
    "    # list for baseline v2\n",
    "    for i in range(number_countries):\n",
    "        for j in range(number_w):\n",
    "            w = window_list[j]\n",
    "            dummy_crps_v2_list = []\n",
    "            for k in range(number_s):\n",
    "                dummy_crps_v2_list.append(task2_baseline_list[task2_index][i]['baseline'][k]['CRPS'][j])\n",
    "            v2_baseline_crps_list[i]['baseline']['w'].append(w)\n",
    "            v2_baseline_crps_list[i]['baseline']['CRPS'].append(np.mean(dummy_crps_v2_list))\n",
    "        \n",
    "    # dataframe with the w that minimizes the CRPS for every country (v2)\n",
    "    data_v2 = {\n",
    "        'country_id':[],\n",
    "        'w':[],\n",
    "        'CRPS':[]\n",
    "    }\n",
    "    for i in range(len(v2_baseline_crps_list)):\n",
    "        # get the index of the minimal CRPS value\n",
    "        min_index = v2_baseline_crps_list[i]['baseline']['CRPS'].index(min(v2_baseline_crps_list[i]['baseline']['CRPS']))\n",
    "        \n",
    "        # store values in dict\n",
    "        data_v2['country_id'].append(v2_baseline_crps_list[i]['country_id'])\n",
    "        data_v2['w'].append(v2_baseline_crps_list[i]['baseline']['w'][min_index])\n",
    "        data_v2['CRPS'].append(v2_baseline_crps_list[i]['baseline']['CRPS'][min_index])\n",
    "        \n",
    "    v2_baseline_crps = pd.DataFrame(data_v2)\n",
    "    w_minimization_list[task2_index]['minWData'].append(v2_baseline_crps)\n",
    "    w_compute_list[task2_index]['data'].append(v2_baseline_crps_list)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## baseline v3---------------------------------------------------------------------------\n",
    "    for s_index in range(number_s):\n",
    "        dummy_crps_v3_list = []\n",
    "        s = s_prediction_list[s_index]\n",
    "        for w_index in range(number_w):\n",
    "            w = window_list[w_index]\n",
    "            for i in range(number_countries):\n",
    "                dummy_crps_v3_list.append(task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS'][w_index])\n",
    "            v3_baseline_crps_list[s_index]['w'].append(w)\n",
    "            v3_baseline_crps_list[s_index]['CRPS'].append(np.mean(dummy_crps_v3_list))\n",
    "\n",
    "    # dataframe with the w that minimize the CRPS for each prediction window s\n",
    "    data_v3 = {\n",
    "        's':[],\n",
    "        'w':[],\n",
    "        'CRPS':[]\n",
    "    }\n",
    "    # length of the v3_baseline list is the number of prediction windows\n",
    "    for i in range(len(v3_baseline_crps_list)):\n",
    "        s = s_prediction_list[i]\n",
    "        # get the index of the minimal CRPS value\n",
    "        min_index = v3_baseline_crps_list[i]['CRPS'].index(min(v3_baseline_crps_list[i]['CRPS']))\n",
    "\n",
    "        # store values in dict\n",
    "        data_v3['s'].append(s)\n",
    "        data_v3['w'].append(v3_baseline_crps_list[i]['w'][min_index])\n",
    "        data_v3['CRPS'].append(v3_baseline_crps_list[i]['CRPS'][min_index])\n",
    "\n",
    "    v3_baseline_crps = pd.DataFrame(data_v3)\n",
    "\n",
    "    w_minimization_list[task2_index]['minWData'].append(v3_baseline_crps)\n",
    "    w_compute_list[task2_index]['data'].append(v3_baseline_crps_list)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    ## baseline v4---------------------------------------------------------------------------\n",
    "    v4_baseline_crps = [{'country_id':country,\n",
    "                        's':[],\n",
    "                        'w':[],\n",
    "                        'CRPS':[]\n",
    "                        } for country in country_list]\n",
    "\n",
    "    # loop over all countries\n",
    "    for i in range(len(task2_baseline_list[task2_index])):\n",
    "        # loop over all prediction windows\n",
    "        for s_index in range(number_s):\n",
    "            s = s_prediction_list[s_index]\n",
    "            # get the index of the minimal CRPS value\n",
    "            min_index = task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS'].index(min(task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS']))\n",
    "        \n",
    "            # store values in dict\n",
    "            v4_baseline_crps[i]['s'].append(s)\n",
    "            v4_baseline_crps[i]['w'].append(task2_baseline_list[task2_index][i]['baseline'][s_index]['w'][min_index])\n",
    "            v4_baseline_crps[i]['CRPS'].append(task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS'][min_index])\n",
    "\n",
    "        v4_baseline_crps[i] = pd.DataFrame(v4_baseline_crps[i])\n",
    "\n",
    "    w_minimization_list[task2_index]['minWData'].append(v4_baseline_crps)\n",
    "    w_compute_list[task2_index]['data'].append(task2_baseline_list[task2_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump([task2_baseline_list, w_minimization_list, w_compute_list], \n",
    "       'task2_baseline_wmin_vars.joblib')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
