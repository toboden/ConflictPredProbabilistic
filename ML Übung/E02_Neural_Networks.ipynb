{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=/Library/TeX/texbin:$PATH\n",
    "#!pip install nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second ML1 exercise. We will build our first neural network, which will have a hidden layer. We will also see how to train a neural network for various purposes. We will recognise the difference between several activation functions. \n",
    "\n",
    "**You will learn how to:**\n",
    "- Implement a 2-class classification neural network with a single hidden layer\n",
    "- Use units with a non-linear activation function, such as tanh \n",
    "- Compute the cross entropy loss \n",
    "- Implement forward and backward propagation\n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will train a neural network with a single hidden layer.\n",
    "\n",
    "**Here is our neural network**:\n",
    "\n",
    "<img src=\"imgs/NN.png\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer\n",
    "Each node in the input layer refers to each feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "The next layers after the input layer are where all the magic happens. The hidden layer takes the input layer and applies a non-linear activation function to it. Mathematically, we can represent the function of the hidden layer as follows:\n",
    "$$f(x) =  \\sigma(Wx + b) $$\n",
    "Where $\\sigma $ refers to the non-linear activation function\n",
    "\n",
    "To keep things simple, we will only use one hidden layer in our model for this notebook. Increasing the number of hidden layers tends to increase the model complexity and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "This is the last layer of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "When designing the neural network model architecture, we also need to decide what activation functions to use for each layer. Activation functions have an important role to play in neural networks. You can think of activation functions as transformers in neural networks; they take an input value, transform the input value, and pass the transformed value to the next layer. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "**Why do we need non-linear activation functions?**  \n",
    "A neural network without an activation function is essentially just a linear regression model. The activation function applies a non-linear transformation to the input making it capable to learn and perform more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Libraries\n",
    "first we will import all the packages that are required for this exercise. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear \n",
    "You can interpret this as no activation function. We usually use it in the output layer of regression networks, since we don't want to squash the output into a certain range of numbers.\n",
    "$$f(x) = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def der_linear(x):\n",
    "    return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "# Create input from -10 to 10 in 100 steps\n",
    "x  = np.linspace(-10, 10, 100)\n",
    "f  = linear(x)\n",
    "df = der_linear(x)\n",
    "# Plot the linear layer and its derivative\n",
    "plt.plot(x, f, label=\"linear\") \n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"linear(X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sigmoid\n",
    "The sigmoid activation function simply takes a value and squashes it between 0 and 1. You can interpret it as the probability of an output prediction. Therefore, we usually use it in the output layer in binary classification networks. Besides that, we sometimes use it in hidden layers. However, it should be noted that the sigmoid function is monotonic but its derivative is not. Hence, the neural network may get stuck at a suboptimal solution. Furthermore, the gradient vanishes in saturation areas.\n",
    "$$f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "Its derivative \n",
    "$$f'(x) = \\sigma(x)*(1-\\sigma(x)) $$\n",
    "\n",
    "**Exercise**: Implement the sigmoid function with numpy and the function `np.exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return sigmoid(x) * (1- sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "# Create input from -10 to 10 in 100 steps\n",
    "x  = np.linspace(-10, 10, 100)\n",
    "f  = sigmoid(x)\n",
    "df = der_sigmoid(x)\n",
    "# Plot the sigmoid layer and its derivative\n",
    "plt.plot(x, f, label=\"sigmoid\") \n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use sigmoid: ####\n",
    "* If you want output values in the range from 0 to 1, use sigmoid as output layer\n",
    "* If you are doing binary classification problems, use sigmoid\n",
    "\n",
    "#### Problem with sigmoid: ####\n",
    "* If the x value is small or big, the sigmoid saturates, creating a vanishing gradient, resulting in no learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Hyperbolic Tangent\n",
    "The tanh function is another possible activation function that can be used as a non-linear function between layers of a neural network. It actually shares a few things in common with the sigmoid activation function. They both look very similar. But while a sigmoid function will map input values to the range of 0 to 1, thanh will map values between -1 and 1.\n",
    "\n",
    "$$f(x) = tanh(x) = \\frac {sinh(x)}{cosh(x)} = \\frac {e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "Its derivative \n",
    "$$f'(x) = 1 - tanh(x)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def der_tanh(x):\n",
    "    return 1- tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "# Create input from -10 to 10 in 100 steps\n",
    "x  = np.linspace(-10, 10, 100)\n",
    "f  = tanh(x)\n",
    "df = der_tanh(x)\n",
    "# Plot the tanh layer and its derivative\n",
    "plt.plot(x, f, label=\"hyperbolic\") \n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"tanh(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use tanh: ####\n",
    "* Usually used in hidden layers of a neural network. It helps in centering the data by bringing the mean close to 0. This makes learning for the next layer much easier (Optimization is easier in this method). \n",
    "\n",
    "#### Problems with tanh: ####\n",
    "* Vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ReLU\n",
    "Today, ReLU is the most popular choice of activation function for DNNs, and it has become a default choice for activation functions. Its range is from 0 to infinity, and both the function itself and its derivative are monotonic. One drawback of the ReLU function is that the gradient for negative inputs is zero. This phenomenon is also called \"dying ReLU\", meaning that the layers connected to this \"neuron\" will not receive any gradient information and the parameters won't be adjusted. To fix this problem, *Leaky ReLU* was invented to introduce a small slope in the negative part and recover gradient information.\n",
    "$$f(x) = \\max(0,x)$$\n",
    "\n",
    "The derivative of the ReLU is :\n",
    "$$f(x)= \\begin{cases}1 & \\text { if } x > 0 \\\\ 0 & x <= 0\\end{cases}$$\n",
    "\n",
    "**Instructions**:\n",
    "- You will compare a vector x with scalar 0 and return a new array containing the element-wise maximum \n",
    "    - Use: `numpy.maximum(x, 0)`.\n",
    "- You will compare vector x with scalar 0 and return a new array containing either ones or zeros\n",
    "    - Use: `np.where(x <= 0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x) :\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def der_relu(x):\n",
    "    i     = np.where(x <= 0)\n",
    "    df    = np.ones_like(x)\n",
    "    df[i] = 0 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "# Create input from -10 to 10 in 100 steps\n",
    "x = np.linspace(-10, 10, 100)\n",
    "f = relu(x)\n",
    "df = der_relu(x)\n",
    "# Plot the ReLU layer and its derivative\n",
    "plt.plot(x, f,  label=\"relu\")\n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"ReLU(X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with ReLU: ####\n",
    "* The only issue is that the derivative is not defined at $x = 0$, which we can overcome by assigning the derivative to $0$ at $x = 0$. However, this means that for $x <= 0$ the gradient is zero and learning won't take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Leaky ReLU\n",
    "Leaky ReLU is an improved version of the ReLU function. Recall that the gradient is 0 for x<0 for a ReLU function, which made the neurons die for activations in that region. The leaky ReLU will instead have a small negative slope (of 0.01, or so), allowing gradient information to propagate to earlier layers. \n",
    "$$f(x) = max(0.01x, x)$$\n",
    "\n",
    "The derivative of the leaky ReLU is :\n",
    "\n",
    "$$f(x)= \\begin{cases}1 & \\text { if } x > 0 \\\\ 0.01 & x <= 0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x) :\n",
    "    return np.maximum(x, 0.01*x)\n",
    "\n",
    "def der_leaky_relu(x):\n",
    "    i     = np.where(x <= 0)\n",
    "    df    = np.ones_like(x)\n",
    "    df[i] = 0.01 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function\n",
    "# Create input from -10 to 10 in 100 steps\n",
    "x = np.linspace(-10, 10, 100)\n",
    "f = leaky_relu(x)\n",
    "df = der_leaky_relu(x)\n",
    "# Plot the leaky ReLU layer and its derivative\n",
    "plt.plot(x, f,  label=\"leaky_relu\")\n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Leaky_ReLU(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Softmax\n",
    "Softmax is a generalized version of the sigmoid function used for multiclass classification. Hence, we use it in the output layer of multiclass classification networks.\n",
    "\n",
    "#### Properties of Softmax function ####\n",
    "\n",
    "1. The calculated probabilities will be in the range from 0 to 1.\n",
    "2. The sum of all the probabilities equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2.1, -0.2 , -1.2])\n",
    "y_probas = softmax(x)\n",
    "print('Probabilities: ', y_probas)\n",
    "print('The sum of all probabilities: ', np.sum(y_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Train a Neural Network from Scratch\n",
    "General methodology to build neural networks:\n",
    "1. Define the neural network structure/architecture ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Forward propagation\n",
    "    - Compute loss\n",
    "    - Backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "On a feedforward neural network, we have a set of input features and initially random weights. The forward pass propagates the input signal through the network all the way to the last layer and outputs a prediction:\n",
    "\n",
    "**Mathematically**:   \n",
    "For one example $x^{(i)}$  \n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\text{if } \\hat{y}^{(i)} > 0.5 \\\\ 0 & \\text{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "**Diagram**:\n",
    "\n",
    "<img src=\"imgs/NN.png\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Empirical Loss\n",
    "The loss function $\\mathcal{L}$ is a performance metric that gives a continuous-valued score, given the prediction $\\hat{y}$ of a model and a target value $y$. In other words, it measures the difference between a predicted and desired value for one example. Usually we construct loss functions in such a way, that improving the loss also improves the performance on a desired task. For our binary classificator, we will use the cross entropy loss, defined below:\n",
    "$$ \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(\\hat{y}^{(i)}) - (1-y^{(i)} )  \\log(1-\\hat{y}^{(i)})\\tag{6}$$\n",
    "\n",
    "For supervised learning tasks, we usually use loss functions, that decrease, if the performance on the task improves and increase, if the performance gets worse, assigning a higher loss value.\n",
    "\n",
    "**Example**:\n",
    "* Correct classification\n",
    "  * Label: $(y = 1)$\n",
    "  * Prediction: $\\hat{y} = 1$\n",
    "  * $\\mathcal{L} = 0$\n",
    "* Wrong classification\n",
    "  * Label: $(y = 0)$\n",
    "  * Prediction: $\\hat{y} = 1$\n",
    "  * $\\mathcal{L} = \\infty$\n",
    "\n",
    "The empirical loss is the average performance on a set of examples. Usually, we use batch gradient decent and therefore the empirical loss denotes the average performance on a batch of examples.\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\\tag{7}$$\n",
    "\n",
    "### Backward Pass / Backpropagation\n",
    "The backward pass takes the error at the end of the network, calculated by the loss function, and propagates it through all the layers of the network all the way to the first layer, calculating the gradient for each parameter. Backpropagation is the efficient, recursive implementation of the chain rule to obtain the gradients of all parameters in linear instead of exponential time. Without the backpropagation algorithm, deep neural networks wouldn't be possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain Rule\n",
    "The chain rule is a fundamental rule of differentiation that allows to calculate the derivative of the composition of two functions.\n",
    "\n",
    "Simple example:\n",
    "* $h(x)=f \\circ g$\n",
    "* $\\frac{\\partial h}{\\partial x}=\\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network example: In order to update the parameters, we need to calculate the gradients $dW^{[2]},db^{[2]},dW^{[1]},db^{[1]}$.\n",
    "\n",
    "\n",
    "<img src=\"imgs/BP.png\" style=\"width:900px;height:200px;\">\n",
    "\n",
    "1. Chain rule for the parameters's gradients in the second layer:\n",
    "$$ dW^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[2]}} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial W^{[2]}}\\tag{8}$$\n",
    "$$ db^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[2]}} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial b^{[2]}}\\tag{9}$$\n",
    "\n",
    "2. Chain rule for the parameters's gradients in the first layer:\n",
    "$$dW^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[1]}} =  \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial a^{[1]}}\\frac{\\partial a^{[1]} }{\\partial z^{[1]}}\\frac{\\partial z^{[1]} }{\\partial W^{[1]}} \\tag{10}$$\n",
    "$$db^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[1]}} =  \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial a^{[1]}}\\frac{\\partial a^{[1]} }{\\partial z^{[1]}}\\frac{\\partial z^{[1]} }{\\partial b^{[1]}} \\tag{11}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To calculate the gradients for the second layer, we first need to compute gradient of the empirical loss $\\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}}$ with respect to its input and the activation function activation $\\frac{\\partial a^{[2]} }{\\partial z^{[2]}}$ with respect to its input.\n",
    "$$da^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} =  - (\\frac{y }{a^{[2]}} - \\frac{1-y }{1-a^{[2]}})\\tag{12}$$\n",
    "$$dz^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial z^{[2]}}   = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}}  \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}  = da^{[2]} \\sigma'(z^{[2]}) \\tag{13}$$\n",
    "\n",
    "1. We can now calculate the gradients for the parameters of the second layer, based on the previous gradients.\n",
    "$$  dW^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[2]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]} }{\\partial W^{[2]}}= dz^{[2]} a^{[1] T} \\tag{14}  $$\n",
    "$$ db^{[2]} =  \\frac{\\partial \\mathcal{L} }{\\partial b^{[2]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]} }{\\partial b^{[2]}}= dz^{[2]} \\tag{15}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To calculate the gradients for the first layer, we first need to compute the gradient of the linear layer $\\frac{\\partial z^{[2]} }{\\partial a^{[1]}}$ with respect to its input and the activation layer $\\frac{\\partial a^{[1]} }{\\partial z^{[1]}}$ with respect to its input.\n",
    "$$da^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[1]}} = \\frac{\\partial\\mathcal{L}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]} }{\\partial a^{[1]}} = W^{[2] T} dz^{[2]} \\tag{16}$$\n",
    "$$dz^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial z^{[1]}}   = \\frac{\\partial \\mathcal{L} }{\\partial a^{[1]}}  \\frac{\\partial a^{[1]} }{\\partial z^{[1]}} = da^{[1]}  \\tanh'(z^{[1]}) \\tag{17} $$\n",
    "\n",
    "6. Now we are able to compute the gradients of the parameters in the first layer.\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial W^{[1]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}= dz^{[1]} x^{T} \\tag{18}  $$\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial b^{[1]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}= dz^{[1]} \\tag{19}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "The optimizer uses the gradient information to update all the parameters with a new value, ideally improving the loss in the next iteration. Gradient descent is an iterative first order optimization algorithm that tries to find a local optimum of an objective function by adjusting a set of model parameters. It calculates the direction of steepest ascent of the loss surface and goes into the opposite direction. It takes into account, a user-defined learning rate, and an initial set of parameter values.\n",
    "\n",
    "Working: (Iterative)\n",
    "1. Start with initial parameter values.\n",
    "2. Calculate forward pass.\n",
    "3. Calculate empirical loss.\n",
    "4. Calculate the backward pass\n",
    "   1. Calculate the parameter gradients, the direction of steepest ascent\n",
    "   2. Update parameter values using the update function and learning rate.\n",
    "5. Go to 6., if a certain empirical loss value is reached, otherwise go to 2.\n",
    "6. Returns minimized costs and set of optimal parameters.\n",
    "\n",
    "Update rule:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{20}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{21}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $l \\in \\{1,2\\}$.\n",
    "\n",
    "<img src=\"imgs/GD.jpg\" style=\"width:350px;height:250px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In this practical session, we will start with the implementation of a shallow 2 layer network for binary classification, which we will build from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "We will be using numpy to load the data set, which is contained in serialized array objects (.npy).\n",
    "\n",
    "### Overview of the Data Set\n",
    "For this exercise, we will use the \"Sign Language Digit Data Set\". In this data set, there are 2062 sign language digit images (64x64px). You know that the numbers are from 0 to 9, so there are ten unique characters. \n",
    "\n",
    "To start the exercise: We will use only the symbols 0 and 1 for simplicity. \n",
    "\n",
    "Data layout:\n",
    "* Examples displaying the number zero have indices between 204 and 408 (205 examples)\n",
    "* Examples displaying the number one have indices between 822 and 1027 (206 examples)\n",
    "* We will use 205 samples from each class\n",
    "\n",
    "**Note:** 205 samples are tiny for deep learning. But since it is a tutorial, it is not that important.\n",
    "\n",
    "\n",
    "Lets prepare our X and Y arrays, in which the data set should reside. X should be the image arrays and Y should be the label array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set\n",
    "x_l = np.load('./data/X.npy')\n",
    "Y_l = np.load('./data/Y.npy')\n",
    "img_size = 64\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x_l[260].reshape(img_size, img_size),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(x_l[900].reshape(img_size, img_size),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the Data Set\n",
    "* Choose the examples that display the number zero and one and concatenate them to a new array X and Y respectively\n",
    "* Flatten X and Y for convenience\n",
    "* Create a train and test set by splitting the data\n",
    "\n",
    "The final training data set should look like this:\n",
    "\n",
    "<img src=\"imgs/example.png\" style=\"width:500px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the examples that display the number zero and one and concatenate them to a new array X. Create the corresponding label array for Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the input images that display zeros and ones along the row axis.\n",
    "# Results in X with indices 0 to 204 being zero sign examples and indices 205 to 410 being one sign examples.\n",
    "X = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0)\n",
    "# Create the output data, the array of the labels.\n",
    "Y_zeros = np.zeros(205)\n",
    "Y_ones = np.ones(205)\n",
    "Y = np.concatenate((Y_zeros, Y_ones), axis=0).reshape(X.shape[0],1)\n",
    "print(\"X shape: \" , X.shape)\n",
    "print(\"Y shape: \" , Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of X is (410, 64, 64):\n",
    "* 410 images\n",
    "* 64, 64 is the size of the images (64x64 pixels)\n",
    "\n",
    "The shape of the Y is (410,1):\n",
    "* 410 labels (0 and 1)\n",
    "* 1, size of each label (scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split X and Y into train and test sets, by randomizing the data and splitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then lets create x_train, y_train, x_test, y_test arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "number_of_train = X_train.shape[0]\n",
    "number_of_test = X_test.shape[0]\n",
    "print(\"Number of samples in the training data set: \" , number_of_train)\n",
    "print(\"Number of samples in the test data set: \" , number_of_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `test_size` = Percentage of test size.\n",
    "* `random_state` = Use same seed while randomizing. It means that if we call train_test_split repeatedly, it always * creates the same train and test distribution because we randomize with a fixed random_state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our neural network accepts only 2-dimensional inputs, we need to flatten the images in the input array from 64x64 to 4096. Our label array Y is already a 2-dimensional array, so we leave it like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = X_train.reshape(number_of_train, X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flatten = X_test.reshape(number_of_test, X_test.shape[1]*X_test.shape[2])\n",
    "print(\"X train flatten\", X_train_flatten.shape)\n",
    "print(\"X test flatten\", X_test_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to transpose the data in order to process it with the neural network. We want each sample as a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train_flatten.T\n",
    "x_test = X_test_flatten.T\n",
    "y_train = Y_train.T\n",
    "y_test = Y_test.T\n",
    "print(\"x train: \", x_train.shape)\n",
    "print(\"x test: \", x_test.shape)\n",
    "print(\"y train: \", y_train.shape)\n",
    "print(\"y test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture and Forward Pass\n",
    "**Exercise:**\n",
    "The forward pass computes all steps from the input to the final prediction by propagating the data through the network. Use equations (1)-(5) and the helper functions `tanh()` and `sigmoid()` to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    # Implement the forward pass of the NN architecture to calculate the final output a2 (class probability)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Test if the shapes are correct\n",
    "    assert(a2.shape == (1, X.shape[1]))\n",
    "    cache = {\"z1\": z1,\n",
    "             \"a1\": a1,\n",
    "             \"W1\": W1,\n",
    "             \"b1\": b1,\n",
    "             \"z2\": z2,\n",
    "             \"a2\": a2,\n",
    "             \"W2\": W2,\n",
    "             \"b2\": b2}    \n",
    "    return a2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization\n",
    "Before we can start optimizing the weights/parameters of our neural network, we first need to initialize them with an initial value. Initialization can matter a lot for neural networks an decide if a network is able to learn a certain task or not.\n",
    "We will initialize the weights with values from an uniform distribution. The number of parameters we have to initialize is: \n",
    "* $n_x$ -- size of the input layer\n",
    "* $n_h$ -- size of the hidden layer\n",
    "* $n_y$ -- size of the output layer\n",
    "\n",
    "**Exercise**: Implement the function `initialize_parameters()`, which will be used to initialize the parameters for our two layer network.\n",
    "\n",
    "**Remember** that when we compute $W X + b$ in numpy, broadcasting is applied to fit mismatching array dimensions. For example, if: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{22}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{23}  $$\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.\n",
    "- You will initialize the weight matrices with random values from a uniform distribution. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- You will initialize the bias vectors with zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims, init_method =\"random\"):\n",
    "    np.random.seed(1)\n",
    "    # n_x -- size of the input layer\n",
    "    # n_h -- size of the hidden layer\n",
    "    # n_y -- size of the output layer\n",
    "    n_x, n_h, n_y = layers_dims\n",
    "    if init_method == \"random\":\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        W1 = \n",
    "        b1 = \n",
    "        W2 = \n",
    "        b2 = \n",
    "        ### END CODE HERE ###\n",
    "    elif init_method == \"xavier\":\n",
    "        W1 = np.random.randn(n_h,n_x) * np.sqrt(1/n_h)\n",
    "        b1 = np.zeros(shape=(n_h, 1))\n",
    "        W2 = np.random.randn(n_y,n_h)\n",
    "        b2 = np.zeros(shape=(n_y, 1))\n",
    "    elif init_method == \"zeros\":\n",
    "        W1 = np.zeros(shape=(n_h, n_x))\n",
    "        b1 = np.zeros(shape=(n_h, 1))\n",
    "        W2 = np.zeros(shape=(n_y, n_h))\n",
    "        b2 = np.zeros(shape=(n_y, 1))\n",
    "    \n",
    "    # Test if the shape is correct\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x=3 \n",
    "n_h=2 \n",
    "n_y=1\n",
    "layers_dims = [n_x,n_h,n_y]\n",
    "parameters = initialize_parameters(layers_dims, init_method = \"random\")\n",
    "print(f\"W1 = \\n {parameters['W1']}\")\n",
    "print(f\"b1 = \\n {parameters['b1']}\")\n",
    "print(f\"W2 = \\n {parameters['W2']}\")\n",
    "print(f\"b2 = \\n {parameters['b2']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(layers_dims, init_method = \"xavier\")\n",
    "print(f\"W1 = \\n {parameters['W1']}\")\n",
    "print(f\"b1 = \\n {parameters['b1']}\")\n",
    "print(f\"W2 = \\n {parameters['W2']}\")\n",
    "print(f\"b2 = \\n {parameters['b2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "**Exercise:**\n",
    "Implement the loss function according to equation (6) and (7). This function takes the output a2 of `forward_propagation()` and the true label Y as input and returns the loss.\n",
    "Be careful: The cross entropy loss can produce NANs, use `np.nansum()` for the empirical loss to ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(a2, Y):\n",
    "    m = Y.shape[1]\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return empirical_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "**Exercise:**\n",
    "Backpropagation is all steps from cost calculation to calculation of the gradient, use the equations (11)-(18) and the help functions `der_tanh` and `der_sigmoid` to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache):\n",
    "    m = X.shape[1]\n",
    "    # Retrieve each variable from the dictionary \"cache\"\n",
    "    z1 = cache['z1']\n",
    "    a1 = cache['a1']\n",
    "    W1 = cache['W1']\n",
    "    b1 = cache['b1']\n",
    "    z2 = cache['z2']\n",
    "    a2 = cache['a2']\n",
    "    W2 = cache['W2']\n",
    "    b2 = cache['b2']\n",
    "    \n",
    "    ### START CODE HERE ### (8 line of code) \n",
    "    # Initialize backpropagation with the gradient of the empirical loss da2.\n",
    "    \n",
    "    # 2. layer (sigmoid -> linear) gradients. Calculate the gradient dz2\n",
    "    \n",
    "    # Calculate the gradients of the parameters for the 2. layer dW2 and db2. (approx. 2 lines)\n",
    "    \n",
    "    # Calculate the gradient for da1. linear z2 -> activation a2\n",
    "    \n",
    "    # 1. layer (tanh -> linear) gradients. Calculate the gradient dz1\n",
    "    \n",
    "    # Calculate the gradients of the parameters for the 1. layer dW1 and db1. (approx. 2 lines)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Test if the shape correct is\n",
    "    assert (da2.shape == a2.shape)\n",
    "    assert (dz2.shape == z2.shape)\n",
    "    assert (dW2.shape == W2.shape)\n",
    "    assert (db2.shape == b2.shape)\n",
    "    assert (da1.shape == a1.shape)\n",
    "    assert (dz1.shape == z1.shape)\n",
    "    assert (dW1.shape == W1.shape)\n",
    "    assert (db1.shape == b1.shape)\n",
    "    gradients = {\"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the update rule using gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
    "\n",
    "**General gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$, where $\\alpha$ is the learning rate and $\\theta$ represents all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    n = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for k in range(n):\n",
    "        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
    "        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prediction\n",
    "\n",
    "In order to measure the accuracy during training and to predict which sign is displayed with the trained model (a zero or a one), we create a `predict()` function.\n",
    "We use `forward_propagation()` to predict our results.\n",
    "\n",
    "**Reminder**: $$y^{(i)}_{prediction} = \\begin{cases} 1 & \\text{if } \\hat{y}^{(i)} > 0.5 \\\\ 0 & \\text{otherwise } \\end{cases}\\tag{5}$$\n",
    "    \n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    # Prediction for one image or for a batch of images  \n",
    "    m = X.shape[1]  # batch of images      \n",
    "    p = np.zeros((1,m), dtype=int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a2, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Convert probabilities to 0/1 predictions\n",
    "    for i in range(0, a2.shape[1]):\n",
    "        if a2[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "Now we want to train our model. Therefore we need to combine all functions to implement the forward and backward pass.\n",
    "\n",
    "1. Forward pass\n",
    "2. Estimate empirical loss\n",
    "3. Backward pass (backpropagation)\n",
    "4. Update parameters\n",
    "5. Start at 1. if stopping criterion is not met\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn the optimal setting of $W$ and $b$ by minimizing the empirical loss $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"random\"):\n",
    "    grads      = {}\n",
    "    losses      = []  # to keep track of the loss\n",
    "    accuracies = []  # to keep track of the accuracy\n",
    "    m = X.shape[1]  # number of examples\n",
    "    layers_dims = [X.shape[0], 10, 1]\n",
    "    \n",
    "    # Initialize parameter dictionary.\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters(layers_dims, init_method =\"zeros\")\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters(layers_dims, init_method =\"random\")\n",
    "    elif initialization == \"xavier\":\n",
    "        parameters = initialize_parameters(layers_dims, init_method =\"xavier\")\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: linear -> ReLU -> linear -> ReLU -> linear -> sigmoid.\n",
    "        a2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        loss = compute_loss(a2, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(f\"Iteration: {i}\")\n",
    "            print(f\"Empirical loss: {loss}\")\n",
    "            p = predict(X, parameters)\n",
    "            accuracy = np.mean(p[0,:] == Y[0,:])\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "    # plot the loss, accuracy\n",
    "    fig1, (ax1, ax2) = plt.subplots(figsize=(10,12), nrows=2, ncols=1)\n",
    "    ax1.plot(losses)\n",
    "    ax1.set_ylabel('empirical loss')\n",
    "    ax1.set_xlabel('iterations (per hundreds)')\n",
    "    ax1.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax2.plot(accuracies)\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_xlabel('iterations (per hundreds)')\n",
    "    ax2.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = optimize(x_train, y_train, learning_rate = 0.01, num_iterations = 10000, print_cost = True, initialization = \"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "With the trained model, we can now predict unseen instances of hand signs. In other words, which number has been displayed, zero or one, given an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "plt.imshow(X_test[t].reshape(img_size, img_size), cmap='gray')\n",
    "plt.show()\n",
    "real_image = X_test[t]\n",
    "prep_image = np.expand_dims(x_test[:, t], axis=1)\n",
    "p = predict(prep_image, parameters) \n",
    "print(f\"Model prediction: {int(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predict(x_test, parameters)\n",
    "accuracy = np.mean(p[0,:] == y_test[0,:])\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Choice of learning rate\n",
    "\n",
    "**Reminder**:\n",
    "The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Varying the hidden layer size ####\n",
    "In the example above we picked a hidden layer size of 10. Let’s now get a sense of how varying the hidden layer size can affect the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preventing overfitting in neural networks ####\n",
    "\n",
    "#### Dropout #### \n",
    "\n",
    "Means ignoring a certain set of hidden nodes during the learning phase of a neural network, they are dropped out. These hidden nodes are chosen randomly given a specified probability. In the forward pass during a training iteration, the randomly selected nodes are temporarily not used in calculating the loss; in the backward pass, the randomly selected nodes are not updated temporarily.\n",
    "\n",
    "#### Early stopping ####\n",
    "As the name implies, training a network with early stopping will end if the model performance doesn't improve for a certain number of iterations. The model performance is measured on a validation set that is different from the training set, in order to assess how well it generalizes. During training, if the performance degrades after several (let's say 50) iterations, it means the model is overfitting and not able to generalize well anymore. Hence, stopping the learning early in this case helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Instead of gradient descent, we could use stochastic gradient descent or mini batch gradient descent to train the network. Mini batch gradient descent typically performs better in practice.\n",
    "2. We used a fixed learning rate $\\alpha$ for gradient descent. Implement an annealing schedule for the gradient descent learning rate.\n",
    "3. We used a $\\tanh$ activation function for our hidden layer. We could experiment with other activation functions (some are mentioned above). Note that changing the activation function also means changing the backpropagation derivative.\n",
    "4. Extend the network from two to three classes. You will need to generate an appropriate dataset for this and also use the Softmax activation function in the last layer.\n",
    "5. Extend the network to four layers. Experiment with the layer size. Adding another hidden layer means you will need to adjust both the forward propagation as well as the backpropagation code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://www.geeksforgeeks.org/activation-functions/\n",
    "2. https://medium.com/@omkar.nallagoni/activation-functions-with-derivative-and-python-code-sigmoid-vs-tanh-vs-relu-44d23915c1f4\n",
    "3. https://maelfabien.github.io/deeplearning/act/#linear-activation\n",
    "4. Deep Learning Specialization on Coursera\n",
    "5. https://github.com/Kulbear/deep-learning-coursera\n",
    "6. https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
