{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "At first all four datasets are modified in a way, that all of them contain the same countries with at least the last 36 months of observations. Countries that are not present in all datasets are not used for the minimization of the CRPS in dependecy of w or s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import nbinom\n",
    "from scipy.stats import poisson\n",
    "import CRPS.CRPS as pscore\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # paths to the data\n",
    "    absolute_path = os.path.abspath('')\n",
    "    relative_path_features = \"data\\cm_features_to_oct\" + feature_years[i] + \".parquet\"\n",
    "    relative_path_actuals = \"data\\cm_actuals_\" + actual_years[i] + \".parquet\"\n",
    "\n",
    "    path_features = os.path.join(absolute_path, relative_path_features)\n",
    "    path_actuals = os.path.join(absolute_path, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations to_oct_17\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "# function to check, if the last 36 months are in the dataset of a country\n",
    "def check_last_36Months(country, yearindex):\n",
    "    month_list = features_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "    month_list = month_list[-36:]\n",
    "     \n",
    "    last_36_months = True\n",
    "    for month in month_list:\n",
    "        if month not in country.index.get_level_values('month_id'):\n",
    "            last_36_months = False\n",
    "            break\n",
    "\n",
    "    return last_36_months\n",
    "\n",
    "\n",
    "# list of all countries that are present in all four datasets\n",
    "country_list = []\n",
    "for i in range(len(features_df_list)):\n",
    "    country_list.extend(features_df_list[i]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "unique_list = []\n",
    "\n",
    "for item in country_list:\n",
    "    if country_list.count(item) == 4:\n",
    "        unique_list.append(item)\n",
    "\n",
    "country_list = list(set(unique_list))\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))\n",
    "\n",
    "\n",
    "# modify country_list so that it contains only country_ids \n",
    "# that have the last 36 months of observations in ALL DATASETS!\n",
    "dummy_list = []\n",
    "for countryIndex in country_list:\n",
    "    # loop through datasets\n",
    "    for i in range(len(features_df_list)):\n",
    "        dummy_hasLast36_months = True\n",
    "        if check_last_36Months(country_feature_group_list[i].get_group(countryIndex), i) is not True:\n",
    "            dummy_hasLast36_months = False\n",
    "    \n",
    "    if dummy_hasLast36_months is True:\n",
    "        dummy_list.append(countryIndex)\n",
    "\n",
    "# the values in country_list are the 'country_id'\n",
    "country_list = dummy_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The minimization is based on calculating the quantiles for each country, w and year (of the datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country 191/191\r"
     ]
    }
   ],
   "source": [
    "# list to save the predictions for each country\n",
    "baseline_country_predict_list = [{'country_id': country, 'prediction': {'2018': [], '2019': [], '2020': [], '2021': []}} for country in country_list]\n",
    "index_list = ['2018', '2019', '2020', '2021']\n",
    "# list of the (prediction) windows\n",
    "window_list = list(range(2, 37))\n",
    "s_prediction_list = list(range(3, 15))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" ## changes, so that the calculation does not take a long time -------------------\n",
    "#shorter windows\n",
    "window_list = list(range(2, 25))\n",
    "# remove all but ten countries\n",
    "elements_to_remove = country_list[0:(len(country_list)-10)]\n",
    "country_list = [element for element in country_list if element not in elements_to_remove]\n",
    "\n",
    "baseline_country_predict_list = [{'country_id': country, 'prediction': {'2018': [], '2019': []}} for country in country_list]\n",
    "index_list = ['2018', '2019']\n",
    "##---------------------------------------------------------------------------------- \"\"\"\n",
    "\n",
    "\n",
    "number_countries = len(country_list)\n",
    "number_dataframes = len(features_df_list)\n",
    "number_w = len(window_list)\n",
    "\n",
    "\n",
    "# loop through all countries (that are present in each dataset)\n",
    "for index in range(number_countries):\n",
    "    country = country_list[index]\n",
    "\n",
    "    print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "\n",
    "    # list to store the predictions for each year temporally\n",
    "    baseline_predict_list = [[] for _ in range(number_dataframes)]\n",
    "    \n",
    "    # loop through datasets\n",
    "    for i in range(number_dataframes): \n",
    "        features = country_feature_group_list[i].get_group(country) # features of country in dataset i\n",
    "        \n",
    "        baseline_predict_list[i] = []\n",
    "\n",
    "        quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "        quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "\n",
    "        dummy_quantile_list = [f\"{round(q * 100, 1)}%\" for q in quantiles]\n",
    "\n",
    "        # loop through windows\n",
    "        for j in range(number_w):\n",
    "            w = window_list[j] # current window\n",
    "\n",
    "            # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(features.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(features.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "            #hier verteilung = nbinom ppf als\n",
    "            dummy_fatalities_list = []\n",
    "\n",
    "            # string to store distribution\n",
    "            dist_string = ''\n",
    "\n",
    "            if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "            elif var != 0 and var <= mean:\n",
    "                    dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "\n",
    "                    dist_string = 'Pois'\n",
    "\n",
    "            else:\n",
    "                    dummy_fatalities_list = [0] * 999\n",
    "                    dist_string = 'None'\n",
    "\n",
    "            baseline_predict_list[i].append({'window':w, 'country_id':country, 'dist':dist_string, \n",
    "                                             'mean':mean, 'var':var, 'quantile':[], 'fatalities':[]}) \n",
    "\n",
    "            baseline_predict_list[i][j]['quantile'] = dummy_quantile_list    \n",
    "            baseline_predict_list[i][j]['fatalities'] = dummy_fatalities_list\n",
    "\n",
    "            baseline_predict_list[i][j] = pd.DataFrame(baseline_predict_list[i][j])\n",
    "            baseline_predict_list[i][j].set_index(['window', 'quantile'], inplace=True)\n",
    "\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]] = baseline_predict_list[i]\n",
    "\n",
    "        # combine each w dataset together\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]] = pd.concat(baseline_country_predict_list[index]['prediction'][index_list[i]], axis=0)\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]].sort_index(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 60, 62, 63, 64, 65, 66, 67, 69, 70, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 198, 199, 205, 206, 209, 213, 214, 218, 220, 222, 223, 231, 232, 233, 234, 235, 237, 242, 243, 244, 245, 246]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>dist</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>fatalities</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.5%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.6%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.7%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.8%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.9%</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country_id  dist  mean  var  fatalities\n",
       "quantile                                         \n",
       "0.1%              10  None   0.0  0.0           0\n",
       "0.2%              10  None   0.0  0.0           0\n",
       "0.3%              10  None   0.0  0.0           0\n",
       "0.4%              10  None   0.0  0.0           0\n",
       "0.5%              10  None   0.0  0.0           0\n",
       "...              ...   ...   ...  ...         ...\n",
       "99.5%             10  None   0.0  0.0           0\n",
       "99.6%             10  None   0.0  0.0           0\n",
       "99.7%             10  None   0.0  0.0           0\n",
       "99.8%             10  None   0.0  0.0           0\n",
       "99.9%             10  None   0.0  0.0           0\n",
       "\n",
       "[999 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(country_list)\n",
    "baseline_country_predict_list[9]['prediction']['2018'].xs(12, level = 'window')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1-4\n",
    "\n",
    "Optimize **w** (through the CRPS) regarding\n",
    "|            | datasets    | countries   | prediction windows |\n",
    "|------------|-------------|-------------|--------------------|\n",
    "| baseline 1 | all         | all         | all                |\n",
    "| baseline 2 | all         | inidvidual  | all                |\n",
    "| baseline 3 | all         | all         | individual         |\n",
    "| baseline 4 | all         | inidvidual  | individual         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country 191/191   prediction window 12/12\r"
     ]
    }
   ],
   "source": [
    "# list to store all crps values\n",
    "baseline_crps_list = [\n",
    "    {\n",
    "        'country_id': country,\n",
    "        'baseline': [\n",
    "            {'s': s, 'w': [], 'CRPS': []}\n",
    "            for s in s_prediction_list\n",
    "        ]\n",
    "    }\n",
    "    for country in country_list\n",
    "]\n",
    "\n",
    "# numver of prediction windows\n",
    "number_s = len(s_prediction_list)\n",
    "\n",
    "# fill list with crps calculations\n",
    "for s in s_prediction_list:\n",
    "    print('                  prediction window ' + str(s-2) + '/' + str(number_s), end='\\r')\n",
    "\n",
    "    for index in range(number_countries):\n",
    "        country = country_list[index]\n",
    "        print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "            \n",
    "        for i in range(number_w):\n",
    "            w = window_list[i]\n",
    "            dummy_crps_list = [] \n",
    "\n",
    "            for j in range(number_dataframes):\n",
    "                year = actual_years[j]\n",
    "                monthly_totals_actuals = country_actual_group_list[j].get_group(country)\n",
    "                true_obs = monthly_totals_actuals.iloc[s-3,0]\n",
    "\n",
    "                NB_prediction = baseline_country_predict_list[index]['prediction'][year].xs(w, level=\"window\")\n",
    "\n",
    "                crps = pscore(NB_prediction.loc[:,'fatalities'].to_numpy(),true_obs).compute()[0]\n",
    "                dummy_crps_list.append(crps)\n",
    "\n",
    "            baseline_crps_list[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_baseline_crps_dict = {'w':[],'CRPS':[]}\n",
    "v2_baseline_crps_list = [{'country_id': country, 'baseline': {'w':[],'CRPS':[]}} for country in country_list]\n",
    "v3_baseline_crps_list = [{'s':s,'w':[],'CRPS':[]} for s in s_prediction_list]\n",
    "\n",
    "## baseline v1---------------------------------------------------------------------------\n",
    "# loop over w\n",
    "for j in range(number_w):\n",
    "    w = window_list[j]\n",
    "    dummy_crps_v1_list = []\n",
    "    # loop over countries\n",
    "    for i in range(number_countries):\n",
    "        # loop over prediction windows s\n",
    "        for k in range(number_s):\n",
    "            dummy_crps_v1_list.append(baseline_crps_list[i]['baseline'][k]['CRPS'][j])\n",
    "    v1_baseline_crps_dict['w'].append(w)\n",
    "    v1_baseline_crps_dict['CRPS'].append(np.mean(dummy_crps_v1_list))\n",
    "\n",
    "v1_baseline_crps = pd.DataFrame(v1_baseline_crps_dict)\n",
    "v1_baseline_crps = v1_baseline_crps[v1_baseline_crps.CRPS == v1_baseline_crps.loc[:,'CRPS'].min()]\n",
    "v1_baseline_crps.set_index(pd.Index(range(len(v1_baseline_crps))), inplace=True)\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "## baseline v2----------------------------------------------------------------------------\n",
    "# list for baseline v2\n",
    "for i in range(number_countries):\n",
    "    for j in range(number_w):\n",
    "        w = window_list[j]\n",
    "        dummy_crps_v2_list = []\n",
    "        for k in range(number_s):\n",
    "            dummy_crps_v2_list.append(baseline_crps_list[i]['baseline'][k]['CRPS'][j])\n",
    "        v2_baseline_crps_list[i]['baseline']['w'].append(w)\n",
    "        v2_baseline_crps_list[i]['baseline']['CRPS'].append(np.mean(dummy_crps_v2_list))\n",
    "    \n",
    "# dataframe with the w that minimizes the CRPS for every country (v2)\n",
    "data_v2 = {\n",
    "    'country_id':[],\n",
    "    'w':[],\n",
    "    'CRPS':[]\n",
    "}\n",
    "for i in range(len(v2_baseline_crps_list)):\n",
    "    # get the index of the minimal CRPS value\n",
    "    min_index = v2_baseline_crps_list[i]['baseline']['CRPS'].index(min(v2_baseline_crps_list[i]['baseline']['CRPS']))\n",
    "    \n",
    "    # store values in dict\n",
    "    data_v2['country_id'].append(v2_baseline_crps_list[i]['country_id'])\n",
    "    data_v2['w'].append(v2_baseline_crps_list[i]['baseline']['w'][min_index])\n",
    "    data_v2['CRPS'].append(v2_baseline_crps_list[i]['baseline']['CRPS'][min_index])\n",
    "    \n",
    "v2_baseline_crps = pd.DataFrame(data_v2)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## baseline v3---------------------------------------------------------------------------\n",
    "for s_index in range(number_s):\n",
    "    dummy_crps_v3_list = []\n",
    "    s = s_prediction_list[s_index]\n",
    "    for w_index in range(number_w):\n",
    "        w = window_list[w_index]\n",
    "        for i in range(number_countries):\n",
    "            dummy_crps_v3_list.append(baseline_crps_list[i]['baseline'][s_index]['CRPS'][w_index])\n",
    "        v3_baseline_crps_list[s_index]['w'].append(w)\n",
    "        v3_baseline_crps_list[s_index]['CRPS'].append(np.mean(dummy_crps_v3_list))\n",
    "\n",
    "# dataframe with the w that minimize the CRPS for each prediction window s\n",
    "data_v3 = {\n",
    "    's':[],\n",
    "    'w':[],\n",
    "    'CRPS':[]\n",
    "}\n",
    "# length of the v3_baseline list is the number of prediction windows\n",
    "for i in range(len(v3_baseline_crps_list)):\n",
    "    s = s_prediction_list[i]\n",
    "    # get the index of the minimal CRPS value\n",
    "    min_index = v3_baseline_crps_list[i]['CRPS'].index(min(v3_baseline_crps_list[i]['CRPS']))\n",
    "\n",
    "    # store values in dict\n",
    "    data_v3['s'].append(s)\n",
    "    data_v3['w'].append(v3_baseline_crps_list[i]['w'][min_index])\n",
    "    data_v3['CRPS'].append(v3_baseline_crps_list[i]['CRPS'][min_index])\n",
    "\n",
    "v3_baseline_crps = pd.DataFrame(data_v3)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "## baseline v4---------------------------------------------------------------------------\n",
    "v4_baseline_crps = [{'country_id':country,\n",
    "                    's':[],\n",
    "                    'w':[],\n",
    "                    'CRPS':[]\n",
    "                    } for country in country_list]\n",
    "\n",
    "# loop over all countries\n",
    "for i in range(len(baseline_crps_list)):\n",
    "    # loop over all prediction windows\n",
    "    for s_index in range(number_s):\n",
    "        s = s_prediction_list[s_index]\n",
    "        # get the index of the minimal CRPS value\n",
    "        min_index = baseline_crps_list[i]['baseline'][s_index]['CRPS'].index(min(baseline_crps_list[i]['baseline'][s_index]['CRPS']))\n",
    "    \n",
    "        # store values in dict\n",
    "        v4_baseline_crps[i]['s'].append(s)\n",
    "        v4_baseline_crps[i]['w'].append(baseline_crps_list[i]['baseline'][s_index]['w'][min_index])\n",
    "        v4_baseline_crps[i]['CRPS'].append(baseline_crps_list[i]['baseline'][s_index]['CRPS'][min_index])\n",
    "\n",
    "    v4_baseline_crps[i] = pd.DataFrame(v4_baseline_crps[i])\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall CRPS\n",
      "baseline 1: 15.1751\n",
      "baseline 2: 13.7932\n",
      "baseline 3: 15.3875\n",
      "baseline 4: 12.9081\n"
     ]
    }
   ],
   "source": [
    "# calculation of the overall CRPS to compare the impact of the level of detail in modeling \n",
    "dummy_array_v4 = []\n",
    "for countryData in v4_baseline_crps:\n",
    "    dummy_array_v4.append(np.mean(countryData.loc[:,'CRPS']))\n",
    "\n",
    "print('Overall CRPS')\n",
    "print('baseline 1: ' + str(np.round(v1_baseline_crps.iloc[0,1], decimals = 4)))\n",
    "print('baseline 2: ' + str(np.round(np.mean(v2_baseline_crps.loc[:,'CRPS']), decimals = 4)))\n",
    "print('baseline 3: ' + str(np.round(np.mean(v3_baseline_crps.loc[:,'CRPS']), decimals = 4)))\n",
    "print('baseline 4: ' + str(np.round(np.mean(dummy_array_v4), decimals = 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseline_variables.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# save variables in joblib file\n",
    "dump([country_list, baseline_country_predict_list, baseline_crps_list, v1_baseline_crps_dict,\n",
    "      v2_baseline_crps_list, v3_baseline_crps_list,\n",
    "      v1_baseline_crps, v2_baseline_crps, v3_baseline_crps, v4_baseline_crps], 'baseline_variables.joblib')\n",
    "\n",
    "# load variables\n",
    "#geladene_variablen = load('baseline_variables.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
