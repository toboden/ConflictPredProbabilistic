{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import nbinom\n",
    "from scipy.stats import poisson\n",
    "import CRPS.CRPS as pscore\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # paths to the data\n",
    "    absolute_path = os.path.abspath('')\n",
    "    relative_path_features = \"data\\cm_features_to_oct\" + feature_years[i] + \".parquet\"\n",
    "    relative_path_actuals = \"data\\cm_actuals_\" + actual_years[i] + \".parquet\"\n",
    "\n",
    "    path_features = os.path.join(absolute_path, relative_path_features)\n",
    "    path_actuals = os.path.join(absolute_path, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "# function to check, if the last n months are in the dataset of a country,\n",
    "# other than that the last month of a country in the feature dataset has to be 3 months before the first actuals month!!\n",
    "def check_last_nMonths(n, country_id, yearindex):\n",
    "    country = country_feature_group_list[yearindex].get_group(country_id)\n",
    "\n",
    "    # reference month of the actual dataset\n",
    "    actual_month_list = actuals_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (actual_month_list[0] - 3) != country.index.get_level_values('month_id').unique().tolist()[-1]:\n",
    "        return False\n",
    "    else:\n",
    "        month_list = features_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "        last_month = month_list[-1] # equals the first month - 3 from the corresponding actuals dataset\n",
    "        first_month = month_list[0]\n",
    "\n",
    "        last_n_months = True\n",
    "\n",
    "        if last_month-n+1 < first_month:\n",
    "            last_n_months = False\n",
    "        else:\n",
    "            month_list = list(range(last_month-n+1, last_month+1))\n",
    "            \n",
    "            for month in month_list:\n",
    "                if month not in country.index.get_level_values('month_id'):\n",
    "                    last_n_months = False\n",
    "                    break\n",
    "\n",
    "        return last_n_months\n",
    "        #return True\n",
    "\n",
    "    \n",
    "\n",
    "## hier unnötig, da alle länder in jedem Datensatz sind, allerdings sind nicht alle Länder bis zum letzten Monat vorhanden-----\n",
    "# list of all countries that are present in all four datasets\n",
    "\"\"\" country_list = []\n",
    "for i in range(len(features_df_list)):\n",
    "    country_list.extend(features_df_list[i]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "unique_list = []\n",
    "\n",
    "for item in country_list:\n",
    "    if country_list.count(item) == 4:\n",
    "        unique_list.append(item)\n",
    "\n",
    "country_list = list(set(unique_list)) \"\"\"\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# because of the concatination only the last dataframe is used (later on the appended months are dropped for datasets before 2020)\n",
    "features_1990to2020_df = features_df_list[3]['data']\n",
    "country_list = features_1990to2020_df.index.get_level_values('country_id').unique().tolist()\n",
    "\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))\n",
    "\n",
    "# same reason as mentioned two lines earlier\n",
    "country_feature_group_1990to2020 = country_feature_group_list[3]\n",
    "\n",
    "\n",
    "print(len(country_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#country_feature_group_list[0].get_group(191)\n",
    "#country_feature_group_list[3].get_group(246).index.get_level_values('month_id').unique().tolist()\n",
    "#country_actual_group_list[0].get_group(246).index.get_level_values('month_id').unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_last_nMonths(112, 246, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the predictions for each country\n",
    "baseline_country_predict_list = [{'country_id': country, 'prediction': {'2018': [], '2019': [], '2020': [], '2021': []}} for country in country_list]\n",
    "index_list = ['2018', '2019', '2020', '2021']\n",
    "# list of the (prediction) windows\n",
    "#window_list = list(range(2, 37))\n",
    "s_prediction_list = list(range(3, 15))\n",
    "\n",
    "\n",
    "\n",
    "## changes, so that the calculation does not take a long time -------------------\n",
    "#shorter windows\n",
    "window_list = list(range(2, 25))\n",
    "# remove all but ten countries\n",
    "elements_to_remove = country_list[0:(len(country_list)-10)]\n",
    "country_list = [element for element in country_list if element not in elements_to_remove]\n",
    "\n",
    "baseline_country_predict_list = [{'country_id': country, 'prediction': {'2018': [], '2019': []}} for country in country_list]\n",
    "actual_years = ['2021']\n",
    "index_list = ['2021']\n",
    "##----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toOct2020_monthlist = features_df_list[3]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "numberMonths_toOct2020 = len(toOct2020_monthlist)\n",
    "toOct2020_monthlist[-1]\n",
    "numberMonths_toOct2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify country_list so that it contains only country_ids \n",
    "# that have the last n months of observations in the last dataset (2020)!\n",
    "numberMonths_toOct20 = 96 # 96 = 5*12 (5 jahre für 2017) + 3*12 (jedes Jahr 12 Monate mehr also 2020 8 Jahre)\n",
    "#-- note------\n",
    "# dataset 2020 is used, because of the structure of the other datasets.\n",
    "# 2020 is dataset 2019 with 12 additional rows (months) etc.\n",
    "# for the CRPS calculation  of the datasets != 2020 the last 12*x windows are deleted\n",
    "# this procedure is saving computation time\n",
    "#-------------\n",
    "\n",
    "\n",
    "#IMPORTANT\n",
    "#if you do not minimize over all countries but only the single countries, \n",
    "# it is sufficient to check if all countries contain the last month in the features dataset (this way you use the full information). \n",
    "# But you still have to check check_last_nMonths(len(countrymonths), countryIndex, 3), so that no month is missing in between.\n",
    "\n",
    "# => so currently not all information is used for each country\n",
    "\n",
    "dummy_list = []\n",
    "for countryIndex in country_list:\n",
    "    dummy_hasLastN_months = True\n",
    "\n",
    "    # index 3 is the last dataset\n",
    "    # 76, da Land 246 z.b. genau die letzten 112 Monate (in '2020') als Beobachtungen hat \n",
    "    if check_last_nMonths(numberMonths_toOct20, countryIndex, 3) is not True:\n",
    "        dummy_hasLastN_months = False  \n",
    "    \n",
    "    if dummy_hasLastN_months is True:\n",
    "        dummy_list.append(countryIndex)\n",
    "\n",
    "# the values in country_list are the 'country_id'\n",
    "country_list = dummy_list\n",
    "\n",
    "#IMPORTANT\n",
    "# all countries that have the last month as observation have the last 96 months as observations (in 2020)!!! so no country is excluded\n",
    "# checked by modifing the check_last_nMonths function -> else: return True\n",
    "\n",
    "len(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country 10/10\r"
     ]
    }
   ],
   "source": [
    "number_countries = len(country_list)\n",
    "number_dataframes = len(actual_years)\n",
    "number_w = len(window_list)\n",
    "\n",
    "#calculate the number of subsets, that are used to estimate the distribution and validate it via 12 months of actuals \n",
    "# the number is set with the maximal w (e.g. 24): if w=24, actuals are 12 months (starting with s=3-s=14) \n",
    "# -> 24 + 3 + 12 = 39 observations of ged_sb per window\n",
    "# example: if the dataset has 120 observations there are 120 - 39 + 1 = 82 shiftable windows\n",
    "numberWindows = numberMonths_toOct20 - (number_w + 3 + 12) + 1\n",
    "\n",
    "\n",
    "\n",
    "# loop through all countries (that are present in each dataset)\n",
    "for index in range(number_countries):\n",
    "    country = country_list[index]\n",
    "\n",
    "    print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "\n",
    "    # list to store the predictions for each year temporally\n",
    "    baseline_predict_list = [[] for _ in range(number_dataframes)]\n",
    "    \n",
    "    # loop through datasets\n",
    "    for i in range(number_dataframes): \n",
    "        features = country_feature_group_list[i].get_group(country) # features of country in dataset i\n",
    "        \n",
    "        baseline_predict_list[i] = []\n",
    "\n",
    "        quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "        quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "\n",
    "        dummy_quantile_list = [f\"{round(q * 100, 1)}%\" for q in quantiles]\n",
    "\n",
    "        # calculate number of parts \n",
    "\n",
    "        # loop through windows\n",
    "        for j in range(number_w):    \n",
    "\n",
    "            w = window_list[j] # current window\n",
    "            baseline_predict_list[i].append({'window':w, 'predictions':[]})\n",
    "\n",
    "\n",
    "\n",
    "            # loop through all X equal parts of the feature dataset (traindata length w, actuals is vector of the next t+3 till t+12 observations)\n",
    "\n",
    "            baseline_predict_list[i]['predictions'].append([{'country_id':country, 'w':w, 'dist':dist_string, \n",
    "                                                             'mean':mean, 'var':var, 'last_month_id':0, \n",
    "                                                             'quantile':[], 'fatalities':[]}, \n",
    "                                                             [{'s':[], 'unreal_actuals':[]}]])\n",
    "\n",
    "            {'window':w, 'country_id':country, 'dist':dist_string, \n",
    "                                             'mean':mean, 'var':var, 'quantile':[], 'fatalities':[]}\n",
    "\n",
    "            # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(features.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(features.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "            #hier verteilung = nbinom ppf als\n",
    "            dummy_fatalities_list = []\n",
    "\n",
    "            # string to store distribution\n",
    "            dist_string = ''\n",
    "\n",
    "            if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "            elif var != 0 and var <= mean:\n",
    "                    dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "\n",
    "                    dist_string = 'Pois'\n",
    "\n",
    "            else:\n",
    "                    dummy_fatalities_list = [0] * 999\n",
    "                    dist_string = 'None'\n",
    "\n",
    "            baseline_predict_list[i].append({'window':w, 'country_id':country, 'dist':dist_string, \n",
    "                                             'mean':mean, 'var':var, 'quantile':[], 'fatalities':[]}) \n",
    "\n",
    "            baseline_predict_list[i][j]['quantile'] = dummy_quantile_list    \n",
    "            baseline_predict_list[i][j]['fatalities'] = dummy_fatalities_list\n",
    "\n",
    "            baseline_predict_list[i][j] = pd.DataFrame(baseline_predict_list[i][j])\n",
    "            baseline_predict_list[i][j].set_index(['window', 'quantile'], inplace=True)\n",
    "\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]] = baseline_predict_list[i]\n",
    "\n",
    "        # combine each w dataset together\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]] = pd.concat(baseline_country_predict_list[index]['prediction'][index_list[i]], axis=0)\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]].sort_index(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>dist</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>fatalities</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.5%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.6%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.7%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.8%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.9%</th>\n",
       "      <td>246</td>\n",
       "      <td>NBinom</td>\n",
       "      <td>33.416667</td>\n",
       "      <td>1375.537879</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country_id    dist       mean          var  fatalities\n",
       "quantile                                                        \n",
       "0.1%             246  NBinom  33.416667  1375.537879         0.0\n",
       "0.2%             246  NBinom  33.416667  1375.537879         0.0\n",
       "0.3%             246  NBinom  33.416667  1375.537879         0.0\n",
       "0.4%             246  NBinom  33.416667  1375.537879         0.0\n",
       "0.5%             246  NBinom  33.416667  1375.537879         0.0\n",
       "...              ...     ...        ...          ...         ...\n",
       "99.5%            246  NBinom  33.416667  1375.537879       198.0\n",
       "99.6%            246  NBinom  33.416667  1375.537879       207.0\n",
       "99.7%            246  NBinom  33.416667  1375.537879       218.0\n",
       "99.8%            246  NBinom  33.416667  1375.537879       234.0\n",
       "99.9%            246  NBinom  33.416667  1375.537879       262.0\n",
       "\n",
       "[999 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_country_predict_list[9]['prediction']['2018'].xs(12, level = 'window')\n",
    "#baseline_country_predict_list[9]['prediction']['2018']['window'][12] = liste von dicts mit verteilung und actual realisationen (vektor der länge 12!) aus features\n",
    "# liste hat die länge floor(len(featuresdata)/max window w + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country 10/10     prediction window 12/12\r"
     ]
    }
   ],
   "source": [
    "# list to store all crps values\n",
    "baseline_crps_list = [\n",
    "    {\n",
    "        'country_id': country,\n",
    "        'baseline': [\n",
    "            {'s': s, 'w': [], 'CRPS': []}\n",
    "            for s in s_prediction_list\n",
    "        ]\n",
    "    }\n",
    "    for country in country_list\n",
    "]\n",
    "\n",
    "# numver of prediction windows\n",
    "number_s = len(s_prediction_list)\n",
    "\n",
    "# fill list with crps calculations\n",
    "for s in s_prediction_list:\n",
    "    print('                  prediction window ' + str(s-2) + '/' + str(number_s), end='\\r')\n",
    "\n",
    "    for index in range(number_countries):\n",
    "        country = country_list[index]\n",
    "        print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "            \n",
    "        for i in range(number_w):\n",
    "            w = window_list[i]\n",
    "            dummy_crps_list = [] \n",
    "\n",
    "            for j in range(number_dataframes):\n",
    "                year = actual_years[j]\n",
    "                monthly_totals_actuals = country_actual_group_list[j].get_group(country)\n",
    "                true_obs = monthly_totals_actuals.iloc[s-3,0]\n",
    "\n",
    "                NB_prediction = baseline_country_predict_list[index]['prediction'][year].xs(w, level=\"window\")\n",
    "\n",
    "                crps = pscore(NB_prediction.loc[:,'fatalities'].to_numpy(),true_obs).compute()[0]\n",
    "                dummy_crps_list.append(crps)\n",
    "\n",
    "            baseline_crps_list[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list))\n",
    "    \n",
    "# time to calculate: ~66 min with all 190 countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
