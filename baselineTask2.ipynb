{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import nbinom, poisson\n",
    "\n",
    "## functions for the distribtion models\n",
    "# truncated negative binomial---------------------------------------------------\n",
    "def truncNegBin_CDF(y, n, p):\n",
    "    f_zero = nbinom.pmf(0, n, p)\n",
    "    if y > 0:\n",
    "        return (nbinom.cdf(y, n, p) - nbinom.cdf(0, n, p)) / (1 - f_zero)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def truncNegBin_PPF(x, n, p, epsilon=1e-6, max_iterations=100):\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if (1 - nbinom.pmf(0, n, p)) == 1:\n",
    "        return nbinom.ppf(x, n, p)\n",
    "    else:\n",
    "        # Define the range of y where the solution might exist\n",
    "        lower_bound = 0\n",
    "        upper_bound = 1000000000  # Adjust this based on the expected range of y\n",
    "\n",
    "        # Bisection method\n",
    "        for _ in range(max_iterations):\n",
    "            y = (lower_bound + upper_bound) / 2\n",
    "            cdf_value = truncNegBin_CDF(y, n, p)\n",
    "\n",
    "            if abs(cdf_value - x) < epsilon:\n",
    "                return np.ceil(y)  # Found a good approximation\n",
    "\n",
    "            if cdf_value < x:\n",
    "                lower_bound = y\n",
    "            else:\n",
    "                upper_bound = y\n",
    "\n",
    "        # Return the best approximation if max_iterations is reached\n",
    "        return np.ceil(y)\n",
    "\n",
    "def calculate_trunc_nbinom_quantile(quantile, n, p):\n",
    "    return truncNegBin_PPF(quantile, n, p)\n",
    "\n",
    "# truncated poisson---------------------------------------------------\n",
    "def truncPois_CDF(y, mu):\n",
    "    f_zero = poisson.pmf(0, mu)\n",
    "    if y > 0:\n",
    "        return (poisson.cdf(y, mu) - poisson.cdf(0, mu)) / (1 - f_zero)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def truncPois_PPF(x, mu, epsilon=1e-6, max_iterations=100):\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if (1 - poisson.pmf(0, mu)) == 1:\n",
    "        return poisson.ppf(x, mu)\n",
    "    else:\n",
    "        # Define the range of y where the solution might exist\n",
    "        lower_bound = 0\n",
    "        upper_bound = 1000000000  # Adjust this based on the expected range of y\n",
    "\n",
    "        # Bisection method\n",
    "        for _ in range(max_iterations):\n",
    "            y = (lower_bound + upper_bound) / 2\n",
    "            cdf_value = truncPois_CDF(y, mu)\n",
    "\n",
    "            if abs(cdf_value - x) < epsilon:\n",
    "                return np.ceil(y)  # Found a good approximation\n",
    "\n",
    "            if cdf_value < x:\n",
    "                lower_bound = y\n",
    "            else:\n",
    "                upper_bound = y\n",
    "\n",
    "        # Return the best approximation if max_iterations is reached\n",
    "        return np.ceil(y)\n",
    "\n",
    "def calculate_trunc_pois_quantile(quantile, mu):\n",
    "    return truncPois_PPF(quantile, mu)\n",
    "\n",
    "### function to compute distribution-------------------------------------------------\n",
    "def baseFatalModel_quantiles(featureSeries, quantiles, w=None, model='hurdle'):\n",
    "    # list to store quantiles \n",
    "    dummy_fatalities_list = []\n",
    "    # string to store model distribution\n",
    "    dist_string = ''\n",
    "\n",
    "    mean = None\n",
    "    var = None\n",
    "\n",
    "    # hurdle model\n",
    "    if model == 'hurdle':\n",
    "        if w == None:\n",
    "            \n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (featureSeries.value_counts().get(0, 0) / featureSeries.count())\n",
    "            # calculate n (r) and p via average/variance without the zero values\n",
    "            mean = pd.Series.mean(featureSeries[featureSeries != 0])\n",
    "            var = pd.Series.var(featureSeries[featureSeries != 0])\n",
    "\n",
    "        elif w <= 0:\n",
    "            return 'w has to be > 0'\n",
    "        \n",
    "        else:\n",
    "            features = featureSeries.tail(w).loc[:,'ged_sb']\n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (features.value_counts().get(0, 0) / features.count())\n",
    "            # calculate n (r) and p via average/variance without the zero values\n",
    "            mean = pd.Series.mean(features[features != 0])\n",
    "            var = pd.Series.var(features[features != 0])\n",
    "\n",
    "        # pd.Series.var or mean returns Nan in case of a passed series of length 1\n",
    "        if np.isnan(mean):\n",
    "            mean = 0\n",
    "        if np.isnan(var):\n",
    "            var = 0\n",
    "\n",
    "        # check if there are values above zero, otherwise no second component (trunc dist.) needed\n",
    "        if p_t > 0:\n",
    "            # component 1, y=0: Bernoulli\n",
    "            comp2_quantiles = [q for q in quantiles if q > p_t] #quantiles for the second component\n",
    "            removed_elements_length = len(quantiles)-len(comp2_quantiles)\n",
    "            zeros_array = np.zeros(removed_elements_length) #zero values that originate from the bernoulli dist\n",
    "\n",
    "            # component 2, y>0\n",
    "            if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                trunc_nbinom_quantiles = Parallel(n_jobs=-1)(delayed(calculate_trunc_nbinom_quantile)(quantile, n, p) for quantile in comp2_quantiles) #fast way\n",
    "                trunc_nbinom_quantiles = np.array(trunc_nbinom_quantiles)\n",
    "\n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_nbinom_quantiles)).tolist()\n",
    "                dist_string = 'BernoulliTruncNBinom'\n",
    "\n",
    "            elif var != 0 and var <= mean:\n",
    "                trunc_pois_quantiles = Parallel(n_jobs=-1)(delayed(calculate_trunc_pois_quantile)(quantile, mean) for quantile in comp2_quantiles) #fast way\n",
    "                trunc_pois_quantiles = np.array(trunc_pois_quantiles)\n",
    "                \n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_pois_quantiles)).tolist()\n",
    "                dist_string = 'BernoulliTruncPois'\n",
    "            \n",
    "        # p_t = 0 so no second component is needed    \n",
    "        else:\n",
    "            dummy_fatalities_list = [0] * 999\n",
    "            dist_string = 'BernoulliHurdle'\n",
    "\n",
    "    # nbinom model\n",
    "    elif model == 'nbinom':\n",
    "        if w == None:\n",
    "             # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries)\n",
    "            var = pd.Series.var(featureSeries)\n",
    "        elif w <= 0:\n",
    "            return 'w has to be > 0'\n",
    "        else:\n",
    "            # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "        if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "        elif var != 0 and var <= mean:\n",
    "                dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "        else:\n",
    "                dummy_fatalities_list = [0] * 999\n",
    "                dist_string = 'Bernoulli'\n",
    "\n",
    "    return {'fatalities': dummy_fatalities_list, 'dist': dist_string, 'mean': mean, 'var': var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # paths to the data\n",
    "    absolute_path = os.path.abspath('')\n",
    "    relative_path_features = \"data\\cm_features_to_oct\" + feature_years[i] + \".parquet\"\n",
    "    relative_path_actuals = \"data\\cm_actuals_\" + actual_years[i] + \".parquet\"\n",
    "\n",
    "    path_features = os.path.join(absolute_path, relative_path_features)\n",
    "    path_actuals = os.path.join(absolute_path, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "# function to check, if the last n months are in the dataset of a country,\n",
    "# other than that the last month of a country in the feature dataset has to be 3 months before the first actuals month!!\n",
    "def check_last_nMonths(n, country_id, yearindex):\n",
    "    country = country_feature_group_list[yearindex].get_group(country_id)\n",
    "\n",
    "    # reference month of the actual dataset\n",
    "    actual_month_list = actuals_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (actual_month_list[0] - 3) != country.index.get_level_values('month_id').unique().tolist()[-1]:\n",
    "        return False\n",
    "    else:\n",
    "        month_list = features_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "        last_month = month_list[-1] # equals the first month - 3 from the corresponding actuals dataset\n",
    "        first_month = month_list[0]\n",
    "\n",
    "        last_n_months = True\n",
    "\n",
    "        if last_month-n+1 < first_month:\n",
    "            last_n_months = False\n",
    "        else:\n",
    "            month_list = list(range(last_month-n+1, last_month+1))\n",
    "            \n",
    "            for month in month_list:\n",
    "                if month not in country.index.get_level_values('month_id'):\n",
    "                    last_n_months = False\n",
    "                    break\n",
    "\n",
    "        return last_n_months\n",
    "        #return True\n",
    "\n",
    "\"\"\" def nBinom_quantiles(featureSeries, w, quantiles):\n",
    "        if w == 'None':\n",
    "             # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries)\n",
    "            var = pd.Series.var(featureSeries)\n",
    "        else:\n",
    "            # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "        #hier verteilung = nbinom ppf als\n",
    "        dummy_fatalities_list = []\n",
    "\n",
    "        # string to store distribution\n",
    "        dist_string = ''\n",
    "\n",
    "        if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "        elif var != 0 and var <= mean:\n",
    "                dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "        else:\n",
    "                dummy_fatalities_list = [0] * 999\n",
    "                dist_string = 'None'\n",
    "\n",
    "        return {'fatalities': dummy_fatalities_list, 'dist': dist_string, 'mean': mean, 'var': var} \"\"\"\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# because of the concatination only the last dataframe is used (later on the appended months are dropped for datasets before 2020)\n",
    "\n",
    "# IF THIS CODE IS USED FOR THE 2024 PREDICTION ADJUST THE features_1990to2020 in the whole file to 1990to2023\n",
    "\n",
    "features_1990to2020_df = features_df_list[3]['data']\n",
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))\n",
    "\n",
    "\"\"\" # same reason as mentioned two lines earlier\n",
    "country_feature_group_1990to2020 = country_feature_group_list[3] \"\"\"\n",
    "\n",
    "\n",
    "print(len(country_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify country_list so that it contains only country_ids \n",
    "# that have at least the last n months of observations in the last dataset (2020)!\n",
    "numberMonths_toOct20 = 96 # 96 = 5*12 (5 jahre für 2017) + 3*12 (jedes Jahr 12 Monate mehr also 2020 8 Jahre)\n",
    "#ABER: um konsistent zu bleiben wird für jedes Jahr (jeden to_octX Datensatz) nur die letzten 5 Jahre verwendet!!!\n",
    "\n",
    "#-- note------\n",
    "# dataset 2020 is used, because of the structure of the other datasets.\n",
    "# 2020 is dataset 2019 with 12 additional rows (months) etc.\n",
    "# for the CRPS calculation  of the datasets != 2020 the last 12*x windows are deleted\n",
    "# this procedure is saving computation time\n",
    "#-------------\n",
    "\n",
    "\n",
    "#IMPORTANT\n",
    "#if you do not minimize over all countries but only the single countries, \n",
    "# it is sufficient to check if all countries contain the last month in the features dataset (this way you use the full information). \n",
    "# But you still have to check check_last_nMonths(len(countrymonths), countryIndex, 3), so that no month is missing in between.\n",
    "\n",
    "# => so currently not all information is used for each country\n",
    "\n",
    "dummy_list = []\n",
    "for countryIndex in country_list:\n",
    "    dummy_hasLastN_months = True\n",
    "\n",
    "    # index 3 is the last dataset\n",
    "    # 76, da Land 246 z.b. genau die letzten 112 Monate (in '2020') als Beobachtungen hat \n",
    "    if check_last_nMonths(numberMonths_toOct20, countryIndex, 3) is not True:\n",
    "        dummy_hasLastN_months = False  \n",
    "    \n",
    "    if dummy_hasLastN_months is True:\n",
    "        dummy_list.append(countryIndex)\n",
    "\n",
    "# the values in country_list are the 'country_id'\n",
    "country_list = dummy_list\n",
    "\n",
    "#IMPORTANT\n",
    "# all countries that have the last month as observation have the last 96 months as observations (in 2020)!!! so no country is excluded\n",
    "# checked by modifing the check_last_nMonths function -> else: return True\n",
    "\n",
    "len(country_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1-4\n",
    "\n",
    "Optimize **w** (through the CRPS) regarding\n",
    "|            | datasets    | countries   | prediction windows |\n",
    "|------------|-------------|-------------|--------------------|\n",
    "| baseline 1 | all         | all         | all                |\n",
    "| baseline 2 | all         | inidvidual  | all                |\n",
    "| baseline 3 | all         | all         | individual         |\n",
    "| baseline 4 | all         | inidvidual  | individual         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model type\n",
    "estimModel = 'hurdle' #nbinom or hurdle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The minimization is based on calculating the quantiles for each country, w and year (of the four datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the (prediction) windows\n",
    "window_list = list(range(2, 37))\n",
    "s_prediction_list = list(range(3, 15))\n",
    "\n",
    "\n",
    "\n",
    "## changes, so that the calculation does not take a long time -------------------\n",
    "#shorter windows\n",
    "window_list = list(range(2, 13))\n",
    "# remove all but ten countries\n",
    "elements_to_remove = country_list[0:(len(country_list)-1)] # only country 246\n",
    "country_list = [element for element in country_list if element not in elements_to_remove]\n",
    "len(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_countries = len(country_list)\n",
    "number_dataframes = len(actual_years)\n",
    "number_w = len(window_list)\n",
    "\n",
    "# lists for the estimation of the distribution\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "string_quantile_list = [f\"{round(q * 100, 1)}%\" for q in quantiles] # sting values of the quantiles\n",
    "\n",
    "# last month of the dataframe as reference for the moving prediction windows\n",
    "last_month = features_df_list[3]['data'].index.get_level_values('month_id').tolist()[-1]\n",
    "\n",
    "# list to store the estimations/predictions for each w\n",
    "baseline_estimate_list = []\n",
    "\n",
    "# loop through windows\n",
    "for i in range(number_w):    \n",
    "\n",
    "    print('                              window ' + str(i+1) + '/' + str(number_w)  , end='\\r')\n",
    "\n",
    "    w = window_list[i] # current window\n",
    "    baseline_estimate_list.append({'window':w, \n",
    "                                'country_predict_list':[{'country_id':country, 'predictionWindowsN':[]} for country in country_list]})\n",
    "    \n",
    "    #calculate the number of subsets, that are used to estimate the distribution and validate it via 12 months of actuals \n",
    "    # the number is dependent of the actual w. E.g. with the maximal w (e.g. 24): if w=24, actuals are 12 months (starting with s=3 to s=14) \n",
    "    # -> 24 + 2 + 12 = 39 observations of ged_sb per window\n",
    "    # so if the dataset has 96 observations there are 96 - 38 = 58 shiftable windows for 2020\n",
    "    numberWindows = numberMonths_toOct20 - (w + 2 + 12)\n",
    "\n",
    "    windowLength = w + 2 + 12 # length of the individual window for the current w\n",
    "    \n",
    "    # loop through all countries\n",
    "    for index in range(number_countries):\n",
    "        country = country_list[index]\n",
    "    \n",
    "        print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "\n",
    "        features = country_feature_group_list[3].get_group(country) # features of country\n",
    "        \n",
    "\n",
    "        # loop through all X equal parts of the feature dataset (traindata length w, actuals is vector of the next t+3 till t+12 observations)\n",
    "        for j in range(numberWindows):\n",
    "            starting_month_window = last_month - windowLength + 1 - numberWindows + 1  + j\n",
    "            ending_month_window = starting_month_window + w - 1\n",
    "\n",
    "            starting_month_actuals = ending_month_window + 3\n",
    "            ending_month_actuals = starting_month_actuals + 11\n",
    "            \n",
    "            window_features = features.loc[(slice(starting_month_window, ending_month_window), slice(None)), 'ged_sb']\n",
    "            window_actuals = features.loc[(slice(starting_month_actuals, ending_month_actuals), slice(None)), 'ged_sb']\n",
    "\n",
    "            #predict = nBinom_quantiles(window_features, 'None', quantiles)\n",
    "            predict = baseFatalModel_quantiles(window_features, quantiles, model=estimModel)\n",
    "            \n",
    "            baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'].append(\n",
    "                [{'country_id':country, 'w':w, 'dist':predict['dist'], \n",
    "                'mean':predict['mean'], 'var':predict['var'], 'first_month_feature':starting_month_window, \n",
    "                'quantile':string_quantile_list, 'fatalities':predict['fatalities']}, \n",
    "                {'s':s_prediction_list, \n",
    "                    'month_id': window_actuals.index.get_level_values('month_id'),\n",
    "                    'unreal_actuals':window_actuals.values}])\n",
    "\n",
    "# with 10 countries 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                     w                        country         rollingWindow|predicton/actuals\n",
    "baseline_estimate_list[0]['country_predict_list'][0]['predictionWindowsN'][11][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the average over all indiviual moving windows per w and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store all crps values\n",
    "baseline_crps_list_to_oct20 = [\n",
    "    {\n",
    "        'country_id': country,\n",
    "        'baseline': [\n",
    "            {'s': s, 'w': [], 'CRPS': []}\n",
    "            for s in s_prediction_list\n",
    "        ]\n",
    "    }\n",
    "    for country in country_list\n",
    "]\n",
    "baseline_crps_list_to_oct19 = copy.deepcopy(baseline_crps_list_to_oct20)\n",
    "baseline_crps_list_to_oct18 = copy.deepcopy(baseline_crps_list_to_oct20)\n",
    "baseline_crps_list_to_oct17 = copy.deepcopy(baseline_crps_list_to_oct20)\n",
    "\n",
    "# number of prediction windows\n",
    "number_s = len(s_prediction_list)\n",
    "\n",
    "# fill lists with crps calculations\n",
    "for s in s_prediction_list:\n",
    "    print('                  prediction window ' + str(s-2) + '/' + str(number_s), end='\\r')\n",
    "\n",
    "    for index in range(number_countries):\n",
    "        country = country_list[index]\n",
    "        print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "            \n",
    "        for i in range(number_w):\n",
    "            w = window_list[i]\n",
    "            dummy_crps_list = [] \n",
    "\n",
    "            # loop over all subset windows of the country and w \n",
    "            for j in range(len(baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'])):\n",
    "\n",
    "                distribution = baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'][j][0]['fatalities']\n",
    "                actual = baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'][j][1]['unreal_actuals'][s-3]\n",
    "\n",
    "                crps = pscore(np.array(distribution),actual).compute()[0]\n",
    "                dummy_crps_list.append(crps)\n",
    "\n",
    "            # dataframe to_oct17\n",
    "            baseline_crps_list_to_oct17[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct17[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[:-(3*12)]))\n",
    "\n",
    "            # dataframe to_oct18\n",
    "            baseline_crps_list_to_oct18[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct18[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[12:-(2*12)]))\n",
    "\n",
    "            # dataframe to_oct19\n",
    "            baseline_crps_list_to_oct19[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct19[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[(2*12):-12]))\n",
    "\n",
    "            # dataframe to_oct20\n",
    "            baseline_crps_list_to_oct20[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_to_oct20[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list[(3*12):]))\n",
    "\n",
    "\n",
    "task2_baseline_list = [baseline_crps_list_to_oct17, baseline_crps_list_to_oct18,\n",
    "                       baseline_crps_list_to_oct19, baseline_crps_list_to_oct20]\n",
    "\n",
    "# with 10 countries 18m\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimization\n",
    "'w_minimization_list' contains the minimal w's for the different baselines for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the results of the minimal w's\n",
    "w_minimization_list = [{'predictionYear':year, 'minWData':[]} for year in actual_years]\n",
    "\n",
    "# list to store the list to compute the minimal w's\n",
    "w_compute_list = [{'predictionYear':year, 'data':[]} for year in actual_years]\n",
    "\n",
    "# loop over the four different datasets to predict (18-21)\n",
    "for task2_index in range(len(task2_baseline_list)):\n",
    "    v1_baseline_crps_dict = {'w':[],'CRPS':[]}\n",
    "    v2_baseline_crps_list = [{'country_id': country, 'baseline': {'w':[],'CRPS':[]}} for country in country_list]\n",
    "    v3_baseline_crps_list = [{'s':s,'w':[],'CRPS':[]} for s in s_prediction_list]\n",
    "\n",
    "    ## baseline v1---------------------------------------------------------------------------\n",
    "    # loop over w\n",
    "    for j in range(number_w):\n",
    "        w = window_list[j]\n",
    "        dummy_crps_v1_list = []\n",
    "        # loop over countries\n",
    "        for i in range(number_countries):\n",
    "            # loop over prediction windows s\n",
    "            for k in range(number_s):\n",
    "                dummy_crps_v1_list.append(task2_baseline_list[task2_index][i]['baseline'][k]['CRPS'][j])\n",
    "        v1_baseline_crps_dict['w'].append(w)\n",
    "        v1_baseline_crps_dict['CRPS'].append(np.mean(dummy_crps_v1_list))\n",
    "\n",
    "    v1_baseline_crps = pd.DataFrame(v1_baseline_crps_dict)\n",
    "\n",
    "    w_compute_list[task2_index]['data'].append(v1_baseline_crps)\n",
    "\n",
    "    v1_baseline_crps = v1_baseline_crps[v1_baseline_crps.CRPS == v1_baseline_crps.loc[:,'CRPS'].min()]\n",
    "    v1_baseline_crps.set_index(pd.Index(range(len(v1_baseline_crps))), inplace=True)\n",
    "        \n",
    "    w_minimization_list[task2_index]['minWData'].append(v1_baseline_crps)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    ## baseline v2----------------------------------------------------------------------------\n",
    "    # list for baseline v2\n",
    "    for i in range(number_countries):\n",
    "        for j in range(number_w):\n",
    "            w = window_list[j]\n",
    "            dummy_crps_v2_list = []\n",
    "            for k in range(number_s):\n",
    "                dummy_crps_v2_list.append(task2_baseline_list[task2_index][i]['baseline'][k]['CRPS'][j])\n",
    "            v2_baseline_crps_list[i]['baseline']['w'].append(w)\n",
    "            v2_baseline_crps_list[i]['baseline']['CRPS'].append(np.mean(dummy_crps_v2_list))\n",
    "        \n",
    "    # dataframe with the w that minimizes the CRPS for every country (v2)\n",
    "    data_v2 = {\n",
    "        'country_id':[],\n",
    "        'w':[],\n",
    "        'CRPS':[]\n",
    "    }\n",
    "    for i in range(len(v2_baseline_crps_list)):\n",
    "        # get the index of the minimal CRPS value\n",
    "        min_index = v2_baseline_crps_list[i]['baseline']['CRPS'].index(min(v2_baseline_crps_list[i]['baseline']['CRPS']))\n",
    "        \n",
    "        # store values in dict\n",
    "        data_v2['country_id'].append(v2_baseline_crps_list[i]['country_id'])\n",
    "        data_v2['w'].append(v2_baseline_crps_list[i]['baseline']['w'][min_index])\n",
    "        data_v2['CRPS'].append(v2_baseline_crps_list[i]['baseline']['CRPS'][min_index])\n",
    "        \n",
    "    v2_baseline_crps = pd.DataFrame(data_v2)\n",
    "    w_minimization_list[task2_index]['minWData'].append(v2_baseline_crps)\n",
    "    w_compute_list[task2_index]['data'].append(v2_baseline_crps_list)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## baseline v3---------------------------------------------------------------------------\n",
    "    for s_index in range(number_s):\n",
    "        dummy_crps_v3_list = []\n",
    "        s = s_prediction_list[s_index]\n",
    "        for w_index in range(number_w):\n",
    "            w = window_list[w_index]\n",
    "            for i in range(number_countries):\n",
    "                dummy_crps_v3_list.append(task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS'][w_index])\n",
    "            v3_baseline_crps_list[s_index]['w'].append(w)\n",
    "            v3_baseline_crps_list[s_index]['CRPS'].append(np.mean(dummy_crps_v3_list))\n",
    "\n",
    "    # dataframe with the w that minimize the CRPS for each prediction window s\n",
    "    data_v3 = {\n",
    "        's':[],\n",
    "        'w':[],\n",
    "        'CRPS':[]\n",
    "    }\n",
    "    # length of the v3_baseline list is the number of prediction windows\n",
    "    for i in range(len(v3_baseline_crps_list)):\n",
    "        s = s_prediction_list[i]\n",
    "        # get the index of the minimal CRPS value\n",
    "        min_index = v3_baseline_crps_list[i]['CRPS'].index(min(v3_baseline_crps_list[i]['CRPS']))\n",
    "\n",
    "        # store values in dict\n",
    "        data_v3['s'].append(s)\n",
    "        data_v3['w'].append(v3_baseline_crps_list[i]['w'][min_index])\n",
    "        data_v3['CRPS'].append(v3_baseline_crps_list[i]['CRPS'][min_index])\n",
    "\n",
    "    v3_baseline_crps = pd.DataFrame(data_v3)\n",
    "\n",
    "    w_minimization_list[task2_index]['minWData'].append(v3_baseline_crps)\n",
    "    w_compute_list[task2_index]['data'].append(v3_baseline_crps_list)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    ## baseline v4---------------------------------------------------------------------------\n",
    "    v4_baseline_crps = [{'country_id':country,\n",
    "                        's':[],\n",
    "                        'w':[],\n",
    "                        'CRPS':[]\n",
    "                        } for country in country_list]\n",
    "\n",
    "    # loop over all countries\n",
    "    for i in range(len(task2_baseline_list[task2_index])):\n",
    "        # loop over all prediction windows\n",
    "        for s_index in range(number_s):\n",
    "            s = s_prediction_list[s_index]\n",
    "            # get the index of the minimal CRPS value\n",
    "            min_index = task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS'].index(min(task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS']))\n",
    "        \n",
    "            # store values in dict\n",
    "            v4_baseline_crps[i]['s'].append(s)\n",
    "            v4_baseline_crps[i]['w'].append(task2_baseline_list[task2_index][i]['baseline'][s_index]['w'][min_index])\n",
    "            v4_baseline_crps[i]['CRPS'].append(task2_baseline_list[task2_index][i]['baseline'][s_index]['CRPS'][min_index])\n",
    "\n",
    "        v4_baseline_crps[i] = pd.DataFrame(v4_baseline_crps[i])\n",
    "\n",
    "    w_minimization_list[task2_index]['minWData'].append(v4_baseline_crps)\n",
    "    w_compute_list[task2_index]['data'].append(task2_baseline_list[task2_index])\n",
    "    #----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                dataset/year |baseline method|country (only baseline 4)\n",
    "w_minimization_list[0]['minWData'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with the estimated baseline models\n",
    "With the w values for the four different baselines computed above the prediction is calculated and evaluated in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the predictions for each country\n",
    "baseline_prediction_list = [[{'country_id': country, 'base': 1, 'prediction': {'2018': [], \n",
    "                                                                               '2019': [], \n",
    "                                                                               '2020': [], \n",
    "                                                                               '2021': []}} for country in country_list],\n",
    "                            [{'country_id': country, 'base': 2, 'prediction': {'2018': [], \n",
    "                                                                               '2019': [], \n",
    "                                                                               '2020': [], \n",
    "                                                                               '2021': []}} for country in country_list],\n",
    "                            [{'country_id': country, 'base': 3, 'prediction': {'2018': [], \n",
    "                                                                               '2019': [], \n",
    "                                                                               '2020': [], \n",
    "                                                                               '2021': []}} for country in country_list],\n",
    "                            [{'country_id': country, 'base': 4, 'prediction': {'2018': [], \n",
    "                                                                               '2019': [], \n",
    "                                                                               '2020': [], \n",
    "                                                                               '2021': []}} for country in country_list]]\n",
    "\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "string_quantile_list = [f\"{round(q * 100, 1)}%\" for q in quantiles]\n",
    "\n",
    "\n",
    "# loop through all countries (that are present in each dataset)\n",
    "for index in range(number_countries):\n",
    "    country = country_list[index]\n",
    "\n",
    "    # list to store the predictions for each year temporally\n",
    "    baseline_prediction = [[] for _ in range(number_dataframes)]\n",
    "    \n",
    "    # loop through datasets\n",
    "    for i in range(number_dataframes): \n",
    "        features = country_feature_group_list[i].get_group(country) # features of country in dataset i\n",
    "        actuals = country_actual_group_list[i].get_group(country) # actuals of country in dataset i\n",
    "\n",
    "        data_year = actual_years[i]\n",
    "\n",
    "        # loop over the four different baseline minimization methods\n",
    "        for j in range(len(w_minimization_list[i]['minWData'])):\n",
    "\n",
    "            # baseline 1\n",
    "            if j == 0:\n",
    "                w = w_minimization_list[i]['minWData'][j].loc[0,'w'] # use the w obtained by minimization on the feature dataset\n",
    "                #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "                fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "                dummy_crps_list = []\n",
    "                for s in s_prediction_list:\n",
    "                    true_obs = actuals.iloc[s-3,0] # true observation of the month s\n",
    "                    NB_prediction = fit['fatalities'] # value of the quantiles\n",
    "                    crps = pscore(np.array(NB_prediction),true_obs).compute()[0] # compute crps\n",
    "                    dummy_crps_list.append(crps)\n",
    "\n",
    "                baseline_prediction_list[j][index]['prediction'][data_year].append({'s':s_prediction_list, 'w':w, \n",
    "                                                                                    'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                    'var':fit['var'], \n",
    "                                                                                    'quantile':string_quantile_list, \n",
    "                                                                                    'fatalities':fit['fatalities'],\n",
    "                                                                                    'actual':actuals.iloc[:,0].tolist(),\n",
    "                                                                                    'CRPS':dummy_crps_list})\n",
    "\n",
    "            # baseline 2\n",
    "            elif j == 1:\n",
    "                if country == w_minimization_list[i]['minWData'][j].loc[index,'country_id']:\n",
    "                    w = w_minimization_list[i]['minWData'][j].loc[index,'w']\n",
    "                    #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "                    fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "                    dummy_crps_list = []\n",
    "                    for s in s_prediction_list:\n",
    "                        true_obs = actuals.iloc[s-3,0] # true observation of the month s\n",
    "                        NB_prediction = fit['fatalities'] # value of the quantiles\n",
    "                        crps = pscore(np.array(NB_prediction),true_obs).compute()[0] # compute crps\n",
    "                        dummy_crps_list.append(crps)\n",
    "\n",
    "                    baseline_prediction_list[j][index]['prediction'][data_year].append({'s':s_prediction_list, 'w':w, \n",
    "                                                                                        'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                        'var':fit['var'], \n",
    "                                                                                        'quantile':string_quantile_list, \n",
    "                                                                                        'fatalities':fit['fatalities'],\n",
    "                                                                                        'actual':actuals.iloc[:,0].tolist(),\n",
    "                                                                                        'CRPS':dummy_crps_list})\n",
    "                else:\n",
    "                    print('Stopp')\n",
    "                    break\n",
    "\n",
    "            # baseline 3\n",
    "            elif j == 2:\n",
    "                for s in s_prediction_list:\n",
    "                    w = w_minimization_list[i]['minWData'][j].loc[s-3,'w']\n",
    "                    #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "                    fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "                    true_obs = actuals.iloc[s-3,0] # true observation of the month s\n",
    "                    NB_prediction = fit['fatalities'] # value of the quantiles\n",
    "                    crps = pscore(np.array(NB_prediction),true_obs).compute()[0] # compute crps\n",
    "\n",
    "                    baseline_prediction_list[j][index]['prediction'][data_year].append({'s':s, 'w':w, \n",
    "                                                                                        'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                        'var':fit['var'], \n",
    "                                                                                        'quantile':string_quantile_list, \n",
    "                                                                                        'fatalities':fit['fatalities'],\n",
    "                                                                                        'actual':true_obs,\n",
    "                                                                                        'CRPS':crps})\n",
    "\n",
    "            # baseline 4\n",
    "            elif j == 3:\n",
    "                if country == w_minimization_list[i]['minWData'][j][index].loc[0,'country_id']:\n",
    "                    for s in s_prediction_list:\n",
    "                        w = w_minimization_list[i]['minWData'][j][index].loc[s-3,'w']\n",
    "                        #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "                        fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "                        true_obs = actuals.iloc[s-3,0] # true observation of the month s\n",
    "                        NB_prediction = fit['fatalities'] # value of the quantiles\n",
    "                        crps = pscore(np.array(NB_prediction),true_obs).compute()[0] # compute crps\n",
    "\n",
    "                        baseline_prediction_list[j][index]['prediction'][data_year].append({'s':s, 'w':w, \n",
    "                                                                                            'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                            'var':fit['var'], \n",
    "                                                                                            'quantile':string_quantile_list, \n",
    "                                                                                            'fatalities':fit['fatalities'],\n",
    "                                                                                            'actual':true_obs,\n",
    "                                                                                            'CRPS':crps})\n",
    "                else:\n",
    "                    print('Stopp')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          baseline method|country                  s-3 (only index 0 for baseline 1 and 2 due to the non minimizing s)\n",
    "baseline_prediction_list[2][0]['prediction']['2018'][11]['CRPS']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRPS averaged over all countries and years for each baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline1_average_crps_list = []\n",
    "baseline2_average_crps_list = []\n",
    "baseline3_average_crps_list = []\n",
    "baseline4_average_crps_list = []\n",
    "\n",
    "for i in range(number_dataframes):\n",
    "    year = actual_years[i]\n",
    "    for index in range(number_countries):\n",
    "        baseline1_average_crps_list.append(np.mean(baseline_prediction_list[0][index]['prediction'][year][0]['CRPS'])) #crps is stored as list and not individual values\n",
    "        baseline2_average_crps_list.append(np.mean(baseline_prediction_list[1][index]['prediction'][year][0]['CRPS'])) #crps is stored as list and not individual values\n",
    "\n",
    "        for s in s_prediction_list:\n",
    "            baseline3_average_crps_list.append(np.mean(baseline_prediction_list[2][index]['prediction'][year][s-3]['CRPS']))\n",
    "            baseline4_average_crps_list.append(np.mean(baseline_prediction_list[3][index]['prediction'][year][s-3]['CRPS']))\n",
    "\n",
    "baseline1_average_crps = np.mean(baseline1_average_crps_list)\n",
    "baseline2_average_crps = np.mean(baseline2_average_crps_list)\n",
    "baseline3_average_crps = np.mean(baseline3_average_crps_list)\n",
    "baseline4_average_crps = np.mean(baseline4_average_crps_list)\n",
    "\n",
    "print('Overall CRPS')\n",
    "print('baseline 1: ' + str(np.round(baseline1_average_crps, decimals = 4)))\n",
    "print('baseline 2: ' + str(np.round(baseline2_average_crps, decimals = 4)))\n",
    "print('baseline 3: ' + str(np.round(baseline3_average_crps, decimals = 4)))\n",
    "print('baseline 4: ' + str(np.round(baseline4_average_crps, decimals = 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variables in joblib file\n",
    "\"\"\" dump([country_list, baseline_estimate_list, task2_baseline_list, w_minimization_list, baseline_prediction_list,\n",
    "       baseline1_average_crps, baseline2_average_crps, baseline3_average_crps, baseline4_average_crps], \n",
    "       'task2_baseline_variables.joblib') \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
