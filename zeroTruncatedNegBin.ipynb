{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.664901396367409e-115\n",
      "3.664901396367399e-115\n",
      "3.664901396367431e-115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import nbinom\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def truncNegBin_PDF(y, n, p):\n",
    "    one_f_zero = float(1 - nbinom.pmf(0, n, p))\n",
    "    return nbinom.pmf(y, n, p) / one_f_zero\n",
    "\n",
    "def truncNegBin_CDF(y, n, p):\n",
    "    f_zero = nbinom.pmf(0, n, p)\n",
    "    if y > 0:\n",
    "        return (nbinom.cdf(y, n, p) - nbinom.cdf(0, n, p)) / (1 - f_zero)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def truncNegBin_CDF2(a,n,p):\n",
    "    p=float(p)\n",
    "    if a <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        pdf_array = np.array([truncNegBin_PDF(yi, n, p) for yi in range(1, a+1)])\n",
    "        return np.sum(pdf_array)\n",
    "\n",
    "def truncNegBin_PPF(x, n, p, epsilon=1e-6, max_iterations=100):\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if (1 - nbinom.pmf(0, n, p)) == 1:\n",
    "        return nbinom.ppf(x, n, p)\n",
    "    else:\n",
    "        # Define the range of y where the solution might exist\n",
    "        lower_bound = 0\n",
    "        upper_bound = 1000000000  # Adjust this based on the expected range of y\n",
    "\n",
    "        # Bisection method\n",
    "        for _ in range(max_iterations):\n",
    "            y = (lower_bound + upper_bound) / 2\n",
    "            cdf_value = truncNegBin_CDF(y, n, p)\n",
    "\n",
    "            if abs(cdf_value - x) < epsilon:\n",
    "                return np.ceil(y)  # Found a good approximation\n",
    "\n",
    "            if cdf_value < x:\n",
    "                lower_bound = y\n",
    "            else:\n",
    "                upper_bound = y\n",
    "\n",
    "        # Return the best approximation if max_iterations is reached\n",
    "        return np.ceil(y)\n",
    "\n",
    "def calculate_trunc_nbinom_quantile(quantile, n, p):\n",
    "    return truncNegBin_PPF(quantile, n, p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean = 1000\n",
    "var = 8000\n",
    "\n",
    "n = (mean**2) / (var - mean) # equivalent to r\n",
    "p = mean / var\n",
    "\n",
    "\n",
    "print(nbinom.cdf(10, n, p))\n",
    "print(truncNegBin_CDF(10, n, p))\n",
    "print(truncNegBin_CDF2(10, n, p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof that the inverse function leads to the same results as the naive way of calculating the quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.0\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import nbinom\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "mean = 100\n",
    "var = 200\n",
    "\n",
    "n = (mean**2) / (var - mean) # equivalent to r\n",
    "p = mean / var\n",
    "\n",
    "x = 0.001\n",
    "inverse_value = truncNegBin_PPF(x, n, p)\n",
    "print(inverse_value)\n",
    "\n",
    "y_values = range(1, int(inverse_value)+100)\n",
    "probabilities = np.array([truncNegBin_PDF(yi, n, p) for yi in y_values])\n",
    "cdf_array = np.cumsum(probabilities)\n",
    "quantile = np.argmax(cdf_array >= list([x])) + 1\n",
    "print(quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of quantiles\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "#trunc_nbinom_quantiles = np.array([truncNegBin_PPF(quantile, n, p) for quantile in quantiles]) #slow way\n",
    "trunc_nbinom_quantiles = Parallel(n_jobs=-1)(delayed(calculate_trunc_nbinom_quantile)(quantile, n, p) for quantile in quantiles) #fast way\n",
    "trunc_nbinom_quantiles = np.array(trunc_nbinom_quantiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_nbinom_quantiles[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hurdle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import nbinom\n",
    "from scipy.stats import poisson\n",
    "import CRPS.CRPS as pscore\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # paths to the data\n",
    "    absolute_path = os.path.abspath('')\n",
    "    relative_path_features = \"data\\cm_features_to_oct\" + feature_years[i] + \".parquet\"\n",
    "    relative_path_actuals = \"data\\cm_actuals_\" + actual_years[i] + \".parquet\"\n",
    "\n",
    "    path_features = os.path.join(absolute_path, relative_path_features)\n",
    "    path_actuals = os.path.join(absolute_path, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "# function to check, if the last n months are in the dataset of a country,\n",
    "# other than that the last month of a country in the feature dataset has to be 3 months before the first actuals month!!\n",
    "def check_last_nMonths(n, country_id, yearindex):\n",
    "    country = country_feature_group_list[yearindex].get_group(country_id)\n",
    "\n",
    "    # reference month of the actual dataset\n",
    "    actual_month_list = actuals_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (actual_month_list[0] - 3) != country.index.get_level_values('month_id').unique().tolist()[-1]:\n",
    "        return False\n",
    "    else:\n",
    "        month_list = features_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "        last_month = month_list[-1] # equals the first month - 3 from the corresponding actuals dataset\n",
    "        first_month = month_list[0]\n",
    "\n",
    "        last_n_months = True\n",
    "\n",
    "        if last_month-n+1 < first_month:\n",
    "            last_n_months = False\n",
    "        else:\n",
    "            month_list = list(range(last_month-n+1, last_month+1))\n",
    "            \n",
    "            for month in month_list:\n",
    "                if month not in country.index.get_level_values('month_id'):\n",
    "                    last_n_months = False\n",
    "                    break\n",
    "\n",
    "        return last_n_months\n",
    "        #return True\n",
    "\n",
    "def nBinom_quantiles(featureSeries, w, quantiles):\n",
    "        if w == 'None':\n",
    "             # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries)\n",
    "            var = pd.Series.var(featureSeries)\n",
    "        else:\n",
    "            # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "        #hier verteilung = nbinom ppf als\n",
    "        dummy_fatalities_list = []\n",
    "\n",
    "        # string to store distribution\n",
    "        dist_string = ''\n",
    "\n",
    "        if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "        elif var != 0 and var <= mean:\n",
    "                dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "        else:\n",
    "                dummy_fatalities_list = [0] * 999\n",
    "                dist_string = 'None'\n",
    "\n",
    "        return {'fatalities': dummy_fatalities_list, 'dist': dist_string, 'mean': mean, 'var': var}\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# because of the concatination only the last dataframe is used (later on the appended months are dropped for datasets before 2020)\n",
    "\n",
    "# IF THIS CODE IS USED FOR THE 2024 PREDICTION ADJUST THE features_1990to2020 in the whole file to 1990to2023\n",
    "\n",
    "features_1990to2020_df = features_df_list[3]['data']\n",
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurdleModel_quantiles(featureSeries, w=None, quantiles):\n",
    "    if w == None:\n",
    "        # calculate pt, i.e. the probability that y>0\n",
    "        p_t = 1 - (featureSeries.value_counts().get(0, 0) / featureSeries.count())\n",
    "        # calculate n (r) and p via average/variance\n",
    "\n",
    "\n",
    "        # mean und var aus ohne 0!!!!\n",
    "\n",
    "        mean = pd.Series.mean(featureSeries)\n",
    "        var = pd.Series.var(featureSeries)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif w <= 0:\n",
    "        return 'w has to be > 0'\n",
    "    else:\n",
    "        features\n",
    "        # calculate pt, i.e. the probability that y>0\n",
    "        p_t = 1 - (featureSeries.tail(w).loc[:,'ged_sb'].value_counts().get(0, 0) / featureSeries.tail(w).loc[:,'ged_sb'].count())\n",
    "        # calculate n (r) and p via average/variance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        mean = pd.Series.mean(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "        var = pd.Series.var(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #hier verteilung = nbinom ppf als\n",
    "\n",
    "    # list to store quantiles \n",
    "    dummy_fatalities_list = []\n",
    "\n",
    "    # string to store distribution\n",
    "    dist_string = ''\n",
    "\n",
    "    \n",
    "    if p_t > 0:\n",
    "        # component 1, y=0: Bernoulli\n",
    "        comp2_quantiles = [q for q in quantiles if q > p_t]\n",
    "        removed_elements_length = len(quantiles)-len(comp2_quantiles)\n",
    "        zeros_array = np.zeros(removed_elements_length)\n",
    "\n",
    "        # component 2, y>0\n",
    "        if var != 0 and var > mean:\n",
    "            n = (mean**2) / (var - mean) # equivalent to r\n",
    "            p = mean / var\n",
    "            trunc_nbinom_quantiles = Parallel(n_jobs=-1)(delayed(calculate_trunc_nbinom_quantile)(quantile, n, p) for quantile in quantiles) #fast way\n",
    "            trunc_nbinom_quantiles = np.array(trunc_nbinom_quantiles)\n",
    "\n",
    "            dummy_fatalities_list = np.concatenate((zeros_array, trunc_nbinom_quantiles)).tolist()\n",
    "            dist_string = 'BernoullitruncNBinom'\n",
    "\n",
    "        elif var != 0 and var <= mean:\n",
    "            dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "\n",
    "            dist_string = 'BernoullitruncPois'\n",
    "\n",
    "        # benötigt?!?!?!?!?!?!\n",
    "        else:\n",
    "            dummy_fatalities_list = [0] * 999\n",
    "            dist_string = 'None'\n",
    "\n",
    "    else:\n",
    "        dummy_fatalities_list = [0] * 999\n",
    "        dist_string = 'Bernoulli'\n",
    "\n",
    "    return {'fatalities': dummy_fatalities_list, 'dist': dist_string, 'mean': mean, 'var': var}\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Nullen: 0.05\n",
      "Wahrscheinlichkeit für > Null: 0.95\n"
     ]
    }
   ],
   "source": [
    "features = country_feature_group_list[0].get_group(246)\n",
    "\n",
    "w = 20\n",
    "\n",
    "# Schritt 3: Berechne die Wahrscheinlichkeit für eine Null\n",
    "probability_of_zero = features.tail(w).loc[:,'ged_sb'].value_counts().get(0, 0) / features.tail(w).loc[:,'ged_sb'].count()\n",
    "prob_of_bzero = 1-probability_of_zero\n",
    "\n",
    "# Gib das Ergebnis aus\n",
    "print(\"Wahrscheinlichkeit für Nullen:\", probability_of_zero)\n",
    "print(\"Wahrscheinlichkeit für > Null:\", prob_of_bzero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "mean = 100\n",
    "var = 200\n",
    "\n",
    "n = (mean**2) / (var - mean) # equivalent to r\n",
    "p = mean / var\n",
    "\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "orig_len_quant = len(quantiles)\n",
    "\n",
    "p_t = 0.3\n",
    "\n",
    "quantiles = [q for q in quantiles if q > p_t]\n",
    "\n",
    "removed_elements_length = orig_len_quant-len(quantiles)  # Länge der entfernten Elemente\n",
    "\n",
    "zeros_array = np.zeros(removed_elements_length)  # Numpy-Array mit Nullen\n",
    "\n",
    "trunc_nbinom_quantiles = Parallel(n_jobs=-1)(delayed(calculate_trunc_nbinom_quantile)(quantile, n, p) for quantile in quantiles) #fast way\n",
    "trunc_nbinom_quantiles = np.array(trunc_nbinom_quantiles)\n",
    "\n",
    "dummy_fatalities_list = np.concatenate((zeros_array, trunc_nbinom_quantiles)).tolist()\n",
    "\n",
    "len(dummy_fatalities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
