{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS: [37.03430057]\n",
      "tf.Tensor(\n",
      "[[30. 31. 32. 32. 33. 33. 34. 34. 34. 34. 35. 35. 35. 35. 35. 35. 36. 36.\n",
      "  36. 36. 36. 36. 36. 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38.\n",
      "  38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 39. 39. 39. 39. 39. 39. 39.\n",
      "  39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40. 40. 40.\n",
      "  40. 40. 40. 40. 40. 40. 40. 40. 40. 40. 40. 40. 40. 40. 41. 41. 41. 41.\n",
      "  41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41.\n",
      "  41. 41. 41. 41. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42.\n",
      "  42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 43.\n",
      "  43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43.\n",
      "  43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44.\n",
      "  44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44.\n",
      "  44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44. 44.\n",
      "  44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n",
      "  45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n",
      "  45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45. 46. 46. 46. 46.\n",
      "  46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46.\n",
      "  46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 46.\n",
      "  46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n",
      "  47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47.\n",
      "  47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47. 47.\n",
      "  47. 47. 47. 47. 47. 47. 47. 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48.\n",
      "  48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48.\n",
      "  48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48.\n",
      "  48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49. 49. 49.\n",
      "  49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49.\n",
      "  49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49.\n",
      "  49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 49. 50. 50. 50. 50. 50.\n",
      "  50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.\n",
      "  50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.\n",
      "  50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 51. 51. 51.\n",
      "  51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51.\n",
      "  51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51.\n",
      "  51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 51. 52. 52.\n",
      "  52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52.\n",
      "  52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52.\n",
      "  52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 52. 53. 53. 53.\n",
      "  53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53.\n",
      "  53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53.\n",
      "  53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 53. 54. 54. 54. 54. 54. 54. 54.\n",
      "  54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54.\n",
      "  54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54. 54.\n",
      "  54. 54. 54. 54. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55.\n",
      "  55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 55.\n",
      "  55. 55. 55. 55. 55. 55. 55. 55. 55. 55. 56. 56. 56. 56. 56. 56. 56. 56.\n",
      "  56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56.\n",
      "  56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 56. 57. 57. 57. 57. 57. 57.\n",
      "  57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57. 57.\n",
      "  57. 57. 57. 57. 57. 57. 57. 57. 57. 58. 58. 58. 58. 58. 58. 58. 58. 58.\n",
      "  58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58.\n",
      "  58. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59. 59.\n",
      "  59. 59. 59. 59. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 60. 60. 60.\n",
      "  60. 60. 60. 60. 60. 60. 60. 60. 60. 61. 61. 61. 61. 61. 61. 61. 61. 61.\n",
      "  61. 61. 61. 61. 61. 61. 61. 61. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      "  62. 62. 62. 63. 63. 63. 63. 63. 63. 63. 63. 63. 63. 63. 64. 64. 64. 64.\n",
      "  64. 64. 64. 64. 65. 65. 65. 65. 65. 65. 66. 66. 66. 66. 66. 67. 67. 67.\n",
      "  67. 68. 68. 69. 69. 70. 70. 72. 73.]], shape=(1, 999), dtype=float32)\n",
      "CRPS Näherung: tf.Tensor([37.0343], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" y_pred = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_true = [20.0] \"\"\"\n",
    "from scipy.stats import poisson\n",
    "\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "\n",
    "\n",
    "mean = 50\n",
    "y_pred = np.array(poisson.ppf(quantiles, mean), dtype=np.float32)\n",
    "y_true = [9.0]\n",
    "\n",
    "\n",
    "import CRPS.CRPS as pscore\n",
    "# CRPS normal berechnen\n",
    "crps_normal = pscore(np.array(y_pred), y_true).compute()[0]\n",
    "print(\"CRPS: \" + str(crps_normal))\n",
    "\n",
    "\n",
    "\n",
    "S = tf.constant([y_pred])\n",
    "print(S)\n",
    "y_true = tf.constant([y_true], dtype=tf.float32)\n",
    "\n",
    "\n",
    "beta=1\n",
    "n_samples = S.shape[-1]\n",
    "\n",
    "def expected_dist(diff, beta):\n",
    "    return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "es_1 = expected_dist(y_true - S, beta)\n",
    "es_2 = 0\n",
    "for i in range(n_samples):\n",
    "    es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "\n",
    "print(\"CRPS Näherung: \" + str(es_1/n_samples - es_2/(2*n_samples**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
