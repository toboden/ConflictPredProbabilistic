{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)\n",
    "\n",
    "\n",
    "# Funktion für die ReLU-Transformation\n",
    "def relu_transform(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "## hyperparameters\n",
    "batchSize = 1 # defines the number of samples to work through before \n",
    "# updating the internal model parameters (sample = (1 inputvector, 1 y_true))\n",
    "epochSize = 10 # defines the number times that the learning algorithm will work through the entire training dataset\n",
    "# -> line plots that show epochs along the x-axis as time and the error or skill of the model on the y-axis (= learning curve)\n",
    "learningRate = 0.001\n",
    "\n",
    "\n",
    "# Define inputs with predefined shape\n",
    "input_shape = (len(X_train[0]),) # Number of used features   10 * 32\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "#print(inputs.shape) # -> (None, 10, 32) no Batch size defined (more flexible)\n",
    "\n",
    "hidden_layer1 = Dense(4, activation='relu')(inputs) \n",
    "# Dense Layer: the 10 neurons in the dense layer get their source of input data \n",
    "# from all the other neurons of the previous layer of the network (= fully connected layer)\n",
    "hidden_layer2 = Dense(4, activation='relu')(hidden_layer1) \n",
    "\n",
    "#\n",
    "output_s3 = Dense(100)(hidden_layer1)\n",
    "sample_output_s3 = Lambda(relu_transform)(output_s3)\n",
    "\n",
    "# Construct model\n",
    "model = Model(inputs=inputs, outputs=sample_output_s3, name = 'feedfwdNN_empirical')\n",
    "\n",
    "# Compile the model with the desired optimizer, loss function, etc.\n",
    "model.compile(optimizer=Adam(learning_rate=learningRate), loss=CRPSLoss())\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping ECON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)\n",
    "\n",
    "# Define custom ReLU activation function\n",
    "class ReLUTransform(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "    \n",
    "    \n",
    "# Funktion für die ReLU-Transformation\n",
    "def relu_transform(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# Define the Feed Forward Neural Network subclass\n",
    "class FeedForwardNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        \n",
    "        self.model = self._build_model(input_shape)\n",
    "\n",
    "\n",
    "\n",
    "    def _build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Defines original IGEP model:\n",
    "        Variance of dim_out of the latent distributions depend on the ensemble spread.\n",
    "        See Janke&Steinke (2020): \"Probabilistic multivariate electricity price forecasting using implicit generative ensemble post-processing\"\n",
    "        https://arxiv.org/abs/2005.13417\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            Keras model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        hidden_layer1 = Dense(4, activation='relu')(inputs) \n",
    "        hidden_layer2 = Dense(4, activation='relu')(hidden_layer1) \n",
    "        output_s3 = Dense(100)(hidden_layer2)\n",
    "        sample_output_s3 = Lambda(relu_transform)(output_s3)\n",
    "\n",
    "        return Model(inputs=inputs, outputs=sample_output_s3, name = 'FeedForwardNN')\n",
    "    \n",
    "    def fit(self, x, y, batch_size=1, epochs=10, learning_rate = 0.001, verbose=0, \n",
    "            callbacks=None, validation_split=0.0, validation_data=None, sample_weight=None, plot_learning_curve=False):\n",
    "        \"\"\"\n",
    "        Fits the model to traning data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"    \n",
    "\n",
    "################################################################################################################\n",
    "        self.model.compile(loss=CRPSLoss(), optimizer=Adam(learning_rate))\n",
    "        self.history = self.model.fit(x=x, \n",
    "                                      y=y,\n",
    "                                      batch_size=batch_size, \n",
    "                                      epochs=epochs, \n",
    "                                      verbose=verbose, \n",
    "                                      callbacks=callbacks, \n",
    "                                      validation_split=validation_split, \n",
    "                                      validation_data=validation_data,\n",
    "                                      shuffle=True,\n",
    "                                      sample_weight=sample_weight)\n",
    "################################################################################################################\n",
    "        if verbose > 0:\n",
    "            keras.utils.plot_model(self.model, show_shapes=True)\n",
    "            self.model.summary()\n",
    "        \n",
    "        if plot_learning_curve:\n",
    "            learning_curve_plot(self.history.history)\n",
    "\n",
    "# Define hyperparameters\n",
    "\"\"\" batchSize = 1\n",
    "epochSize = 10\n",
    "learningRate = 0.001 \"\"\"\n",
    "\n",
    "# Define inputs with predefined shape\n",
    "input_shape = (len(X_train[0]),)\n",
    "\n",
    "# Create an instance of the FeedForwardNN model\n",
    "#model = FeedForwardNN(input_shape=input_shape, classes=100)  # Update 'classes' with the appropriate number\n",
    "model = FeedForwardNN(input_shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
