{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' subsample_list = [1, 57, 235, 13, 121, 223, 220]\\ncountry_list = [x for x in country_list if x in subsample_list] '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" subsample_list = [1, 57, 235, 13, 121, 223, 220]\n",
    "country_list = [x for x in country_list if x in subsample_list] \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set seed so that the model outputs are reproducible\n",
    "After the kernel is restarted the same results are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(0)\n",
    "\n",
    "# This will make TensorFlow ops as deterministic as possible, but it will\n",
    "# affect the overall performance, so it's not enabled by default.\n",
    "# `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 131072 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tobias\\Documents\\BAconflictPrediction\\ConflictPrediction\\old_code\\Main_oneCountry_feedFwdNN_empirical.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tobias/Documents/BAconflictPrediction/ConflictPrediction/old_code/Main_oneCountry_feedFwdNN_empirical.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# append datasets to the lists\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tobias/Documents/BAconflictPrediction/ConflictPrediction/old_code/Main_oneCountry_feedFwdNN_empirical.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     actuals_df_list\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m:actual_years[i], \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m:pd\u001b[39m.\u001b[39mread_parquet(path_actuals, engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m'\u001b[39m)})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tobias/Documents/BAconflictPrediction/ConflictPrediction/old_code/Main_oneCountry_feedFwdNN_empirical.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     features_df_list\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m:feature_years[i], \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m:pd\u001b[39m.\u001b[39;49mread_parquet(path_features, engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpyarrow\u001b[39;49m\u001b[39m'\u001b[39;49m)})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tobias/Documents/BAconflictPrediction/ConflictPrediction/old_code/Main_oneCountry_feedFwdNN_empirical.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# concat the feature datasets, so that every data contains the observations starting with january 1990\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tobias/Documents/BAconflictPrediction/ConflictPrediction/old_code/Main_oneCountry_feedFwdNN_empirical.ipynb#W4sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(features_df_list)):\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 509\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39mread(\n\u001b[0;32m    510\u001b[0m     path,\n\u001b[0;32m    511\u001b[0m     columns\u001b[39m=\u001b[39mcolumns,\n\u001b[0;32m    512\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m    513\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    514\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    515\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    516\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    221\u001b[0m     path,\n\u001b[0;32m    222\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    223\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m    224\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[0;32m    228\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39mcolumns, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    229\u001b[0m     )\n\u001b[0;32m    230\u001b[0m     result \u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\parquet\\core.py:2986\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2975\u001b[0m         \u001b[39m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2976\u001b[0m         dataset \u001b[39m=\u001b[39m ParquetFile(\n\u001b[0;32m   2977\u001b[0m             source, metadata\u001b[39m=\u001b[39mmetadata, read_dictionary\u001b[39m=\u001b[39mread_dictionary,\n\u001b[0;32m   2978\u001b[0m             memory_map\u001b[39m=\u001b[39mmemory_map, buffer_size\u001b[39m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2983\u001b[0m             thrift_container_size_limit\u001b[39m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   2984\u001b[0m         )\n\u001b[1;32m-> 2986\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39;49mread(columns\u001b[39m=\u001b[39;49mcolumns, use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[0;32m   2987\u001b[0m                         use_pandas_metadata\u001b[39m=\u001b[39;49muse_pandas_metadata)\n\u001b[0;32m   2989\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2990\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPassing \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to get the legacy behaviour is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2991\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2992\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbe removed in a future version.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2993\u001b[0m     \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m   2995\u001b[0m \u001b[39mif\u001b[39;00m ignore_prefixes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\parquet\\core.py:2614\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2606\u001b[0m         index_columns \u001b[39m=\u001b[39m [\n\u001b[0;32m   2607\u001b[0m             col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2608\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mdict\u001b[39m)\n\u001b[0;32m   2609\u001b[0m         ]\n\u001b[0;32m   2610\u001b[0m         columns \u001b[39m=\u001b[39m (\n\u001b[0;32m   2611\u001b[0m             \u001b[39mlist\u001b[39m(columns) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(index_columns) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(columns))\n\u001b[0;32m   2612\u001b[0m         )\n\u001b[1;32m-> 2614\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset\u001b[39m.\u001b[39;49mto_table(\n\u001b[0;32m   2615\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_expression,\n\u001b[0;32m   2616\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads\n\u001b[0;32m   2617\u001b[0m )\n\u001b[0;32m   2619\u001b[0m \u001b[39m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2620\u001b[0m \u001b[39m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\_dataset.pyx:546\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\_dataset.pyx:3449\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 131072 failed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_Actuals(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# check if the last month of a country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "def check_last_featureMonth(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        raise ValueError('country does not have actuals')\n",
    "\n",
    "\n",
    "    # last month of the feature dataset\n",
    "    last_feature_month = country_feature_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[-1]\n",
    "\n",
    "    # first month of the actual dataset\n",
    "    first_actual_month = country_actual_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[0]\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (first_actual_month - 3) != last_feature_month:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "data = features_df_list[-1]['data']\n",
    "if 'gleditsch_ward' in data.columns:\n",
    "    data = data.drop(columns='gleditsch_ward') # column not necessary\n",
    "\n",
    "## Features without missing values\n",
    "columns_without_missing_values = data.columns[data.notna().all()]\n",
    "\n",
    "for i in range(len(features_df_list)):\n",
    "    data_set = features_df_list[i]['data']\n",
    "    features_df_list[i]['data'] = data_set[columns_without_missing_values]\n",
    "\n",
    "all_features = features_df_list[-1]['data'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group data by country_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Länder aussortieren\n",
    "die nicht gefordert sind und\n",
    "*  die keine actuals haben\n",
    "*  die zu wenig Beobachtungen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_countrylist = os.path.join('..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "\n",
    "        if check_Actuals(country_id, 0):\n",
    "            if not check_last_featureMonth(country_id, 0): \n",
    "                month_list.append([str(country_id) +' last month missing'])\n",
    "            else:\n",
    "                month_list.append([number_months_feature_data, country_id])\n",
    "        else:\n",
    "            month_list.append(str(country_id) + ' no actuals')\n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "Goal is to estimate the empirical distribution of the fatalities per month.\n",
    "### Definition of the CRPS loss function and the Feed forward Neural Network subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to set the number and partition of the neurons\n",
    "def number_neurons_allLayers(inputNeurons, outputNeurons):\n",
    "    return np.round(np.mean([inputNeurons, outputNeurons]))\n",
    "\n",
    "def split_neurons_3hiddenlayer(numberNeurons):\n",
    "\n",
    "    neuronsHiddenLayer1 = np.round(numberNeurons * 0.5) \n",
    "    neuronsHiddenLayer2 = np.round(numberNeurons * 0.3)\n",
    "    neuronsHiddenLayer3 = numberNeurons - neuronsHiddenLayer1 - neuronsHiddenLayer2\n",
    "\n",
    "    return int(neuronsHiddenLayer1), int(neuronsHiddenLayer2), int(neuronsHiddenLayer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)\n",
    "\n",
    "# Define custom ReLU activation function\n",
    "class ReLUTransform(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "# Define the Feed Forward Neural Network subclass\n",
    "class FeedForwardNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, name=\"FeedFwdNN\"):\n",
    "        super(FeedForwardNN, self).__init__(name=name)\n",
    "        \n",
    "        neurons_input = input_shape[-1]\n",
    "        neurons_output = 200\n",
    "        neurons_hidden_layer = number_neurons_allLayers(neurons_input,neurons_output)\n",
    "        neuronsL1, neuronsL2, neuronsL3 = split_neurons_3hiddenlayer(neurons_hidden_layer)\n",
    "\n",
    "\n",
    "        self.hidden_layer1 = Dense(np.round(neurons_hidden_layer/4), activation='relu')\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.hidden_layer2 = Dense(np.round(neurons_hidden_layer/4), activation='relu')\n",
    "        self.hidden_layer3 = Dense(np.round(neurons_hidden_layer/4), activation='relu')\n",
    "        self.hidden_layer4 = Dense(np.round(neurons_hidden_layer/4), activation='relu')\n",
    "        self.untransformed_output = Dense(neurons_output)\n",
    "        self.final_output = Lambda(ReLUTransform())\n",
    "\n",
    "        self.model = self.build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.hidden_layer1(inputs)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.hidden_layer3(x)\n",
    "        x = self.hidden_layer4(x)\n",
    "        x = self.untransformed_output(x)\n",
    "        y = self.final_output(x)\n",
    "        return y\n",
    "\n",
    "# Definiere den EarlyStopping-Callback\n",
    "patience = 3\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "## function used to calculate w_max, number of rolling windows etc.\n",
    "# length of a whole window (containing w input months and 12 acutal months)\n",
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "def number_valid_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    number_train_valid_months = numberMonths_available - w\n",
    "    number_valid_months = math.floor(number_train_valid_months * relative_validation_size)\n",
    "\n",
    "    return number_valid_months\n",
    "\n",
    "# number of months available for training (after removing the validation and test months)\n",
    "def number_train_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    valid_months = number_valid_months(numberMonths_available, w, relative_validation_size)\n",
    "\n",
    "    #  all months feature data   -  validate set    -   test set input\n",
    "    return numberMonths_available - valid_months - w\n",
    "\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "\n",
    "def find_max_W(numberMonths_available, w_min, w_max, relative_validation_size):\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one validation window\n",
    "    number_valid_months_wmin = number_valid_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_valid_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one validation window with w_min = ' + str(w_min))\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one train window\n",
    "    number_train_months_wmin = number_train_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_train_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "    # find the maximal w\n",
    "    max_W = w_max\n",
    "    number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "    number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while (number_valid_rollwindows_wmax == 0 or number_train_rollwindows_wmax == 0) and max_W > w_min:\n",
    "        max_W -= 1\n",
    "        number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "        number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "## conflict trap\n",
    "# drops all months before a starting conflict\n",
    "# defintion of a beginning conflict: fatalities(monthX) > 0 with mean(fatalities(window_size number months starting with monthX)) > threshold\n",
    "# (average fatalities per month in the starting half year are greater than the threshold)\n",
    "# iterates trough the dataset beginning with the first entry\n",
    "def drop_before_conflict_trap(data, threshold, window_size, minimal_data_size):\n",
    "    index_ged_sb = data.columns.get_loc('ged_sb')\n",
    "\n",
    "    start_index = 0\n",
    "    while start_index < len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "        window = data.iloc[start_index:start_index + window_size, index_ged_sb].to_list()\n",
    "        \n",
    "        if window[0] > 0 and sum(window) / window_size >= threshold:\n",
    "            break\n",
    "        else:\n",
    "            start_index += 1\n",
    "\n",
    "\n",
    "    if len(data) >= minimal_data_size:\n",
    "        # if there is no conflict trap do nothing\n",
    "        if start_index == len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "            return data\n",
    "        # if the truncation would result in a too small dataset prevent this\n",
    "        elif len(data.iloc[start_index:, :]) < minimal_data_size:\n",
    "            return data.iloc[-minimal_data_size:, :]\n",
    "        # drop every entry before the conflict trap \n",
    "        else:\n",
    "            return data.iloc[start_index:, :]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "\n",
    "def TrainValid_ArrayXY_split(w, month_list, data_feature, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    train_months = len(month_list)\n",
    "\n",
    "    number_rolling_windows_train = number_rolling_windows(train_months, w)\n",
    "\n",
    "    for i in range(0, number_rolling_windows_train):\n",
    "        starting_month_features = month_list[i]\n",
    "\n",
    "        index_ending_month_features = i + w - 1\n",
    "        ending_month_features = month_list[index_ending_month_features]\n",
    "\n",
    "        starting_month_unrActuals = month_list[index_ending_month_features + 3]\n",
    "        ending_month_unrActuals = month_list[index_ending_month_features + 14]\n",
    "\n",
    "        window_features = data_feature.loc[slice(starting_month_features, ending_month_features), :] # excluding \"unreal\" actuals\n",
    "        window_actuals = data_feature.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "        normalized_window_features = preprocessing.normalize(window_features)\n",
    "        window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "        window_actual_array = np.array([window_actuals])\n",
    "\n",
    "        X.append(window_features_array)\n",
    "        Y.append(window_actual_array)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def Test_ArrayXY_split(month_list, data_feature, data_actual, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    starting_month_test = month_list[0]\n",
    "    ending_month_test = month_list[-1]\n",
    "\n",
    "    window_features_test = data_feature.loc[slice(starting_month_test, ending_month_test), :] # all w features to predict the fatalities\n",
    "    window_actuals_test = data_actual.iloc[s - 3].values # real actuals\n",
    "\n",
    "    normalized_window_features_test = preprocessing.normalize(window_features_test)\n",
    "    window_features_array_test = np.array([normalized_window_features_test.flatten()])[0]\n",
    "\n",
    "    window_actual_array_test = window_actuals_test\n",
    "\n",
    "    X.append(window_features_array_test)\n",
    "    Y.append(window_actual_array_test)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "### non need of w, due to no test set in the hyperparam validation split\n",
    "def number_valid_monthsRollWindow(numberMonths_available, relative_validation_size):\n",
    "\n",
    "    number_valid_months = math.floor(numberMonths_available * relative_validation_size)\n",
    "\n",
    "    return number_valid_months\n",
    "\n",
    "# number of months available for training (after removing the validation and test months)\n",
    "def number_train_monthsRollWindow(numberMonths_available, relative_validation_size):\n",
    "\n",
    "    valid_months = number_valid_monthsRollWindow(numberMonths_available, relative_validation_size)\n",
    "\n",
    "    #  all months feature data   -  validate set\n",
    "    return numberMonths_available - valid_months\n",
    "\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "# no -w because of no test set\n",
    "def find_max_WRollWindow(numberMonths_available, w_min, w_max, relative_validation_size):\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one validation window\n",
    "    number_valid_months = number_valid_monthsRollWindow(numberMonths_available, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_valid_months, w_min) == 0:\n",
    "        raise ValueError('not enough months for one validation window with w_min = ' + str(w_min))\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one train window\n",
    "    number_train_months = number_train_monthsRollWindow(numberMonths_available, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_train_months, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "    # find the maximal w\n",
    "    max_W = w_max\n",
    "    number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months, max_W)\n",
    "\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_train_months, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while (number_valid_rollwindows_wmax == 0 or number_train_rollwindows_wmax == 0) and max_W > w_min:\n",
    "        max_W -= 1\n",
    "        number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months, max_W)\n",
    "\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_train_months, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def month_lists_TrainValidTest_RollWindow(w_min, w_max, rel_validation_size, k, k_max, month_list_feature_data):\n",
    "\n",
    "    # make sure that the lateron used test dataset is not contained in the train/validation process ?!?\n",
    "    month_list_feature_data_withoutW = month_list_feature_data[:-w_max]\n",
    "\n",
    "    starting_month = k - 1\n",
    "    ending_month = k_max - starting_month - 1\n",
    "\n",
    "    if ending_month == 0:\n",
    "        month_list_rollwindow_k = month_list_feature_data_withoutW[starting_month:]\n",
    "    else: \n",
    "        month_list_rollwindow_k = month_list_feature_data_withoutW[starting_month:-ending_month]\n",
    "    \n",
    "\n",
    "    # numbers of months from the feature dataset\n",
    "    first_month = min(month_list_rollwindow_k)\n",
    "    last_month = max(month_list_rollwindow_k)\n",
    "    number_months_rollwindow_k = len(month_list_rollwindow_k)\n",
    "    \n",
    "    # find w_max (as mentioned above, if there are not enoug months, the w_max has to be < w_max)\n",
    "    w_max_local = find_max_WRollWindow(number_months_rollwindow_k, w_min, w_max, rel_validation_size)\n",
    "\n",
    "    w = w_max_local\n",
    "    print('Wmax = ' + str(w_max) + ' w = ' + str(w))\n",
    "\n",
    "    # length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "    n_train_months = number_train_monthsRollWindow(number_months_rollwindow_k, rel_validation_size)\n",
    "    n_valid_months = number_valid_monthsRollWindow(number_months_rollwindow_k, rel_validation_size)\n",
    "\n",
    "    print('rolling windows train: ' + str(number_rolling_windows(n_train_months, w)))\n",
    "    print('rolling windows validate: ' + str(number_rolling_windows(n_valid_months, w)))\n",
    "\n",
    "    month_list_train = month_list_rollwindow_k[0:n_train_months]\n",
    "    month_list_valid = month_list_rollwindow_k[n_train_months:(n_train_months+n_valid_months)]\n",
    "\n",
    "    month_list_test = month_list_feature_data[-w:]\n",
    "\n",
    "    return month_list_train, month_list_valid, month_list_test, w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------\n",
    "\n",
    "\n",
    "\"\"\" ## training dataset------\n",
    "X_train, Y_train = TrainValid_ArrayXY_split(n_train_months, w, month_list_train, feature_data, s)\n",
    "\n",
    "## validation dataset--------\n",
    "X_validate, Y_validate = TrainValid_ArrayXY_split(n_valid_months, w, month_list_valid, feature_data, s) \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different feature selection from views\n",
    "# 59 features that map the conflict history of a country\n",
    "conflict_history = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'ln_ged_sb_tlag_1',\n",
    "    'ln_ged_sb_tlag_2', 'ln_ged_sb_tlag_3', 'ln_ged_sb_tlag_4',\n",
    "    'ln_ged_sb_tlag_5', 'ln_ged_sb_tlag_6', 'ln_ged_sb_tsum_24',\n",
    "    'decay_ged_sb_100', 'decay_ged_sb_500', 'decay_ged_os_100',\n",
    "    'decay_ged_ns_5', 'decay_ged_ns_100', 'ln_ged_ns', 'ln_ged_os',\n",
    "    'ln_acled_sb', 'ln_acled_sb_count', 'ln_acled_os',\n",
    "    'ln_ged_os_tlag_1', 'decay_acled_sb_5', 'decay_acled_os_5',\n",
    "    'decay_acled_ns_5', 'splag_1_decay_ged_os_5',\n",
    "    'splag_1_decay_ged_ns_5'\n",
    "]\n",
    "# 59 features that are drawn from the Varieties of Democracy project\n",
    "vdem = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'vdem_v2x_delibdem',\n",
    "    'vdem_v2x_egaldem', 'vdem_v2x_libdem', 'vdem_v2x_libdem_48',\n",
    "    'vdem_v2x_partip', 'vdem_v2x_accountability',\n",
    "    'vdem_v2x_civlib', 'vdem_v2x_clphy', 'vdem_v2x_cspart',\n",
    "    'vdem_v2x_divparctrl', 'vdem_v2x_edcomp_thick', 'vdem_v2x_egal',\n",
    "    'vdem_v2x_execorr', 'vdem_v2x_frassoc_thick', 'vdem_v2x_gencs',\n",
    "    'vdem_v2x_gender', 'vdem_v2x_genpp', 'vdem_v2x_horacc',\n",
    "    'vdem_v2x_neopat', 'vdem_v2x_pubcorr', 'vdem_v2x_rule',\n",
    "    'vdem_v2x_veracc', 'vdem_v2x_freexp', 'vdem_v2xcl_acjst', \n",
    "    'vdem_v2xcl_dmove', 'vdem_v2xcl_prpty', 'vdem_v2xcl_rol', \n",
    "    'vdem_v2xcl_slave', 'vdem_v2xdl_delib', 'vdem_v2xeg_eqdr',\n",
    "    'vdem_v2xeg_eqprotec', 'vdem_v2xel_frefair', 'vdem_v2xel_regelec',\n",
    "    'vdem_v2xme_altinf', 'vdem_v2xnp_client', 'vdem_v2xnp_regcorr',\n",
    "    'vdem_v2xpe_exlecon', 'vdem_v2xpe_exlpol', 'vdem_v2xpe_exlgeo',\n",
    "    'vdem_v2xpe_exlgender', 'vdem_v2xpe_exlsocgr', 'vdem_v2xps_party',\n",
    "    'vdem_v2xcs_ccsi', 'vdem_v2xnp_pres', 'vdem_v2xeg_eqaccess',\n",
    "    'vdem_v2x_diagacc', 'vdem_v2clrgunev', 'splag_vdem_v2x_libdem',\n",
    "    'splag_vdem_v2xcl_dmove', 'splag_vdem_v2x_accountability',\n",
    "    'splag_vdem_v2xpe_exlsocgr', 'splag_vdem_v2xcl_rol', 'wdi_sm_pop_netm',\n",
    "    'wdi_sp_dyn_imrt_in'\n",
    "]\n",
    "\n",
    "# 30 features that are drawn from the WDI as well as some conflict history indicators\n",
    "wdi = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'wdi_ag_lnd_frst_k2',\n",
    "    'wdi_dt_oda_odat_pc_zs', 'wdi_ms_mil_xpnd_gd_zs', 'wdi_ms_mil_xpnd_zs',\n",
    "    'wdi_nv_agr_totl_kd', 'wdi_nv_agr_totl_kn', 'wdi_ny_gdp_pcap_kd',\n",
    "    'wdi_sp_dyn_le00_in', 'wdi_se_prm_nenr', 'wdi_sh_sta_maln_zs', \n",
    "    'wdi_sh_sta_stnt_zs', 'wdi_sl_tlf_totl_fe_zs', 'wdi_sm_pop_refg_or', \n",
    "    'wdi_sm_pop_netm', 'wdi_sm_pop_totl_zs', 'wdi_sp_dyn_imrt_in', \n",
    "    'wdi_sh_dyn_mort_fe', 'wdi_sp_pop_1564_fe_zs', 'wdi_sp_pop_65up_fe_zs',\n",
    "    'wdi_sp_pop_grow', 'wdi_sp_urb_totl_in_zs',\n",
    "    'splag_wdi_sl_tlf_totl_fe_zs', 'splag_wdi_sm_pop_refg_or',\n",
    "    'splag_wdi_sm_pop_netm', 'splag_wdi_ag_lnd_frst_k2'\n",
    "]\n",
    "\n",
    "ged = ['ged_sb']\n",
    "\n",
    "feature_subset_dict = {'conflict_history':conflict_history,\n",
    "                       'vdem':vdem,\n",
    "                       'wdi':wdi,\n",
    "                       'all':all_features,\n",
    "                       'ged':ged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction task old\n",
    "prediction_year = '2021'\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "s = 3 # month to predict element out of [3,14]\n",
    "\n",
    "### set hyperparameters for the prediction task\n",
    "## dataset (split) hyperparams\n",
    "rel_validation_size = 0.4 # percentual size of the validation set\n",
    "var_threshold = 0.05 # variance threshold for dropping columns\n",
    "mean_fatlities_per_month_threshold = 5 # threshold for the average number of fatalities per month (conflict trap detection)\n",
    "\n",
    "w_max = 12 # the maximal w (months to estimate the fatalities from) is set to e.g. 3 years (36 months)\n",
    "# BUT: If w_max=36 leads to a number of rolling windows < 1 in the validation dataset,\n",
    "# w_max is set to the maximal w, so that the number of rolling windows is >= 1\n",
    "# => this step is done below in the section for each country\n",
    "\n",
    "w_min = 2 # to calculate the w_max the w_min has to be set as well\n",
    "\n",
    "feature_subset = 'ged' # 'conflict_history', 'vdem', 'wdi', 'all',\n",
    "\n",
    "\n",
    "## country 223\n",
    "country = 220 #220  245\n",
    "\n",
    "# check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "if not check_last_featureMonth(country, dataset_index):\n",
    "    raise ValueError('last month is not contained in the data')\n",
    "\n",
    "### load and prepare datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "# only FEATURE SUBSET\n",
    "feature_data = feature_data.loc[:,feature_subset_dict[feature_subset]]\n",
    "\n",
    "## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "## remove months before the CONDFLICT TRAP (regime change)\n",
    "# if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "# the conflict trap starts and all obsservations before that month are dropped\n",
    "# 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "\n",
    "\n",
    "# numbers of months from the feature dataset\n",
    "month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "first_month = min(month_list_feature_data)\n",
    "last_month = max(month_list_feature_data)\n",
    "number_months_feature_data = len(month_list_feature_data) # number of months in the feature dataset\n",
    "\n",
    "# find w_max (as mentioned above, if there are not enoug months, the w_max has to be < 36)\n",
    "w_max_local = find_max_W(number_months_feature_data, w_min, w_max, rel_validation_size)\n",
    "\n",
    "w = w_max_local\n",
    "\n",
    "### split data in train-, validation- and test-dataset\n",
    "\"\"\" The data sizes are calculated in a way, that missing months are no problem. Additionaly due to the fact, that the data is sorted\n",
    "    regarding months, this step can be skipped. \"\"\"\n",
    "\n",
    "## train-valid-test------------------\n",
    "# length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "n_train_months = number_train_months(number_months_feature_data, w, rel_validation_size)\n",
    "n_valid_months = number_valid_months(number_months_feature_data, w, rel_validation_size)\n",
    "n_test_months = w\n",
    "\n",
    "month_list_train = month_list_feature_data[0:n_train_months]\n",
    "month_list_valid = month_list_feature_data[n_train_months:(n_train_months+n_valid_months)]\n",
    "month_list_test = month_list_feature_data[number_months_feature_data-n_test_months:]\n",
    "#--------------------\n",
    "\n",
    "\n",
    "## training dataset------\n",
    "X_train, Y_train = TrainValid_ArrayXY_split(w, month_list_train, feature_data, s)\n",
    "\n",
    "## validation dataset--------\n",
    "X_validate, Y_validate = TrainValid_ArrayXY_split(w, month_list_valid, feature_data, s)\n",
    "\n",
    "## test dataset-------\n",
    "X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "\n",
    "\n",
    "print('(first, last) train month: (' + str(month_list_train[0])+','+str(month_list_train[-1])+')')\n",
    "print('(first, last) validate month: (' + str(month_list_valid[0])+','+str(month_list_valid[-1])+')')\n",
    "print('(first, last) test month: (' + str(month_list_test[0])+','+str(month_list_test[-1])+')')\n",
    "print('train + valid + test = ' + str(n_train_months + n_valid_months +n_test_months) + ' != ' + str(number_months_feature_data))\n",
    "\n",
    "\n",
    "print('')\n",
    "print('# rolling w train: ' + str(number_rolling_windows(n_train_months, w)) + '      # rolling w validate: ' + str(number_rolling_windows(n_valid_months, w)))\n",
    "print('w = ' + str(w) + ' w_max = ' + str(w_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[28, 60, 65, 67, 96, 121, 133, 136, 145, 149, 218, 223, 245, 246]\n",
    "\n",
    "country = 223 #220  245\n",
    "\n",
    "feature_subset = 'all'\n",
    "\n",
    "# check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "if not check_last_featureMonth(country, dataset_index):\n",
    "    raise ValueError('last month is not contained in the data')\n",
    "\n",
    "### load and prepare datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "# only FEATURE SUBSET\n",
    "feature_data = feature_data.loc[:,feature_subset_dict[feature_subset]]\n",
    "\n",
    "## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "## remove months before the CONDFLICT TRAP (regime change)\n",
    "# if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "# the conflict trap starts and all obsservations before that month are dropped\n",
    "# 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "\n",
    "\n",
    "\n",
    "month_liste = feature_data.index.get_level_values('month_id').tolist()\n",
    "w_min = 1\n",
    "w_max = 6\n",
    "rel_validation_size = 0.3\n",
    "k=1\n",
    "k_max = 1\n",
    "\n",
    "\n",
    "YYdistribution_list = [{'Y_train':None, 'Y_valid':None } for _ in range(14)]\n",
    "\n",
    "\n",
    "print('Starting month: ' + str(month_liste[0]))\n",
    "print('Ending month: ' + str(month_liste[-1]))\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "for s in range(3,14+1):\n",
    "    month_list_train, month_list_valid, month_list_test, w = month_lists_TrainValidTest_RollWindow(w_min, w_max, rel_validation_size, k, k_max, month_liste)\n",
    "\n",
    "    print(' ')\n",
    "    print('k = ' + str(k))\n",
    "    \n",
    "\n",
    "    print(month_list_train)\n",
    "    print(month_list_valid)\n",
    "    print(month_list_test)\n",
    "\n",
    "    ## training dataset------\n",
    "    X_train, Y_train = TrainValid_ArrayXY_split(w, month_list_train, feature_data, s)\n",
    "    print('len x train input ' + str(len(X_train[0])))\n",
    "\n",
    "    ## validation dataset--------\n",
    "    X_validate, Y_validate = TrainValid_ArrayXY_split(w, month_list_valid, feature_data, s)\n",
    "    print('len x validate input ' + str(len(X_validate[0])))\n",
    "\n",
    "    ## test dataset-------\n",
    "    X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "    print('len x test input ' + str(len(X_test[0])))\n",
    "    print(' ')\n",
    "\n",
    "    print('(first, last) train month: (' + str(month_list_train[0])+','+str(month_list_train[-1])+')')\n",
    "    print('(first, last) validate month: (' + str(month_list_valid[0])+','+str(month_list_valid[-1])+')')\n",
    "    print('(first, last) test month: (' + str(month_list_test[0])+','+str(month_list_test[-1])+')')\n",
    "    print('')\n",
    "\n",
    "    YYdistribution_list[s-3]['Y_train'] = Y_train\n",
    "    YYdistribution_list[s-3]['Y_valid'] = Y_validate\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Daten für das erste Histogramm (Y_train)\n",
    "data = YYdistribution_list[0]['Y_train']\n",
    "\n",
    "# Daten für das zweite Histogramm (Y_validate)\n",
    "data2 = YYdistribution_list[0]['Y_valid']\n",
    "\n",
    "# Erstellen Sie eine Figur mit zwei Subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Histogramm für das erste Dataset (Y_train)\n",
    "n1, bins1, patches1 = ax1.hist(data, bins=20, edgecolor='black')\n",
    "ax1.set_xlabel('Todesfälle pro Monat')\n",
    "ax1.set_ylabel('Anzahl')\n",
    "ax1.set_title('Histogramm der Todesfälle pro Monat (Y_train)')\n",
    "\n",
    "\n",
    "# Histogramm für das zweite Dataset (Y_validate)\n",
    "n2, bins2, patches2 = ax2.hist(data2, bins=20, edgecolor='black')\n",
    "ax2.set_xlabel('Todesfälle pro Monat')\n",
    "ax2.set_ylabel('Anzahl')\n",
    "ax2.set_title('Histogramm der Todesfälle pro Monat (Y_validate)')\n",
    "ax1.set_ylim(0, max(max(n1), max(n2)) + 1)  # Y-Achse auf das Maximum der maximalen Häufigkeiten einstellen\n",
    "ax2.set_ylim(0, max(max(n1), max(n2)) + 1)  # Y-Achse auf das Maximum der maximalen Häufigkeiten einstellen\n",
    "\n",
    "# Abstand zwischen den Subplots einstellen\n",
    "plt.tight_layout()\n",
    "\n",
    "# Zeigen Sie die beiden Histogramme an\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for s in range(3,14+1):\n",
    "    # Erstellen Sie den Boxplot für data\n",
    "    plt.boxplot(YYdistribution_list[s-3]['Y_train'], positions=[s-3 + 1], labels=[str(s)])\n",
    "\n",
    "# Optional: Beschriftungen hinzufügen\n",
    "plt.xlabel('Prediction window s', fontsize=16)\n",
    "plt.ylabel('Fatalities per month', fontsize=16)\n",
    "plt.title('Country ' +str(country) + ' - w=' + str(w) + ' - training dataset (70%)', fontsize=18)\n",
    "\n",
    "# Den Boxplot anzeigen\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for s in range(3,14+1):\n",
    "    # Erstellen Sie den Boxplot für data2 neben data\n",
    "    plt.boxplot(YYdistribution_list[s-3]['Y_valid'], positions=[s - 3 +1], labels=[str(s)])\n",
    "\n",
    "# Optional: Beschriftungen hinzufügen\n",
    "plt.xlabel('Prediction window s', fontsize=16)\n",
    "plt.ylabel('Fatalities per month', fontsize=16)\n",
    "plt.title('Country ' +str(country) + ' - w=' + str(w) + ' - validation dataset (30%)', fontsize=18)\n",
    "\n",
    "# Den Boxplot anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country = 223 #220  245\n",
    "\n",
    "feature_subset = 'all'\n",
    "\n",
    "# check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "if not check_last_featureMonth(country, dataset_index):\n",
    "    raise ValueError('last month is not contained in the data')\n",
    "\n",
    "### load and prepare datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "# only FEATURE SUBSET\n",
    "feature_data = feature_data.loc[:,feature_subset_dict[feature_subset]]\n",
    "\n",
    "## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "## remove months before the CONDFLICT TRAP (regime change)\n",
    "# if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "# the conflict trap starts and all obsservations before that month are dropped\n",
    "# 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "\n",
    "\n",
    "\n",
    "month_liste = feature_data.index.get_level_values('month_id').tolist()\n",
    "w_min = 1\n",
    "rel_validation_size = 0.3\n",
    "k=1\n",
    "k_max = 1\n",
    "\n",
    "s = 8\n",
    "\n",
    "YYdistribution_list = [{'Y_train':None, 'Y_valid':None } for _ in range(12)]\n",
    "\n",
    "for w_max in range(1,12+1):\n",
    "    month_list_train, month_list_valid, month_list_test, w = month_lists_TrainValidTest_RollWindow(w_min, w_max, rel_validation_size, k, k_max, month_liste)\n",
    "\n",
    "    ## training dataset------\n",
    "    X_train, Y_train = TrainValid_ArrayXY_split(w, month_list_train, feature_data, s)\n",
    "\n",
    "    ## validation dataset--------\n",
    "    X_validate, Y_validate = TrainValid_ArrayXY_split(w, month_list_valid, feature_data, s)\n",
    "\n",
    "    ## test dataset-------\n",
    "    X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "    print('')\n",
    "\n",
    "    YYdistribution_list[w_max-1]['Y_train'] = Y_train\n",
    "    YYdistribution_list[w_max-1]['Y_valid'] = Y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for w in range(1,12+1):\n",
    "    # Erstellen Sie den Boxplot für data\n",
    "    plt.boxplot(YYdistribution_list[w-1]['Y_train'], positions=[w-1 + 1], labels=[str(w)])\n",
    "\n",
    "# Optional: Beschriftungen hinzufügen\n",
    "plt.xlabel('Number of months for prediction w', fontsize=16)\n",
    "plt.ylabel('Fatalities per month', fontsize=16)\n",
    "plt.title('Country ' +str(country) + ' - s=' + str(s) + ' - training dataset (70%)', fontsize=18)\n",
    "\n",
    "# Den Boxplot anzeigen\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for w in range(1,12+1):\n",
    "    # Erstellen Sie den Boxplot für data2 neben data\n",
    "    plt.boxplot(YYdistribution_list[w-1]['Y_valid'], positions=[w - 1 +1], labels=[str(w)])\n",
    "\n",
    "# Optional: Beschriftungen hinzufügen\n",
    "plt.xlabel('Number of months for prediction w', fontsize=16)\n",
    "plt.ylabel('Fatalities per month', fontsize=16)\n",
    "plt.title('Country ' +str(country) + ' - s=' + str(s) + ' - validation dataset (30%)', fontsize=18)\n",
    "\n",
    "# Den Boxplot anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Country 13: Few Fatalities at Present, Greater Incidence in the Past**  \n",
    "  In this context, there are presently few fatalities, but historically, there were higher fatalities over a considerable period. The conflict causing these fatalities may have waned, yet remnants continue to pose a minor threat.\n",
    "\n",
    "- **Country 60:**\n",
    "\n",
    "- **Country 121: On-and-Off Conflict, Resuming and Ceasing Periodically**  \n",
    "  This scenario pertains to conflicts that had ceased, then restarted, and subsequently ceased again.\n",
    "\n",
    "- **Country 223: Sustained Moderate Fatalities, with a Single Month of Zero Fatalities**  \n",
    "  Here, the conflict is characterized by consistent moderate fatalities, with the exception of one occurrence out of 334 months when there were no fatalities.\n",
    "\n",
    "- **Country 220: Erupted Severe Conflict with High Fatalities and Conflict Trap**  \n",
    "  This category involves conflicts that have erupted, leading to significant fatalities and establishing a conflict trap, characterized by prolonged and sustained violence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_list = [79, 220, 223]\n",
    "country_list_original = country_list\n",
    "country_list = [x for x in country_list if x in subsample_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subset = 'all'\n",
    "w_min = 1\n",
    "rel_validation_size = 0.3\n",
    "k=1\n",
    "k_max = 1\n",
    "w_max = 6\n",
    "\n",
    "YYdistribution_list = [{'Y_train':None, 'Y_valid':None } for _ in range(len(country_list))]\n",
    "\n",
    "s = 8\n",
    "\n",
    "for i,country in enumerate(country_list):\n",
    "    ### load and prepare datasets\n",
    "    feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    # only FEATURE SUBSET\n",
    "    feature_data = feature_data.loc[:,feature_subset_dict[feature_subset]]\n",
    "\n",
    "    ## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "    columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "    feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "    actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    ## remove months before the CONDFLICT TRAP (regime change)\n",
    "    # if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "    # the conflict trap starts and all obsservations before that month are dropped\n",
    "    # 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "    feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "\n",
    "\n",
    "\n",
    "    month_liste = feature_data.index.get_level_values('month_id').tolist()\n",
    "\n",
    "    month_list_train, month_list_valid, month_list_test, w = month_lists_TrainValidTest_RollWindow(w_min, w_max, rel_validation_size, k, k_max, month_liste)\n",
    "\n",
    "    ## training dataset------\n",
    "    X_train, Y_train = TrainValid_ArrayXY_split(w, month_list_train, feature_data, s)\n",
    "\n",
    "    ## validation dataset--------\n",
    "    X_validate, Y_validate = TrainValid_ArrayXY_split(w, month_list_valid, feature_data, s)\n",
    "\n",
    "    ## test dataset-------\n",
    "    X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "\n",
    "    YYdistribution_list[i]['Y_train'] = Y_train\n",
    "    YYdistribution_list[i]['Y_valid'] = Y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "counter = 0\n",
    "for i,country in enumerate(country_list):\n",
    "    if i % 2 == 0:\n",
    "        # Erstellen Sie den Boxplot für data\n",
    "        plt.boxplot(YYdistribution_list[i]['Y_train'], positions=[i + counter + 1], labels=[str(country) + ' train'])\n",
    "        # Erstellen Sie den Boxplot für data\n",
    "        plt.boxplot(YYdistribution_list[i]['Y_valid'], positions=[i + counter + 2], labels=[str(country) + ' valid'])\n",
    "\n",
    "        counter += 1\n",
    "    elif i % 2 == 1:\n",
    "        # Erstellen Sie den Boxplot für data\n",
    "        plt.boxplot(YYdistribution_list[i]['Y_train'], positions=[i + counter + 1], labels=[str(country) + ' train'])\n",
    "        # Erstellen Sie den Boxplot für data\n",
    "        plt.boxplot(YYdistribution_list[i]['Y_valid'], positions=[i + counter + 2], labels=[str(country) + ' valid'])\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "# Optional: Beschriftungen hinzufügen\n",
    "plt.xlabel('Country_id')\n",
    "plt.ylabel('Fatalities per month')\n",
    "#plt.yscale('log', base=2)\n",
    "plt.title('w=' +str(w_max) + ' - s=' + str(s) + ' - 70% training data')\n",
    "\n",
    "# Den Boxplot anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Anzahl der Länder\n",
    "num_countries = len(country_list)\n",
    "\n",
    "# Anzahl der Zeilen und Spalten im Raster\n",
    "num_cols = num_countries\n",
    "\n",
    "# Erstellen Sie das Raster von Subplots\n",
    "fig, axs = plt.subplots(1, num_cols, figsize=(14, 6))\n",
    "\n",
    "# Schleife durch jedes Land\n",
    "for i, country in enumerate(country_list):\n",
    "    # Daten für \"train\" und \"valid\" des aktuellen Landes\n",
    "    Y_train = YYdistribution_list[i]['Y_train']\n",
    "    Y_valid = YYdistribution_list[i]['Y_valid']\n",
    "\n",
    "    # Plotten Sie \"train\" in der linken Spalte des aktuellen Subplots\n",
    "    axs[i].boxplot(YYdistribution_list[i]['Y_train'], positions=[1], labels=['TRAIN'])\n",
    "    axs[i].boxplot(YYdistribution_list[i]['Y_valid'], positions=[2], labels=['VALID'])\n",
    "    axs[i].set_title('Country ' + str(country), fontsize=18)  # Schriftgröße für den Titel erhöhen\n",
    "    axs[i].set_xlabel('Dataset', fontsize=16)  # Schriftgröße für die Achsenbeschriftung erhöhen\n",
    "    axs[i].set_ylabel('Fatalities per month', fontsize=16)  # Schriftgröße für die Achsenbeschriftung erhöhen\n",
    "\n",
    "plt.suptitle(f'w={w_max} - s={s} - 70% training data', fontsize=18)  # Schriftgröße für den Haupttitel erhöhen\n",
    "\n",
    "# Abstand zwischen den Subplots festlegen\n",
    "plt.tight_layout()\n",
    "\n",
    "# Den Plot anzeigen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einschub: Analyse der unterschiedlichen Ländern mit Conflict trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries separated by fatalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_fatalities_country_list = []\n",
    "countries_with_high_percentage_list = []\n",
    "someNonzero_fatalities_country_list = []\n",
    "\n",
    "for country_id in country_list:\n",
    "    feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "    # Berechnen des Prozentsatzes der Werte größer als 0 in der Spalte 'ged_sb'\n",
    "    positive_percentage = (feature_data['ged_sb'] > 0).mean() * 100\n",
    "\n",
    "    if (feature_data['ged_sb'] == 0).all():\n",
    "        zero_fatalities_country_list.append(country_id)\n",
    "    elif positive_percentage >= 60:\n",
    "        countries_with_high_percentage_list.append(country_id)\n",
    "    else:\n",
    "        someNonzero_fatalities_country_list.append(country_id)\n",
    "\n",
    "print('Countries with >= 60% months with fatalities: ' + str(countries_with_high_percentage_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(someNonzero_fatalities_country_list)/191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries with conflict trap and the removed months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list_nonzerofatalities = countries_with_high_percentage_list + someNonzero_fatalities_country_list\n",
    "country_list_nonzerofatalities.sort()\n",
    "country_list_nonzerofatalities\n",
    "\n",
    "for country_id in country_list_nonzerofatalities:\n",
    "    feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "    trap_data = drop_before_conflict_trap(feature_data, 10, 6, 76)\n",
    "\n",
    "    if len(trap_data) < len(feature_data):\n",
    "        print('trap country_id: ' + str(country_id) + ' removed months: ' + str(len(feature_data)-len(trap_data)) + ' len trap_data = ' + str(len(trap_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatalities = 0\n",
    "country_id_max = 0\n",
    "\n",
    "for country_id in country_list_nonzerofatalities:\n",
    "    feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "    if feature_data.loc[:,'ged_sb'].max() > max_fatalities:\n",
    "        max_fatalities = feature_data.loc[:,'ged_sb'].max()\n",
    "        country_id_max = country_id\n",
    "\n",
    "print('Country: ' + str(country_id_max) + ' fatalities = ' + str(max_fatalities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View dataset of country with id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    - 1 keine todesfälle\n",
    "    - 57 extrem selten todesfälle aber ein paar sind extrem hoch (maximum aller länder -> in einem Monat: 48183.0)\n",
    "    - 235 krieg der aufgehört hat\n",
    "    - 13 aktuell wenige todesfälle vor langer zeit mehr (krieg der aufgehört hat aber noch immer etwas schwelt)\n",
    "    - 121 krieg der aufgehört hat aber wieder angefangen hat und dann wieder aufgehört hat\n",
    "    - 223 konstant mittlere todesfälle, nur 1 von 334 monaten 0 \n",
    "    - 220 ausgebrochener krieg mit sehr hohen todeszahlen und conflict trap\n",
    "    \"\"\"\n",
    "\n",
    "data = country_feature_group_list[0].get_group(60)\n",
    "\n",
    "#data = drop_before_conflict_trap(data, 30, 6, 76)\n",
    "\n",
    "#data#.loc[:,'ged_sb'] == 0\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do"
   ]
  },
  {
   "attachments": {
    "train_split.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_split.png](attachment:train_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neural net hyperparams\n",
    "batchSize = 1\n",
    "epochSize = 100\n",
    "learningRate = 0.1\n",
    "\n",
    "# Define inputs with predefined shape\n",
    "input_shape = (len(X_train[0]),)\n",
    "inputs = Input(shape=input_shape)\n",
    "#print(inputs.shape)\n",
    "\n",
    "# Print model summary\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# Create an instance of the FeedForwardNN model\n",
    "model = FeedForwardNN(input_shape=inputs.shape, name='FFwdNN_s3')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learningRate), loss=CRPSLoss())\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=batchSize, epochs=epochSize,\n",
    "                    validation_data=(X_validate, Y_validate),\n",
    "                    callbacks=[early_stopping], #early_stopping\n",
    "                    verbose=1, shuffle=False)\n",
    "\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, Y_test, batch_size=batchSize)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate prediction\")\n",
    "prediction = model.predict(X_test)\n",
    "print(\"predictions shape:\", prediction.shape)\n",
    "empirical_distribution = np.round(np.sort(prediction[0])).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters data prep: ' + ' \\\\'+'\\\\' + 'wMax='+ str(w_max) + ' validationSize='+str(rel_validation_size) + ' conflictTrapThresh=' + str(mean_fatlities_per_month_threshold) + ' features='+feature_subset+ ' \\\\'+'\\\\')\n",
    "print('Parameters NN: ' + ' \\\\'+'\\\\' + 'earlyStoppingPatience=' + str(patience) + ' learningRate='+str(learningRate)+' numberHiddenLayers=2 numberNeurons=neurons_hidden_layer validationSize='+str(rel_validation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def empirical_cdf(data):\n",
    "    n = len(data)\n",
    "    ecdf_values = np.arange(1, n + 1) / n\n",
    "    return ecdf_values\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Berechne die empirische CDF\n",
    "ecdf = empirical_cdf(empirical_distribution)\n",
    "\n",
    "# Erstelle eine Figur mit 1 Zeile und 2 Spalten für die beiden Plots nebeneinander und kleiner\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "\n",
    "axes[0].plot(epochs, loss, 'y', label='Traing')\n",
    "axes[0].plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "axes[0].set_title('Training and validation loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plotte die empirische CDF im zweiten Subplot\n",
    "axes[1].step(empirical_distribution, ecdf, label='Empirical Density Function', color='green', linewidth=2)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('F(x)')\n",
    "axes[1].set_title('Empirical cdf')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()  # Optimiere den Abstand zwischen den Subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y_test[0][0]\n",
    "crps_check = pscore(empirical_distribution,y_true).compute()[0]\n",
    "print('CRPS = ' + str(crps_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    die conflict trap zu beachten hilft extrem\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, wenn\n",
    "- Niedriger Trainingsverlust, hoher Validierungsverlust: Ein deutlich niedrigerer Trainingsverlust im Vergleich zum Validierungsverlust deutet auf Overfitting hin. Das Modell lernt die Trainingsdaten sehr gut, kann aber die Validierungsdaten nicht gut generalisieren.\n",
    "\n",
    "- Schnelle Konvergenz des Trainingsverlusts: Der Trainingsverlust kann sehr schnell abnehmen und sich dennoch auf einem niedrigen Niveau stabilisieren. Dies kann darauf hindeuten, dass das Modell die Trainingsdaten auswendig gelernt hat.\n",
    "\n",
    "- Große Differenz zwischen Trainings- und Validierungsverlust: Overfitting zeigt sich in einer wachsenden Kluft zwischen Trainings- und Validierungsverlust. Während der Trainingsverlust weiter abnimmt, steigt der Validierungsverlust an.\n",
    "\n",
    "- Hohe Modellleistung auf Trainingsdaten: Das Modell erzielt eine hohe Genauigkeit oder R²-Wert auf den Trainingsdaten, jedoch eine niedrigere Leistung auf den Validierungs- oder Testdaten.\n",
    "\n",
    "- Sichtbares Overfitting in Lernkurven: Die Lernkurven können eine Sättigung des Trainingsverlusts zeigen, gefolgt von einem Anstieg des Validierungsverlusts, wenn das Modell übertrainiert wird.\n",
    "\n",
    "- Komplexe Modellarchitektur: Overfitting kann auftreten, wenn das Modell zu komplex ist und eine hohe Anzahl von Parametern hat, die es ihm ermöglichen, die Trainingsdaten genau zu modellieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
