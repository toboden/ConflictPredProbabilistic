{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Baseline\n",
    "At first all four datasets are modified in a way, that all of them contain the same countries with at least the last 36 months of observations. Countries that are not present in all datasets are not used for the minimization of the CRPS in dependecy of w or s. \n",
    "\n",
    "### Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model type\n",
    "estimModel = 'nbinom' #nbinom or hurdle\n",
    "\n",
    "# list of the (prediction) windows\n",
    "max_w = 36 #36\n",
    "window_list = list(range(2, max_w+1))\n",
    "\n",
    "# remove countries\n",
    "removeCountries = True\n",
    "lastX = 2 # all but the last x countries are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "## functions for the distribtion models\n",
    "# truncated negative binomial---------------------------------------------------\n",
    "def truncNegBin_CDF(y, n, p, log=True):\n",
    "    f_zero = nbinom.pmf(0, n, p)\n",
    "    if y > 0:\n",
    "        if log:\n",
    "            return np.log((nbinom.cdf(y, n, p) - nbinom.cdf(0, n, p)) / (1 - f_zero))\n",
    "        else:\n",
    "            return (nbinom.cdf(y, n, p) - nbinom.cdf(0, n, p)) / (1 - f_zero)\n",
    "    else:\n",
    "        if log:\n",
    "            return np.log(0)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "#log.p\tlogical; if TRUE, probabilities p are given as log(p)    \n",
    "def qnbinom_trunc(p, nNbinom, pNbinom, log_p=False):\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if (1 - nbinom.pmf(0, nNbinom, pNbinom)) == 1:\n",
    "        print('Juhu Abkürzung')\n",
    "        return nbinom.ppf(p, nNbinom, pNbinom)\n",
    "    else:\n",
    "        # Convert p to array if it's a single value\n",
    "        if not isinstance(p, (list, np.ndarray)):\n",
    "            p = np.array([p])\n",
    "        \n",
    "        # Set log-probabilities (lower tail)\n",
    "        n = len(p)\n",
    "        if log_p:\n",
    "            logp = p\n",
    "        else:\n",
    "            logp = np.log(p)\n",
    "        \n",
    "        # Set output and deal with special cases (outputs NA and Inf)\n",
    "        quantiles = np.full(n, np.nan)\n",
    "        nna = ~np.isnan(logp)\n",
    "        nlogp = logp[nna]\n",
    "        if len(nlogp) == 0:\n",
    "            return quantiles\n",
    "        \n",
    "        quantiles[nna] = np.full(len(nna), np.inf)\n",
    "        if np.min(nlogp) >= 0:\n",
    "            return quantiles\n",
    "\n",
    "        # calculate mean and variance out of n and p\n",
    "        mean = (nNbinom * (1 - pNbinom)) / pNbinom\n",
    "        var = (nNbinom * (1 - pNbinom)) / (pNbinom**2)\n",
    "\n",
    "        # Set log-CDF vector\n",
    "        lp_max = np.max(nlogp[nlogp < 0])\n",
    "\n",
    "        # find a adequate upper limit, starting from the extreme conservative chebychev inequality\n",
    "        upper = int(mean + np.sqrt(var/(1-np.exp(lp_max)))) #Chebychev inequality\n",
    "        while np.exp(truncNegBin_CDF(upper-50, nNbinom, pNbinom)) > 0.999:\n",
    "            upper = upper - 50\n",
    "\n",
    "        logcdf = np.array([truncNegBin_CDF(yi, nNbinom, pNbinom) for yi in range(1, int(upper)+1)]) \n",
    "\n",
    "        # Compute output\n",
    "        for i in range(n):\n",
    "            if nna[i]:\n",
    "                if logp[i] < 0:\n",
    "                    quantiles[i] = np.sum(logcdf < logp[i]) + 1 #+1 because 0 is truncated\n",
    "        \n",
    "        # Return output\n",
    "        if len(quantiles) == 1:\n",
    "            return quantiles[0]\n",
    "        else:\n",
    "            return quantiles\n",
    "\n",
    "# truncated poisson---------------------------------------------------\n",
    "def log1mexp(x):\n",
    "    if np.any((x < 0) & (~np.isnan(x))):\n",
    "        raise ValueError(\"Inputs need to be non-negative!\")\n",
    "    return np.where(x <= np.log(2), np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))\n",
    "\n",
    "def truncPois_CDF(y, mu, log=True):\n",
    "    f_zero = poisson.pmf(0, mu)\n",
    "    if y > 0:\n",
    "        if log:\n",
    "            return np.log((poisson.cdf(y, mu) - poisson.cdf(0, mu)) / (1 - f_zero))\n",
    "        else:\n",
    "            return (poisson.cdf(y, mu) - poisson.cdf(0, mu)) / (1 - f_zero)\n",
    "    else:\n",
    "        if log:\n",
    "            return np.log(0)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "def qpois_trunc(p, lam, lower_tail=True, log_p=False):\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if (1 - poisson.pmf(0, lam)) == 1:\n",
    "        print('Juhu Abkürzung')\n",
    "        return poisson.ppf(p, lam)\n",
    "    else:\n",
    "        # Convert p to array if it's a single value\n",
    "        if not isinstance(p, (list, np.ndarray)):\n",
    "            p = np.array([p])\n",
    "        \n",
    "        # Set log-probabilities (lower tail)\n",
    "        n = len(p)\n",
    "        if log_p:\n",
    "            logp = p\n",
    "        else:\n",
    "            logp = np.log(p)\n",
    "            \n",
    "        if not lower_tail:\n",
    "            logp = log1mexp(-logp)\n",
    "        \n",
    "        # Set output and deal with special cases (outputs NA and Inf)\n",
    "        quantiles = np.full(n, np.nan)\n",
    "        nna = ~np.isnan(logp)\n",
    "        nlogp = logp[nna]\n",
    "        if len(nlogp) == 0:\n",
    "            return quantiles\n",
    "        \n",
    "        quantiles[nna] = np.full(len(nna), np.inf)\n",
    "        if np.min(nlogp) >= 0:\n",
    "            return quantiles\n",
    "\n",
    "        # Set log-CDF vector\n",
    "        lp_max = np.max(nlogp[nlogp < 0])\n",
    "\n",
    "        # find a adequate upper limit, starting from the extreme conservative chebychev inequality\n",
    "        upper = int(lam + np.sqrt(lam * np.exp(-log1mexp(-lp_max)))) #Chebychev inequality\n",
    "        while np.exp(truncPois_CDF(upper-50, lam)) > 0.999:\n",
    "            upper = upper - 50\n",
    "\n",
    "        logcdf = np.array([truncPois_CDF(yi, lam) for yi in range(1, int(upper)+1)]) \n",
    "\n",
    "        # Compute output\n",
    "        for i in range(n):\n",
    "            if nna[i]:\n",
    "                if logp[i] < 0:\n",
    "                    quantiles[i] = np.sum(logcdf < logp[i]) + 1 #+1 because 0 is truncated\n",
    "        \n",
    "        # Return output\n",
    "        if len(quantiles) == 1:\n",
    "            return quantiles[0]\n",
    "        else:\n",
    "            return quantiles\n",
    "\n",
    "### function to compute distribution-------------------------------------------------\n",
    "def baseFatalModel_quantiles(featureSeries, quantiles, w=None, model='hurdle'):\n",
    "    # list to store quantiles \n",
    "    dummy_fatalities_list = []\n",
    "    # string to store model distribution\n",
    "    dist_string = ''\n",
    "\n",
    "    mean = None\n",
    "    var = None\n",
    "\n",
    "    numberQuantiles = len(quantiles)\n",
    "\n",
    "    # hurdle model\n",
    "    if model == 'hurdle':\n",
    "        if w == None:\n",
    "            \n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (featureSeries.value_counts().get(0, 0) / featureSeries.count())\n",
    "            # calculate n (r) and p via average/variance without the zero values\n",
    "            mean = pd.Series.mean(featureSeries[featureSeries != 0])\n",
    "            var = pd.Series.var(featureSeries[featureSeries != 0])\n",
    "\n",
    "        elif w <= 0:\n",
    "            return 'w has to be > 0'\n",
    "        \n",
    "        else:\n",
    "            features = featureSeries.tail(w).loc[:,'ged_sb']\n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (features.value_counts().get(0, 0) / features.count())\n",
    "            # calculate n (r) and p via average/variance without the zero values\n",
    "            mean = pd.Series.mean(features[features != 0])\n",
    "            var = pd.Series.var(features[features != 0])\n",
    "\n",
    "        # pd.Series.var or mean returns Nan in case of a passed series of length 1\n",
    "        if np.isnan(mean):\n",
    "            mean = 0\n",
    "        if np.isnan(var):\n",
    "            var = 0\n",
    "\n",
    "        # check if there are values above zero, otherwise no second component (trunc dist.) needed\n",
    "        if p_t > 0:\n",
    "            # component 1, y=0: Bernoulli\n",
    "            comp2_quantiles = [q for q in quantiles if q > (1-p_t)] #quantiles for the second component\n",
    "            removed_elements_length = numberQuantiles-len(comp2_quantiles)\n",
    "            zeros_array = np.zeros(removed_elements_length) #zero values that originate from the bernoulli dist\n",
    "\n",
    "            # component 2, y>0\n",
    "            if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                trunc_nbinom_quantiles = qnbinom_trunc(comp2_quantiles, n, p)\n",
    "\n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_nbinom_quantiles)).tolist()\n",
    "                dist_string = 'BernoulliTruncNBinom'\n",
    "\n",
    "            elif mean == 0 and var == 0: # due to faster calculation\n",
    "                dummy_fatalities_list = [0] * numberQuantiles\n",
    "                dist_string = 'BernoulliTruncPois'\n",
    "\n",
    "            else:  # equivalent to all means and 0 < var <= mean\n",
    "                trunc_pois_quantiles = qpois_trunc(comp2_quantiles, mean)\n",
    "                \n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_pois_quantiles)).tolist()\n",
    "                dist_string = 'BernoulliTruncPois'\n",
    "            \n",
    "        # p_t = 0 so no second component is needed    \n",
    "        else:\n",
    "            dummy_fatalities_list = [0] * numberQuantiles\n",
    "            dist_string = 'BernoulliHurdle'\n",
    "\n",
    "    # nbinom model\n",
    "    elif model == 'nbinom':\n",
    "        if w == None:\n",
    "             # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries)\n",
    "            var = pd.Series.var(featureSeries)\n",
    "        elif w <= 0:\n",
    "            return 'w has to be > 0'\n",
    "        else:\n",
    "            # calculate n (r) and p via average/variance\n",
    "            mean = pd.Series.mean(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "        if var != 0 and var > mean:\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "        elif mean == 0 and var == 0: # due to faster calculation\n",
    "                dummy_fatalities_list = [0] * numberQuantiles\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "        else: # equivalent to all means and 0 < var <= mean\n",
    "                dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "    return {'fatalities': dummy_fatalities_list, 'dist': dist_string, 'mean': mean, 'var': var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "data_folder_id = '1RigGnEyyNGnO_SPBSc_RwO9jjdbnPTAV'\n",
    "result_folder_id = '1CNBTHtBOTFXh01WUpP2EF1aEmDi0rSyg'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations to_oct_17\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "# function to check, if the last n months are in the dataset of a country,\n",
    "# other than that the last month of a country in the feature dataset has to be 3 months before the first actuals month!!\n",
    "def check_last_nMonths(n, country_id, yearindex):\n",
    "    country = country_feature_group_list[yearindex].get_group(country_id)\n",
    "\n",
    "    # reference month of the actual dataset\n",
    "    actual_month_list = actuals_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (actual_month_list[0] - 3) != country.index.get_level_values('month_id').unique().tolist()[-1]:\n",
    "        return False\n",
    "    else:\n",
    "        month_list = features_df_list[yearindex]['data'].index.get_level_values('month_id').unique().tolist()\n",
    "        last_month = month_list[-1] # equals the first month - 3 from the corresponding actuals dataset\n",
    "        first_month = month_list[0]\n",
    "\n",
    "        last_n_months = True\n",
    "\n",
    "        if last_month-n+1 < first_month:\n",
    "            last_n_months = False\n",
    "        else:\n",
    "            month_list = list(range(last_month-n+1, last_month+1))\n",
    "            \n",
    "            for month in month_list:\n",
    "                if month not in country.index.get_level_values('month_id'):\n",
    "                    last_n_months = False\n",
    "                    break\n",
    "\n",
    "        return last_n_months\n",
    "        #return True\n",
    "\n",
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))\n",
    "\n",
    "print(len(country_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify country_list so that it contains only country_ids \n",
    "# that have at least the last n months of observations in the last dataset (2020)!\n",
    "numberMonths_toOct20 = 72 # 72 = 3*12 (3 jahre für 2017) + 3*12 (jedes Jahr 12 Monate mehr also 2020 6 Jahre)\n",
    "\n",
    "#-- note------\n",
    "# dataset 2020 is used, because of the structure of the other datasets.\n",
    "# 2020 is dataset 2019 with 12 additional rows (months) etc.\n",
    "# for the CRPS calculation  of the datasets != 2020 the last 12*x windows are deleted\n",
    "# this procedure is saving computation time\n",
    "#-------------\n",
    "\n",
    "\n",
    "#IMPORTANT\n",
    "#if you do not minimize over all countries but only the single countries, \n",
    "# it is sufficient to check if all countries contain the last month in the features dataset (this way you use the full information). \n",
    "# But you still have to check check_last_nMonths(len(countrymonths), countryIndex, 3), so that no month is missing in between.\n",
    "\n",
    "# => so currently not all information is used for each country\n",
    "\n",
    "dummy_list = []\n",
    "for countryIndex in country_list:\n",
    "    dummy_hasLastN_months = True\n",
    "\n",
    "    # index 3 is the last dataset\n",
    "    if check_last_nMonths(numberMonths_toOct20, countryIndex, 3) is not True:\n",
    "        dummy_hasLastN_months = False  \n",
    "    \n",
    "    if dummy_hasLastN_months is True:\n",
    "        dummy_list.append(countryIndex)\n",
    "\n",
    "# the values in country_list are the 'country_id'\n",
    "country_list = dummy_list\n",
    "\n",
    "#IMPORTANT\n",
    "# all countries that have the last month as observation have the last 72 months as observations (in 2020)!!! so no country is excluded\n",
    "# checked by modifing the check_last_nMonths function -> else: return True\n",
    "\n",
    "len(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changes, so that the calculation does not take a long time -------------------\n",
    "if removeCountries == True:\n",
    "    # remove all but last x countries\n",
    "    elements_to_remove = country_list[0:(len(country_list)-lastX)] # only last x countries\n",
    "    country_list = [element for element in country_list if element not in elements_to_remove]\n",
    "    \n",
    "len(country_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The minimization is based on calculating the quantiles for each country, w and year (of the datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the predictions for each country\n",
    "baseline_country_predict_list = [{'country_id': country, 'prediction': {'2018': [], '2019': [], '2020': [], '2021': []}} for country in country_list]\n",
    "index_list = ['2018', '2019', '2020', '2021']\n",
    "s_prediction_list = list(range(3, 15))\n",
    "\n",
    "\n",
    "number_countries = len(country_list)\n",
    "number_dataframes = len(features_df_list)\n",
    "number_w = len(window_list)\n",
    "\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "dummy_quantile_list = [f\"{round(q * 100, 1)}%\" for q in quantiles]\n",
    "\n",
    "# loop through all countries (that are present in each dataset)\n",
    "for index in range(number_countries):\n",
    "    country = country_list[index]\n",
    "\n",
    "    print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "\n",
    "    # list to store the predictions for each year temporally\n",
    "    baseline_predict_list = [[] for _ in range(number_dataframes)]\n",
    "    \n",
    "    # loop through datasets\n",
    "    for i in range(len(index_list)): #range(number_dataframes): \n",
    "        features = country_feature_group_list[i].get_group(country) # features of country in dataset i\n",
    "        \n",
    "        baseline_predict_list[i] = []\n",
    "\n",
    "        # loop through windows\n",
    "        for j in range(number_w):\n",
    "            w = window_list[j] # current window\n",
    "\n",
    "            fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "            baseline_predict_list[i].append({'window':w, 'country_id':country, 'dist':fit['dist'], \n",
    "                                             'mean':fit['mean'], 'var':fit['var'], 'quantile':[], 'fatalities':[]}) \n",
    "\n",
    "            baseline_predict_list[i][j]['quantile'] = dummy_quantile_list    \n",
    "            baseline_predict_list[i][j]['fatalities'] = fit['fatalities']\n",
    "\n",
    "            baseline_predict_list[i][j] = pd.DataFrame(baseline_predict_list[i][j])\n",
    "            baseline_predict_list[i][j].set_index(['window', 'quantile'], inplace=True)\n",
    "\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]] = baseline_predict_list[i]\n",
    "\n",
    "        # combine each w dataset together\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]] = pd.concat(baseline_country_predict_list[index]['prediction'][index_list[i]], axis=0)\n",
    "        baseline_country_predict_list[index]['prediction'][index_list[i]].sort_index(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(country_list)\n",
    "#baseline_country_predict_list[8]['prediction']['2019'].xs(3, level = 'window')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1-4\n",
    "\n",
    "Optimize **w** (through the CRPS) regarding\n",
    "|            | datasets    | countries   | prediction windows |\n",
    "|------------|-------------|-------------|--------------------|\n",
    "| baseline 1 | all         | all         | all                |\n",
    "| baseline 2 | all         | inidvidual  | all                |\n",
    "| baseline 3 | all         | all         | individual         |\n",
    "| baseline 4 | all         | inidvidual  | individual         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store all crps values\n",
    "baseline_crps_list = [\n",
    "    {\n",
    "        'country_id': country,\n",
    "        'baseline': [\n",
    "            {'s': s, 'w': [], 'CRPS': []}\n",
    "            for s in s_prediction_list\n",
    "        ]\n",
    "    }\n",
    "    for country in country_list\n",
    "]\n",
    "\n",
    "# numver of prediction windows\n",
    "number_s = len(s_prediction_list)\n",
    "\n",
    "# fill list with crps calculations\n",
    "for s in s_prediction_list:\n",
    "    print('                  prediction window ' + str(s-2) + '/' + str(number_s), end='\\r')\n",
    "\n",
    "    for index in range(number_countries):\n",
    "        country = country_list[index]\n",
    "        print('country ' + str(index+1) + '/' + str(number_countries), end='\\r')\n",
    "            \n",
    "        for i in range(number_w):\n",
    "            w = window_list[i]\n",
    "            dummy_crps_list = [] \n",
    "\n",
    "            for j in range(number_dataframes):\n",
    "                year = actual_years[j]\n",
    "                monthly_totals_actuals = country_actual_group_list[j].get_group(country)\n",
    "                true_obs = monthly_totals_actuals.iloc[s-3,0]\n",
    "\n",
    "                NB_prediction = baseline_country_predict_list[index]['prediction'][year].xs(w, level=\"window\")\n",
    "\n",
    "                crps = pscore(NB_prediction.loc[:,'fatalities'].to_numpy(),true_obs).compute()[0]\n",
    "                dummy_crps_list.append(crps)\n",
    "\n",
    "            baseline_crps_list[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list))\n",
    "    \n",
    "# time to calculate: ~66 min with all 190 countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_baseline_crps_dict = {'w':[],'CRPS':[]}\n",
    "v2_baseline_crps_list = [{'country_id': country, 'baseline': {'w':[],'CRPS':[]}} for country in country_list]\n",
    "v3_baseline_crps_list = [{'s':s,'w':[],'CRPS':[]} for s in s_prediction_list]\n",
    "\n",
    "## baseline v1---------------------------------------------------------------------------\n",
    "# loop over w\n",
    "for j in range(number_w):\n",
    "    w = window_list[j]\n",
    "    dummy_crps_v1_list = []\n",
    "    # loop over countries\n",
    "    for i in range(number_countries):\n",
    "        # loop over prediction windows s\n",
    "        for k in range(number_s):\n",
    "            dummy_crps_v1_list.append(baseline_crps_list[i]['baseline'][k]['CRPS'][j])\n",
    "    v1_baseline_crps_dict['w'].append(w)\n",
    "    v1_baseline_crps_dict['CRPS'].append(np.mean(dummy_crps_v1_list))\n",
    "\n",
    "v1_baseline_crps = pd.DataFrame(v1_baseline_crps_dict)\n",
    "v1_baseline_crps = v1_baseline_crps[v1_baseline_crps.CRPS == v1_baseline_crps.loc[:,'CRPS'].min()]\n",
    "v1_baseline_crps.set_index(pd.Index(range(len(v1_baseline_crps))), inplace=True)\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "## baseline v2----------------------------------------------------------------------------\n",
    "# list for baseline v2\n",
    "for i in range(number_countries):\n",
    "    for j in range(number_w):\n",
    "        w = window_list[j]\n",
    "        dummy_crps_v2_list = []\n",
    "        for k in range(number_s):\n",
    "            dummy_crps_v2_list.append(baseline_crps_list[i]['baseline'][k]['CRPS'][j])\n",
    "        v2_baseline_crps_list[i]['baseline']['w'].append(w)\n",
    "        v2_baseline_crps_list[i]['baseline']['CRPS'].append(np.mean(dummy_crps_v2_list))\n",
    "    \n",
    "# dataframe with the w that minimizes the CRPS for every country (v2)\n",
    "data_v2 = {\n",
    "    'country_id':[],\n",
    "    'w':[],\n",
    "    'CRPS':[]\n",
    "}\n",
    "for i in range(len(v2_baseline_crps_list)):\n",
    "    # get the index of the minimal CRPS value\n",
    "    min_index = v2_baseline_crps_list[i]['baseline']['CRPS'].index(min(v2_baseline_crps_list[i]['baseline']['CRPS']))\n",
    "    \n",
    "    # store values in dict\n",
    "    data_v2['country_id'].append(v2_baseline_crps_list[i]['country_id'])\n",
    "    data_v2['w'].append(v2_baseline_crps_list[i]['baseline']['w'][min_index])\n",
    "    data_v2['CRPS'].append(v2_baseline_crps_list[i]['baseline']['CRPS'][min_index])\n",
    "    \n",
    "v2_baseline_crps = pd.DataFrame(data_v2)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## baseline v3---------------------------------------------------------------------------\n",
    "for s_index in range(number_s):\n",
    "    dummy_crps_v3_list = []\n",
    "    s = s_prediction_list[s_index]\n",
    "    for w_index in range(number_w):\n",
    "        w = window_list[w_index]\n",
    "        for i in range(number_countries):\n",
    "            dummy_crps_v3_list.append(baseline_crps_list[i]['baseline'][s_index]['CRPS'][w_index])\n",
    "        v3_baseline_crps_list[s_index]['w'].append(w)\n",
    "        v3_baseline_crps_list[s_index]['CRPS'].append(np.mean(dummy_crps_v3_list))\n",
    "\n",
    "# dataframe with the w that minimize the CRPS for each prediction window s\n",
    "data_v3 = {\n",
    "    's':[],\n",
    "    'w':[],\n",
    "    'CRPS':[]\n",
    "}\n",
    "# length of the v3_baseline list is the number of prediction windows\n",
    "for i in range(len(v3_baseline_crps_list)):\n",
    "    s = s_prediction_list[i]\n",
    "    # get the index of the minimal CRPS value\n",
    "    min_index = v3_baseline_crps_list[i]['CRPS'].index(min(v3_baseline_crps_list[i]['CRPS']))\n",
    "\n",
    "    # store values in dict\n",
    "    data_v3['s'].append(s)\n",
    "    data_v3['w'].append(v3_baseline_crps_list[i]['w'][min_index])\n",
    "    data_v3['CRPS'].append(v3_baseline_crps_list[i]['CRPS'][min_index])\n",
    "\n",
    "v3_baseline_crps = pd.DataFrame(data_v3)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "## baseline v4---------------------------------------------------------------------------\n",
    "v4_baseline_crps = [{'country_id':country,\n",
    "                    's':[],\n",
    "                    'w':[],\n",
    "                    'CRPS':[]\n",
    "                    } for country in country_list]\n",
    "\n",
    "# loop over all countries\n",
    "for i in range(len(baseline_crps_list)):\n",
    "    # loop over all prediction windows\n",
    "    for s_index in range(number_s):\n",
    "        s = s_prediction_list[s_index]\n",
    "        # get the index of the minimal CRPS value\n",
    "        min_index = baseline_crps_list[i]['baseline'][s_index]['CRPS'].index(min(baseline_crps_list[i]['baseline'][s_index]['CRPS']))\n",
    "    \n",
    "        # store values in dict\n",
    "        v4_baseline_crps[i]['s'].append(s)\n",
    "        v4_baseline_crps[i]['w'].append(baseline_crps_list[i]['baseline'][s_index]['w'][min_index])\n",
    "        v4_baseline_crps[i]['CRPS'].append(baseline_crps_list[i]['baseline'][s_index]['CRPS'][min_index])\n",
    "\n",
    "    v4_baseline_crps[i] = pd.DataFrame(v4_baseline_crps[i])\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of the overall CRPS to compare the impact of the level of detail in modeling \n",
    "dummy_array_v4 = []\n",
    "for countryData in v4_baseline_crps:\n",
    "    dummy_array_v4.append(np.mean(countryData.loc[:,'CRPS']))\n",
    "\n",
    "print('Overall CRPS')\n",
    "print('baseline 1: ' + str(np.round(v1_baseline_crps.iloc[0,1], decimals = 4)))\n",
    "print('baseline 2: ' + str(np.round(np.mean(v2_baseline_crps.loc[:,'CRPS']), decimals = 4)))\n",
    "print('baseline 3: ' + str(np.round(np.mean(v3_baseline_crps.loc[:,'CRPS']), decimals = 4)))\n",
    "print('baseline 4: ' + str(np.round(np.mean(dummy_array_v4), decimals = 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variables in joblib files\n",
    "variable_string = str(estimModel)+'Wmax'+str(max_w)+'last'+str(lastX)+'ctrs'\n",
    "filename = 'task2_optimal_baseline_' + variable_string + '.joblib'\n",
    "# save variables in joblib file\n",
    "dump([country_list, baseline_country_predict_list, baseline_crps_list, v1_baseline_crps_dict,\n",
    "      v2_baseline_crps_list, v3_baseline_crps_list,\n",
    "      v1_baseline_crps, v2_baseline_crps, v3_baseline_crps, v4_baseline_crps], filename)\n",
    "\n",
    "file1 = drive.CreateFile({'parents': [{'id': result_folder_id}]})\n",
    "file1.SetContentFile(filename)\n",
    "file1.Upload()\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
