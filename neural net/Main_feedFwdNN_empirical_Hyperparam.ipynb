{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set seed so that the model outputs are reproducible\n",
    "After the kernel is restarted the same results are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(0)\n",
    "\n",
    "# This will make TensorFlow ops as deterministic as possible, but it will\n",
    "# affect the overall performance, so it's not enabled by default.\n",
    "# `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_Actuals(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# check if the last month of a country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "def check_last_featureMonth(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        raise ValueError('country does not have actuals')\n",
    "\n",
    "\n",
    "    # last month of the feature dataset\n",
    "    last_feature_month = country_feature_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[-1]\n",
    "\n",
    "    # first month of the actual dataset\n",
    "    first_actual_month = country_actual_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[0]\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (first_actual_month - 3) != last_feature_month:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "data = features_df_list[-1]['data']\n",
    "if 'gleditsch_ward' in data.columns:\n",
    "    data = data.drop(columns='gleditsch_ward') # column not necessary\n",
    "\n",
    "## Features without missing values\n",
    "columns_without_missing_values = data.columns[data.notna().all()]\n",
    "\n",
    "for i in range(len(features_df_list)):\n",
    "    data_set = features_df_list[i]['data']\n",
    "    features_df_list[i]['data'] = data_set[columns_without_missing_values]\n",
    "\n",
    "all_features = features_df_list[-1]['data'].columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group data by country_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Länder aussortieren\n",
    "die nicht gefordert sind und\n",
    "*  die keine actuals haben\n",
    "*  die zu wenig Beobachtungen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_countrylist = os.path.join('..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "\n",
    "        if check_Actuals(country_id, 0):\n",
    "            if not check_last_featureMonth(country_id, 0): \n",
    "                month_list.append([str(country_id) +' last month missing'])\n",
    "            else:\n",
    "                month_list.append([number_months_feature_data, country_id])\n",
    "        else:\n",
    "            month_list.append(str(country_id) + ' no actuals')\n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative country subset\n",
    "Due to long computing time the hyperparameter tuning is based on a representative subsample of countries.\n",
    "\n",
    "- **Country 1: No Fatalities**  \n",
    "  This category involves situations where no fatalities have been recorded.\n",
    "\n",
    "- **Country 57: Extremely Rare Fatalities, with Sporadic High Peaks**  \n",
    "  In this scenario, fatalities are exceptionally infrequent, with only a few instances of significantly elevated fatalities, reaching the maximum across all countries, attaining as high as 48,183 within a single month.\n",
    "\n",
    "- **Country 235: Ceased Conflict**  \n",
    "  This class refers to conflicts that have come to an end.\n",
    "\n",
    "- **Country 13: Few Fatalities at Present, Greater Incidence in the Past**  \n",
    "  In this context, there are presently few fatalities, but historically, there were higher fatalities over a considerable period. The conflict causing these fatalities may have waned, yet remnants continue to pose a minor threat.\n",
    "\n",
    "- **Country 121: On-and-Off Conflict, Resuming and Ceasing Periodically**  \n",
    "  This scenario pertains to conflicts that had ceased, then restarted, and subsequently ceased again.\n",
    "\n",
    "- **Country 223: Sustained Moderate Fatalities, with a Single Month of Zero Fatalities**  \n",
    "  Here, the conflict is characterized by consistent moderate fatalities, with the exception of one occurrence out of 334 months when there were no fatalities.\n",
    "\n",
    "- **Country 220: Erupted Severe Conflict with High Fatalities and Conflict Trap**  \n",
    "  This category involves conflicts that have erupted, leading to significant fatalities and establishing a conflict trap, characterized by prolonged and sustained violence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_list = [1, 57, 235, 13, 121, 223, 220]\n",
    "country_list = [x for x in country_list if x in subsample_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "Goal is to estimate the empirical distribution of the fatalities per month.\n",
    "### Definition of the CRPS loss function and the Feed forward Neural Network subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to set the number and partition of the neurons\n",
    "def number_neurons_allLayers(inputNeurons, outputNeurons):\n",
    "    return np.round(np.mean([inputNeurons, outputNeurons]))\n",
    "\n",
    "def split_neurons_3hiddenlayer(numberNeurons):\n",
    "\n",
    "    neuronsHiddenLayer1 = np.round(numberNeurons * 0.5) \n",
    "    neuronsHiddenLayer2 = np.round(numberNeurons * 0.3)\n",
    "    neuronsHiddenLayer3 = numberNeurons - neuronsHiddenLayer1 - neuronsHiddenLayer2\n",
    "\n",
    "    return int(neuronsHiddenLayer1), int(neuronsHiddenLayer2), int(neuronsHiddenLayer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)\n",
    "\n",
    "# Define custom ReLU activation function\n",
    "class ReLUTransform(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "# Define the Feed Forward Neural Network subclass\n",
    "class FeedForwardNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, name=\"FeedFwdNN\"):\n",
    "        super(FeedForwardNN, self).__init__(name=name)\n",
    "        \n",
    "        neurons_input = input_shape[-1]\n",
    "        neurons_output = 200\n",
    "        neurons_hidden_layer = number_neurons_allLayers(neurons_input,neurons_output)\n",
    "        #neuronsL1, neuronsL2, neuronsL3 = split_neurons_3hiddenlayer(neurons_hidden_layer)\n",
    "\n",
    "\n",
    "        self.hidden_layer1 = Dense(neurons_hidden_layer, activation='relu')\n",
    "        self.dropout = Dropout(0.3)\n",
    "        #self.hidden_layer2 = Dense(neuronsL2, activation='relu')\n",
    "        #self.hidden_layer3 = Dense(neuronsL3, activation='relu')\n",
    "        self.untransformed_output = Dense(neurons_output)\n",
    "        self.final_output = Lambda(ReLUTransform())\n",
    "\n",
    "        self.model = self.build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.hidden_layer1(inputs)\n",
    "        x = self.dropout(x)\n",
    "        #x = self.hidden_layer2(x)\n",
    "        #x = self.hidden_layer3(x)\n",
    "        x = self.untransformed_output(x)\n",
    "        y = self.final_output(x)\n",
    "        return y\n",
    "\n",
    "# Definiere den EarlyStopping-Callback\n",
    "patience = 3\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the functions to perform the train-/validate-/test-split with rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "## function used to calculate w_max, number of rolling windows etc.\n",
    "# length of a whole window (containing w input months and 12 acutal months)\n",
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "def number_valid_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    number_train_valid_months = numberMonths_available - w\n",
    "    number_valid_months = math.floor(number_train_valid_months * relative_validation_size)\n",
    "\n",
    "    return number_valid_months\n",
    "\n",
    "# number of months available for training (after removing the validation and test months)\n",
    "def number_train_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    valid_months = number_valid_months(numberMonths_available, w, relative_validation_size)\n",
    "\n",
    "    #  all months feature data   -  validate set    -   test set input\n",
    "    return numberMonths_available - valid_months - w\n",
    "\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "\n",
    "def find_max_W(numberMonths_available, w_min, w_max, relative_validation_size):\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one validation window\n",
    "    number_valid_months_wmin = number_valid_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_valid_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one validation window with w_min = ' + str(w_min))\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one train window\n",
    "    number_train_months_wmin = number_train_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_train_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "    # find the maximal w\n",
    "    max_W = w_max\n",
    "    number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "    number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while (number_valid_rollwindows_wmax == 0 or number_train_rollwindows_wmax == 0) and max_W > w_min:\n",
    "        max_W -= 1\n",
    "        number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "        number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "## conflict trap\n",
    "# drops all months before a starting conflict\n",
    "# defintion of a beginning conflict: fatalities(monthX) > 0 with mean(fatalities(window_size number months starting with monthX)) > threshold\n",
    "# (average fatalities per month in the starting half year are greater than the threshold)\n",
    "# iterates trough the dataset beginning with the first entry\n",
    "def drop_before_conflict_trap(data, threshold, window_size, minimal_data_size):\n",
    "    index_ged_sb = data.columns.get_loc('ged_sb')\n",
    "\n",
    "    start_index = 0\n",
    "    while start_index < len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "        window = data.iloc[start_index:start_index + window_size, index_ged_sb].to_list()\n",
    "        \n",
    "        if window[0] > 0 and sum(window) / window_size >= threshold:\n",
    "            break\n",
    "        else:\n",
    "            start_index += 1\n",
    "\n",
    "\n",
    "    if len(data) >= minimal_data_size:\n",
    "        # if there is no conflict trap do nothing\n",
    "        if start_index == len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "            return data\n",
    "        # if the truncation would result in a too small dataset prevent this\n",
    "        elif len(data.iloc[start_index:, :]) < minimal_data_size:\n",
    "            return data.iloc[-minimal_data_size:, :]\n",
    "        # drop every entry before the conflict trap \n",
    "        else:\n",
    "            return data.iloc[start_index:, :]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "\n",
    "def TrainValid_ArrayXY_split(train_months, w, month_list, data_feature, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    number_rolling_windows_train = number_rolling_windows(train_months, w)\n",
    "\n",
    "    for i in range(0, number_rolling_windows_train):\n",
    "        starting_month_features = month_list[i]\n",
    "\n",
    "        index_ending_month_features = i + w - 1\n",
    "        ending_month_features = month_list[index_ending_month_features]\n",
    "\n",
    "        starting_month_unrActuals = month_list[index_ending_month_features + 3]\n",
    "        ending_month_unrActuals = month_list[index_ending_month_features + 14]\n",
    "\n",
    "        window_features = data_feature.loc[slice(starting_month_features, ending_month_features), :] # excluding \"unreal\" actuals\n",
    "        window_actuals = data_feature.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "        normalized_window_features = preprocessing.normalize(window_features)\n",
    "        window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "        window_actual_array = np.array([window_actuals])\n",
    "\n",
    "        X.append(window_features_array)\n",
    "        Y.append(window_actual_array)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def Test_ArrayXY_split(month_list, data_feature, data_actual, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    starting_month_test = month_list[0]\n",
    "    ending_month_test = month_list[-1]\n",
    "\n",
    "    window_features_test = data_feature.loc[slice(starting_month_test, ending_month_test), :] # all w features to predict the fatalities\n",
    "    window_actuals_test = data_actual.iloc[s - 3].values # real actuals\n",
    "\n",
    "    normalized_window_features_test = preprocessing.normalize(window_features_test)\n",
    "    window_features_array_test = np.array([normalized_window_features_test.flatten()])[0]\n",
    "\n",
    "    window_actual_array_test = window_actuals_test\n",
    "\n",
    "    X.append(window_features_array_test)\n",
    "    Y.append(window_actual_array_test)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the future fatalites\n",
    "### Feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different feature selection from views\n",
    "# 59 features that map the conflict history of a country\n",
    "conflict_history = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'ln_ged_sb_tlag_1',\n",
    "    'ln_ged_sb_tlag_2', 'ln_ged_sb_tlag_3', 'ln_ged_sb_tlag_4',\n",
    "    'ln_ged_sb_tlag_5', 'ln_ged_sb_tlag_6', 'ln_ged_sb_tsum_24',\n",
    "    'decay_ged_sb_100', 'decay_ged_sb_500', 'decay_ged_os_100',\n",
    "    'decay_ged_ns_5', 'decay_ged_ns_100', 'ln_ged_ns', 'ln_ged_os',\n",
    "    'ln_acled_sb', 'ln_acled_sb_count', 'ln_acled_os',\n",
    "    'ln_ged_os_tlag_1', 'decay_acled_sb_5', 'decay_acled_os_5',\n",
    "    'decay_acled_ns_5', 'splag_1_decay_ged_os_5',\n",
    "    'splag_1_decay_ged_ns_5'\n",
    "]\n",
    "# 59 features that are drawn from the Varieties of Democracy project\n",
    "vdem = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'vdem_v2x_delibdem',\n",
    "    'vdem_v2x_egaldem', 'vdem_v2x_libdem', 'vdem_v2x_libdem_48',\n",
    "    'vdem_v2x_partip', 'vdem_v2x_accountability',\n",
    "    'vdem_v2x_civlib', 'vdem_v2x_clphy', 'vdem_v2x_cspart',\n",
    "    'vdem_v2x_divparctrl', 'vdem_v2x_edcomp_thick', 'vdem_v2x_egal',\n",
    "    'vdem_v2x_execorr', 'vdem_v2x_frassoc_thick', 'vdem_v2x_gencs',\n",
    "    'vdem_v2x_gender', 'vdem_v2x_genpp', 'vdem_v2x_horacc',\n",
    "    'vdem_v2x_neopat', 'vdem_v2x_pubcorr', 'vdem_v2x_rule',\n",
    "    'vdem_v2x_veracc', 'vdem_v2x_freexp', 'vdem_v2xcl_acjst', \n",
    "    'vdem_v2xcl_dmove', 'vdem_v2xcl_prpty', 'vdem_v2xcl_rol', \n",
    "    'vdem_v2xcl_slave', 'vdem_v2xdl_delib', 'vdem_v2xeg_eqdr',\n",
    "    'vdem_v2xeg_eqprotec', 'vdem_v2xel_frefair', 'vdem_v2xel_regelec',\n",
    "    'vdem_v2xme_altinf', 'vdem_v2xnp_client', 'vdem_v2xnp_regcorr',\n",
    "    'vdem_v2xpe_exlecon', 'vdem_v2xpe_exlpol', 'vdem_v2xpe_exlgeo',\n",
    "    'vdem_v2xpe_exlgender', 'vdem_v2xpe_exlsocgr', 'vdem_v2xps_party',\n",
    "    'vdem_v2xcs_ccsi', 'vdem_v2xnp_pres', 'vdem_v2xeg_eqaccess',\n",
    "    'vdem_v2x_diagacc', 'vdem_v2clrgunev', 'splag_vdem_v2x_libdem',\n",
    "    'splag_vdem_v2xcl_dmove', 'splag_vdem_v2x_accountability',\n",
    "    'splag_vdem_v2xpe_exlsocgr', 'splag_vdem_v2xcl_rol', 'wdi_sm_pop_netm',\n",
    "    'wdi_sp_dyn_imrt_in'\n",
    "]\n",
    "\n",
    "# 30 features that are drawn from the WDI as well as some conflict history indicators\n",
    "wdi = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'wdi_ag_lnd_frst_k2',\n",
    "    'wdi_dt_oda_odat_pc_zs', 'wdi_ms_mil_xpnd_gd_zs', 'wdi_ms_mil_xpnd_zs',\n",
    "    'wdi_nv_agr_totl_kd', 'wdi_nv_agr_totl_kn', 'wdi_ny_gdp_pcap_kd',\n",
    "    'wdi_sp_dyn_le00_in', 'wdi_se_prm_nenr', 'wdi_sh_sta_maln_zs', \n",
    "    'wdi_sh_sta_stnt_zs', 'wdi_sl_tlf_totl_fe_zs', 'wdi_sm_pop_refg_or', \n",
    "    'wdi_sm_pop_netm', 'wdi_sm_pop_totl_zs', 'wdi_sp_dyn_imrt_in', \n",
    "    'wdi_sh_dyn_mort_fe', 'wdi_sp_pop_1564_fe_zs', 'wdi_sp_pop_65up_fe_zs',\n",
    "    'wdi_sp_pop_grow', 'wdi_sp_urb_totl_in_zs',\n",
    "    'splag_wdi_sl_tlf_totl_fe_zs', 'splag_wdi_sm_pop_refg_or',\n",
    "    'splag_wdi_sm_pop_netm', 'splag_wdi_ag_lnd_frst_k2'\n",
    "]\n",
    "\n",
    "feature_subset_dict = {'conflict_history':conflict_history,\n",
    "                       'vdem':vdem,\n",
    "                       'wdi':wdi,\n",
    "                       'all':all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002816B22A4D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002816B229C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [06:03<36:21, 363.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [12:54<32:36, 391.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [19:01<25:21, 380.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [21:14<28:19, 424.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2280\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2279\u001b[0m \u001b[39mwith\u001b[39;00m c_api_util\u001b[39m.\u001b[39mtf_buffer() \u001b[39mas\u001b[39;00m buf:\n\u001b[1;32m-> 2280\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39;49mTF_OperationGetAttrValueProto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_c_op, name, buf)\n\u001b[0;32m   2281\u001b[0m   data \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_GetBuffer(buf)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'CRPSLoss/Pow_97' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps_utils.py:105\u001b[0m, in \u001b[0;36mget_read_write_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m   read_only_input_indices \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49mget_attr(READ_ONLY_RESOURCE_INPUTS_ATTR)\n\u001b[0;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m   \u001b[39m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2284\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2282\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   2283\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 2284\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m   2285\u001b[0m x \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'CRPSLoss/Pow_97' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlearningRate), loss\u001b[39m=\u001b[39mCRPSLoss())\n\u001b[0;32m    114\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, Y_train, \n\u001b[0;32m    116\u001b[0m                     batch_size\u001b[39m=\u001b[39;49mbatchSize, epochs\u001b[39m=\u001b[39;49mepochSize,\n\u001b[0;32m    117\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(X_validate, Y_validate),\n\u001b[0;32m    118\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[early_stopping],\n\u001b[0;32m    119\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    121\u001b[0m \u001b[39m## prediction\u001b[39;00m\n\u001b[0;32m    122\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1777\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1778\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1789\u001b[0m         pss_evaluation_shards\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1790\u001b[0m     )\n\u001b[1;32m-> 1791\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1792\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1793\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1794\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1795\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1796\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1797\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1798\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1799\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1800\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1801\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1802\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1803\u001b[0m )\n\u001b[0;32m   1804\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1805\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1806\u001b[0m }\n\u001b[0;32m   1807\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2196\u001b[0m             \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   2197\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2198\u001b[0m             ):\n\u001b[0;32m   2199\u001b[0m                 callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2200\u001b[0m                 logs \u001b[39m=\u001b[39m test_function_runner\u001b[39m.\u001b[39;49mrun_step(\n\u001b[0;32m   2201\u001b[0m                     dataset_or_iterator,\n\u001b[0;32m   2202\u001b[0m                     data_handler,\n\u001b[0;32m   2203\u001b[0m                     step,\n\u001b[0;32m   2204\u001b[0m                     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pss_evaluation_shards,\n\u001b[0;32m   2205\u001b[0m                 )\n\u001b[0;32m   2207\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2208\u001b[0m \u001b[39m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   3999\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(\u001b[39mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[1;32m-> 4000\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function(dataset_or_iterator)\n\u001b[0;32m   4001\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   4002\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:873\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 873\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m    874\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    876\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 694\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    695\u001b[0m     \u001b[39m.\u001b[39m_get_concrete_function_internal_garbage_collected(\n\u001b[0;32m    696\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    699\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:176\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m--> 176\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[0;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[0;32m    169\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:398\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[0;32m    396\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[1;32m--> 398\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[0;32m    399\u001b[0m     args, kwargs, func_graph)\n\u001b[0;32m    401\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:305\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[1;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[0;32m    304\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[1;32m--> 305\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m    307\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m    308\u001b[0m         args,\n\u001b[0;32m    309\u001b[0m         kwargs,\n\u001b[0;32m    310\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    311\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[0;32m    312\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m    313\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[0;32m    314\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m    316\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m    317\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:987\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    985\u001b[0m   deps_control_manager \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mNullContextmanager()\n\u001b[1;32m--> 987\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default(), deps_control_manager \u001b[39mas\u001b[39;00m deps_ctx:\n\u001b[0;32m    988\u001b[0m   current_scope \u001b[39m=\u001b[39m variable_scope\u001b[39m.\u001b[39mget_variable_scope()\n\u001b[0;32m    989\u001b[0m   default_use_resource \u001b[39m=\u001b[39m current_scope\u001b[39m.\u001b[39muse_resource\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:459\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    456\u001b[0m resource_inputs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    457\u001b[0m \u001b[39m# Check for any resource inputs. If we find any, we update control_inputs\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[39m# and last_write_to_resource.\u001b[39;00m\n\u001b[1;32m--> 459\u001b[0m \u001b[39mfor\u001b[39;00m inp, resource_type \u001b[39min\u001b[39;00m _get_resource_inputs(op):\n\u001b[0;32m    460\u001b[0m   is_read \u001b[39m=\u001b[39m resource_type \u001b[39m==\u001b[39m ResourceType\u001b[39m.\u001b[39mREAD_ONLY\n\u001b[0;32m    461\u001b[0m   input_id \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mtensor_id(inp)\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:608\u001b[0m, in \u001b[0;36m_get_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_resource_inputs\u001b[39m(op):\n\u001b[0;32m    607\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m   reads, writes \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mget_read_write_resource_inputs(op)\n\u001b[0;32m    609\u001b[0m   saturated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    610\u001b[0m   \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m saturated:\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps_utils.py:108\u001b[0m, in \u001b[0;36mget_read_write_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    105\u001b[0m   read_only_input_indices \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39mget_attr(READ_ONLY_RESOURCE_INPUTS_ATTR)\n\u001b[0;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m   \u001b[39m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m   writes\u001b[39m.\u001b[39mupdate(t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m op\u001b[39m.\u001b[39;49minputs \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mresource)\n\u001b[0;32m    109\u001b[0m   \u001b[39mreturn\u001b[39;00m (reads, writes)\n\u001b[0;32m    111\u001b[0m read_only_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2184\u001b[0m, in \u001b[0;36mOperation.inputs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2181\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The sequence of `Tensor` objects representing the data inputs of this op.\"\"\"\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2183\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2184\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(\n\u001b[0;32m   2185\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49m_get_tensor_by_tf_output(i)\n\u001b[0;32m   2186\u001b[0m       \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m pywrap_tf_session\u001b[39m.\u001b[39;49mGetOperationInputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_c_op))\n\u001b[0;32m   2187\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m   2188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2185\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2181\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The sequence of `Tensor` objects representing the data inputs of this op.\"\"\"\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2183\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2184\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m-> 2185\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49m_get_tensor_by_tf_output(i)\n\u001b[0;32m   2186\u001b[0m       \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pywrap_tf_session\u001b[39m.\u001b[39mGetOperationInputs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op))\n\u001b[0;32m   2187\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m   2188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3738\u001b[0m, in \u001b[0;36mGraph._get_tensor_by_tf_output\u001b[1;34m(self, tf_output)\u001b[0m\n\u001b[0;32m   3725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_tensor_by_tf_output\u001b[39m(\u001b[39mself\u001b[39m, tf_output):\n\u001b[0;32m   3726\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns the `Tensor` representing `tf_output`.\u001b[39;00m\n\u001b[0;32m   3727\u001b[0m \n\u001b[0;32m   3728\u001b[0m \u001b[39m  Note that there is only one such `Tensor`, i.e. multiple calls to this\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3736\u001b[0m \u001b[39m    The `Tensor` that represents `tf_output`.\u001b[39;00m\n\u001b[0;32m   3737\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3738\u001b[0m   op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_operation_by_tf_operation(tf_output\u001b[39m.\u001b[39;49moper)\n\u001b[0;32m   3739\u001b[0m   \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39moutputs[tf_output\u001b[39m.\u001b[39mindex]\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3701\u001b[0m, in \u001b[0;36mGraph._get_operation_by_tf_operation\u001b[1;34m(self, tf_oper)\u001b[0m\n\u001b[0;32m   3700\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_operation_by_tf_operation\u001b[39m(\u001b[39mself\u001b[39m, tf_oper):\n\u001b[1;32m-> 3701\u001b[0m   op_name \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_OperationName(tf_oper)\n\u001b[0;32m   3702\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_operation_by_name(op_name)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### prediction\n",
    "prediction_year = '2021' # 2019, 2020, 2021\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "\n",
    "s_prediction_list = list(range(3, 15))\n",
    "#s_prediction_list = list(range(3, 4))\n",
    "\n",
    "number_countries = len(country_list)\n",
    "\n",
    "\n",
    "pred_year_string = 'prediction_' + prediction_year\n",
    "# list to save the predictions for each country\n",
    "NNet_prediction_list = [{'country_id': country, pred_year_string: []} for country in country_list]\n",
    "\n",
    "\n",
    "### set hyperparameters for the prediction task\n",
    "## dataset (split) hyperparams\n",
    "rel_validation_size = 0.3 # percentual size of the validation set\n",
    "var_threshold = 0.05 # variance threshold for dropping columns\n",
    "mean_fatlities_per_month_threshold = 5 # threshold for the average number of fatalities per month (conflict trap detection)\n",
    "\n",
    "w_max = 24 # the maximal w (months to estimate the fatalities from) is set to e.g. 3 years (36 months)\n",
    "# BUT: If w_max=36 leads to a number of rolling windows < 1 in the validation dataset,\n",
    "# w_max is set to the maximal w, so that the number of rolling windows is >= 1\n",
    "# => this step is done below in the section for each country\n",
    "\n",
    "w_min = 2 # to calculate the w_max the w_min has to be set as well\n",
    "\n",
    "## neural net hyperparams\n",
    "batchSize = 1\n",
    "epochSize = 100\n",
    "learningRate = 0.1\n",
    "\n",
    "feature_subset = 'wdi' # 'conflict_history', 'vdem', 'wdi', 'all', \n",
    "\n",
    "# loop through all countries\n",
    "for country_index in tqdm(range(number_countries)):\n",
    "    country = country_list[country_index]\n",
    "\n",
    "    ## HIER TRY Catch und dann nuller in liste des country\n",
    "    \"\"\" # check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "    if not check_last_featureMonth(prediction_country_id, dataset_index):\n",
    "        raise ValueError('last month is not contained in the data') \"\"\"\n",
    "\n",
    "\n",
    "    ### load and prepare datasets\n",
    "    feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    # only FEATURE SUBSET\n",
    "    feature_data = feature_data.loc[:,feature_subset_dict[feature_subset]]\n",
    "\n",
    "    ## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "    columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "    feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "    actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    ## remove months before the CONDFLICT TRAP (regime change)\n",
    "    # if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "    # the conflict trap starts and all obsservations before that month are dropped\n",
    "    # 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "    feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "\n",
    "    ## final feature dataset months\n",
    "    # numbers of months from the feature dataset\n",
    "    month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "    first_month = min(month_list_feature_data)\n",
    "    last_month = max(month_list_feature_data)\n",
    "    number_months_feature_data = len(month_list_feature_data) # number of months in the feature dataset\n",
    "\n",
    "    # loop over prediction horizons s\n",
    "    for s in s_prediction_list:\n",
    "\n",
    "        w_max_local = find_max_W(number_months_feature_data, w_min, w_max, rel_validation_size) # if not enough data w < w_max and w >= w_min\n",
    "        w = w_max_local\n",
    "\n",
    "        ### split data in train-, validation- and test-dataset\n",
    "        \"\"\" The data sizes are calculated in a way, that missing months are no problem. Additionaly due to the fact, that the data is sorted\n",
    "            regarding months, this step can be skipped. \"\"\"\n",
    "\n",
    "        # length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "        roll_window_len = rollingWindowLength(w)\n",
    "        n_train_months = number_train_months(number_months_feature_data, w, rel_validation_size)\n",
    "        n_valid_months = number_valid_months(number_months_feature_data, w, rel_validation_size)\n",
    "        n_test_months = w\n",
    "\n",
    "        # Split: train-valid-test\n",
    "        month_list_train = month_list_feature_data[0:n_train_months]\n",
    "        month_list_valid = month_list_feature_data[n_train_months:(n_train_months+n_valid_months)]\n",
    "        month_list_test = month_list_feature_data[number_months_feature_data-n_test_months:]\n",
    "\n",
    "        ## training dataset------\n",
    "        X_train, Y_train = TrainValid_ArrayXY_split(n_train_months, w, month_list_train, feature_data, s)\n",
    "\n",
    "        ## validation dataset--------\n",
    "        X_validate, Y_validate = TrainValid_ArrayXY_split(n_valid_months, w, month_list_valid, feature_data, s)\n",
    "\n",
    "        ## test dataset-------\n",
    "        X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "        \n",
    "        ### prediction with the neural net\n",
    "        ## Define inputs with predefined shape\n",
    "        input_shape = (len(X_train[0]),)\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        ## define neural net\n",
    "        # Create an instance of the FeedForwardNN model\n",
    "        nameString = 'FFwdNN_country' + str(country) + '_s' + str(s)\n",
    "        model = FeedForwardNN(input_shape=inputs.shape, name=nameString)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=learningRate), loss=CRPSLoss())\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(X_train, Y_train, \n",
    "                            batch_size=batchSize, epochs=epochSize,\n",
    "                            validation_data=(X_validate, Y_validate),\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0, shuffle=False)\n",
    "\n",
    "        ## prediction\n",
    "        prediction = model.predict(X_test)\n",
    "        # round the prediction values to integers\n",
    "        empirical_distribution = np.round(np.sort(prediction[0])).astype(int)\n",
    "\n",
    "        # save the training and validation loss to generate plots\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(1, len(loss) + 1)\n",
    "\n",
    "        y_true = Y_test[0][0]\n",
    "        crps_prediction = pscore(empirical_distribution,y_true).compute()[0]\n",
    "\n",
    "\n",
    "        NNet_prediction_list[country_index][pred_year_string].append({'s':s, 'w':w, 'distribution':empirical_distribution, \n",
    "                                                                 'actual':y_true, 'CRPS':crps_prediction,\n",
    "                                                                 'loss':loss, 'val_loss':val_loss, 'epochs':epochs})\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_values_valid = []\n",
    "for country_data in NNet_prediction_list:\n",
    "    country_crps_list = []\n",
    "    for entry in country_data[pred_year_string]:\n",
    "        country_crps_list.append(min(entry['val_loss']))\n",
    "    crps_values_valid.append(np.mean(country_crps_list))\n",
    "\n",
    "mean_crps_valid = np.mean(crps_values_valid)\n",
    "\n",
    "\n",
    "crps_values_test = []\n",
    "for country_data in NNet_prediction_list:\n",
    "    country_crps_list = []\n",
    "    for entry in country_data[pred_year_string]:\n",
    "        country_crps_list.append(entry['CRPS'])\n",
    "    crps_values_test.append(np.mean(country_crps_list))\n",
    "\n",
    "mean_crps_test = np.mean(crps_values_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "s = 3\n",
    "i = 0 #zeile\n",
    "j = 0 #spalte\n",
    "# Erstelle eine Figur mit 1 Zeile und 2 Spalten für die beiden Plots nebeneinander und kleiner\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(14, 7))\n",
    "\n",
    "for country_data in NNet_prediction_list:\n",
    "    loss = country_data[pred_year_string][s-3]['loss']\n",
    "    val_loss = country_data[pred_year_string][s-3]['val_loss']\n",
    "    epochs = country_data[pred_year_string][s-3]['epochs']\n",
    "        \n",
    "    title_string = str(country_data['country_id']) + ' s = ' + str(s) + ' w_max = ' + str(country_data[pred_year_string][s-3]['w'])\n",
    "\n",
    "    axes[i][j].plot(epochs, loss, 'y', label='Traing')\n",
    "    axes[i][j].plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    axes[i][j].set_title(title_string)\n",
    "    axes[i][j].set_xlabel('Epochs')\n",
    "    axes[i][j].set_ylabel('Loss')\n",
    "    axes[i][j].legend()\n",
    "    axes[i][j].grid(True)\n",
    "    if j == 3:\n",
    "        i += 1\n",
    "        j = 0\n",
    "    else:  \n",
    "        j += 1\n",
    "\n",
    "print('Mean CRPS validate = ' + str(mean_crps_valid) + ' \\\\'+'\\\\')\n",
    "print('Mean CRPS test = ' + str(mean_crps_test) + ' \\\\'+'\\\\')\n",
    "print('Parameters data prep - VALID-TRAIN-TEST: ' + ' \\\\'+'\\\\' + 'wMax='+ str(w_max) + ' validationSize='+str(rel_validation_size) + ' conflictTrapThresh=' + str(mean_fatlities_per_month_threshold) + ' features='+feature_subset+ ' \\\\'+'\\\\')\n",
    "print('Parameters NN: ' + ' \\\\'+'\\\\' + 'earlyStoppingPatience=' + str(patience) + ' learningRate='+str(learningRate)+' numberHiddenLayers=3 numberNeurons=neuronshiddenlayer validationSize='+str(rel_validation_size))\n",
    "print('')\n",
    "\n",
    "plt.tight_layout()  # Optimiere den Abstand zwischen den Subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
