{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set seed so that the model outputs are reproducible\n",
    "After the kernel is restarted the same results are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(0)\n",
    "\n",
    "# This will make TensorFlow ops as deterministic as possible, but it will\n",
    "# affect the overall performance, so it's not enabled by default.\n",
    "# `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_Actuals(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# check if the last month of a country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "def check_last_featureMonth(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        raise ValueError('country does not have actuals')\n",
    "\n",
    "\n",
    "    # last month of the feature dataset\n",
    "    last_feature_month = country_feature_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[-1]\n",
    "\n",
    "    # first month of the actual dataset\n",
    "    first_actual_month = country_actual_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[0]\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (first_actual_month - 3) != last_feature_month:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "data = features_df_list[-1]['data']\n",
    "data = data.drop(columns='gleditsch_ward') # column not necessary\n",
    "\n",
    "## Features without missing values\n",
    "columns_without_missing_values = data.columns[data.notna().all()]\n",
    "data = data[columns_without_missing_values]\n",
    "\n",
    "## Features with near zero variance\n",
    "var_threshold = 0.05 # variance threshold\n",
    "columns_to_keep = [col for col in data.columns if data[col].var() >= var_threshold]\n",
    "\n",
    "\n",
    "for i in range(len(features_df_list)):\n",
    "    data = features_df_list[i]['data']\n",
    "    features_df_list[i]['data'] = data[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Länder aussortieren\n",
    "die nicht gefordert sind und\n",
    "*  die keine actuals haben\n",
    "*  die zu wenig Beobachtungen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_countrylist = os.path.join('..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "\n",
    "        if check_Actuals(country_id, 0):\n",
    "            if not check_last_featureMonth(country_id, 0): \n",
    "                month_list.append([str(country_id) +' last month missing'])\n",
    "            else:\n",
    "                month_list.append([number_months_feature_data, country_id])\n",
    "        else:\n",
    "            month_list.append(str(country_id) + ' no actuals')\n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative country subset\n",
    "Due to long computing time the hyperparameter tuning is based on a representative subsample of countries.\n",
    "\n",
    "* 1 keine todesfälle\n",
    "* 57 extrem selten todesfälle aber ein paar sind extrem hoch (maximum aller länder -> in einem Monat: 48183.0)\n",
    "* 235 krieg der aufgehört hat\n",
    "* 13 aktuell wenige todesfälle vor langer zeit mehr (krieg der aufgehört hat aber noch immer etwas schwelt)\n",
    "* 121 krieg der aufgehört hat aber wieder angefangen hat und dann wieder aufgehört hat\n",
    "* 223 konstant mittlere todesfälle, nur 1 von 334 monaten 0 \n",
    "* 220 ausgebrochener krieg mit sehr hohen todeszahlen und conflict trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_list = [1, 57, 235, 13, 121, 223, 220]\n",
    "country_list = [x for x in country_list if x in subsample_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "Goal is to estimate the empirical distribution of the fatalities per month.\n",
    "### Definition of the CRPS loss function and the Feed forward Neural Network subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)\n",
    "\n",
    "# Define custom ReLU activation function\n",
    "class ReLUTransform(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "# Define the Feed Forward Neural Network subclass\n",
    "class FeedForwardNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, name=\"FeedFwdNN\"):\n",
    "        super(FeedForwardNN, self).__init__(name=name)\n",
    "        \n",
    "        neurons_input = input_shape[-1]\n",
    "        neurons_output = 200\n",
    "        neurons_hidden_layer = np.round(np.mean([neurons_input,neurons_output]))\n",
    "\n",
    "        #self.hidden_layer1 = Dense(neurons_hidden_layer/2, activation='relu')\n",
    "        self.hidden_layer1 = Dense(4, activation='relu')\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.hidden_layer2 = Dense(neurons_hidden_layer/2, activation='relu')\n",
    "        self.untransformed_output = Dense(neurons_output)\n",
    "        self.final_output = Lambda(ReLUTransform())\n",
    "\n",
    "        self.model = self.build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.hidden_layer1(inputs)\n",
    "        x = self.dropout(x)\n",
    "        #x = self.hidden_layer2(x)\n",
    "        x = self.untransformed_output(x)\n",
    "        y = self.final_output(x)\n",
    "        return y\n",
    "\n",
    "# Definiere den EarlyStopping-Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the functions to perform the train-/validate-/test-split with rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "## function used to calculate w_max, number of rolling windows etc.\n",
    "# length of a whole window (containing w input months and 12 acutal months)\n",
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "def number_valid_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    number_train_valid_months = numberMonths_available - w\n",
    "    number_valid_months = math.floor(number_train_valid_months * relative_validation_size)\n",
    "\n",
    "    return number_valid_months\n",
    "\n",
    "# number of months available for training (after removing the validation and test months)\n",
    "def number_train_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    valid_months = number_valid_months(numberMonths_available, w, relative_validation_size)\n",
    "\n",
    "    #  all months feature data   -  validate set    -   test set input\n",
    "    return numberMonths_available - valid_months - w\n",
    "\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "\n",
    "def find_max_W(numberMonths_available, w_min, w_max, relative_validation_size):\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one validation window\n",
    "    number_valid_months_wmin = number_valid_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_valid_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one validation window with w_min = ' + str(w_min))\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one train window\n",
    "    number_train_months_wmin = number_train_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_train_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "    # find the maximal w\n",
    "    max_W = w_max\n",
    "    number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "    number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while (number_valid_rollwindows_wmax == 0 or number_train_rollwindows_wmax == 0) and max_W > w_min:\n",
    "        max_W -= 1\n",
    "        number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "        number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "## conflict trap\n",
    "# drops all months before a starting conflict\n",
    "# defintion of a beginning conflict: fatalities(monthX) > 0 with mean(fatalities(window_size number months starting with monthX)) > threshold\n",
    "# (average fatalities per month in the starting half year are greater than the threshold)\n",
    "# iterates trough the dataset beginning with the first entry\n",
    "def drop_before_conflict_trap(data, threshold, window_size, minimal_data_size):\n",
    "    index_ged_sb = data.columns.get_loc('ged_sb')\n",
    "\n",
    "    start_index = 0\n",
    "    while start_index < len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "        window = data.iloc[start_index:start_index + window_size, index_ged_sb].to_list()\n",
    "        \n",
    "        if window[0] > 0 and sum(window) / window_size >= threshold:\n",
    "            break\n",
    "        else:\n",
    "            start_index += 1\n",
    "\n",
    "\n",
    "    if len(data) >= minimal_data_size:\n",
    "        # if there is no conflict trap do nothing\n",
    "        if start_index == len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "            return data\n",
    "        # if the truncation would result in a too small dataset prevent this\n",
    "        elif len(data.iloc[start_index:, :]) < minimal_data_size:\n",
    "            return data.iloc[-minimal_data_size:, :]\n",
    "        # drop every entry before the conflict trap \n",
    "        else:\n",
    "            return data.iloc[start_index:, :]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "\n",
    "def TrainValid_ArrayXY_split(train_months, w, month_list, data_feature, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    number_rolling_windows_train = number_rolling_windows(train_months, w)\n",
    "\n",
    "    for i in range(0, number_rolling_windows_train):\n",
    "        starting_month_features = month_list[i]\n",
    "\n",
    "        index_ending_month_features = i + w - 1\n",
    "        ending_month_features = month_list[index_ending_month_features]\n",
    "\n",
    "        starting_month_unrActuals = month_list[index_ending_month_features + 3]\n",
    "        ending_month_unrActuals = month_list[index_ending_month_features + 14]\n",
    "\n",
    "        window_features = data_feature.loc[slice(starting_month_features, ending_month_features), :] # 'ged_sb':'ged_sb_tlag_4'  ||excluding \"unreal\" actuals\n",
    "        window_actuals = data_feature.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "        normalized_window_features = preprocessing.normalize(window_features)\n",
    "        window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "        window_actual_array = np.array([window_actuals])\n",
    "\n",
    "        X.append(window_features_array)\n",
    "        Y.append(window_actual_array)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def Test_ArrayXY_split(month_list, data_feature, data_actual, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    starting_month_test = month_list[0]\n",
    "    ending_month_test = month_list[-1]\n",
    "\n",
    "    window_features_test = data_feature.loc[slice(starting_month_test, ending_month_test), :] # all w features to predict the fatalities\n",
    "    window_actuals_test = data_actual.iloc[s - 3].values # real actuals\n",
    "\n",
    "    normalized_window_features_test = preprocessing.normalize(window_features_test)\n",
    "    window_features_array_test = np.array([normalized_window_features_test.flatten()])[0]\n",
    "\n",
    "    window_actual_array_test = window_actuals_test\n",
    "\n",
    "    X.append(window_features_array_test)\n",
    "    Y.append(window_actual_array_test)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the future fatalites\n",
    "### Feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different feature selection from views\n",
    "# 59 features that map the conflict history of a country\n",
    "conflict_history = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'ln_ged_sb_tlag_1',\n",
    "    'ln_ged_sb_tlag_2', 'ln_ged_sb_tlag_3', 'ln_ged_sb_tlag_4',\n",
    "    'ln_ged_sb_tlag_5', 'ln_ged_sb_tlag_6', 'ln_ged_sb_tsum_24',\n",
    "    'decay_ged_sb_100', 'decay_ged_sb_500', 'decay_ged_os_100',\n",
    "    'decay_ged_ns_5', 'decay_ged_ns_100', 'ln_ged_ns', 'ln_ged_os',\n",
    "    'ln_acled_sb', 'ln_acled_sb_count', 'ln_acled_os',\n",
    "    'ln_ged_os_tlag_1', 'decay_acled_sb_5', 'decay_acled_os_5',\n",
    "    'decay_acled_ns_5', 'splag_1_decay_ged_os_5',\n",
    "    'splag_1_decay_ged_ns_5'\n",
    "]\n",
    "# 59 features that are drawn from the Varieties of Democracy project\n",
    "vdem = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'vdem_v2x_delibdem',\n",
    "    'vdem_v2x_egaldem', 'vdem_v2x_libdem', 'vdem_v2x_libdem_48',\n",
    "    'vdem_v2x_partip', 'vdem_v2x_accountability',\n",
    "    'vdem_v2x_civlib', 'vdem_v2x_clphy', 'vdem_v2x_cspart',\n",
    "    'vdem_v2x_divparctrl', 'vdem_v2x_edcomp_thick', 'vdem_v2x_egal',\n",
    "    'vdem_v2x_execorr', 'vdem_v2x_frassoc_thick', 'vdem_v2x_gencs',\n",
    "    'vdem_v2x_gender', 'vdem_v2x_genpp', 'vdem_v2x_horacc',\n",
    "    'vdem_v2x_neopat', 'vdem_v2x_pubcorr', 'vdem_v2x_rule',\n",
    "    'vdem_v2x_veracc', 'vdem_v2x_freexp', 'vdem_v2xcl_acjst', \n",
    "    'vdem_v2xcl_dmove', 'vdem_v2xcl_prpty', 'vdem_v2xcl_rol', \n",
    "    'vdem_v2xcl_slave', 'vdem_v2xdl_delib', 'vdem_v2xeg_eqdr',\n",
    "    'vdem_v2xeg_eqprotec', 'vdem_v2xel_frefair', 'vdem_v2xel_regelec',\n",
    "    'vdem_v2xme_altinf', 'vdem_v2xnp_client', 'vdem_v2xnp_regcorr',\n",
    "    'vdem_v2xpe_exlecon', 'vdem_v2xpe_exlpol', 'vdem_v2xpe_exlgeo',\n",
    "    'vdem_v2xpe_exlgender', 'vdem_v2xpe_exlsocgr', 'vdem_v2xps_party',\n",
    "    'vdem_v2xcs_ccsi', 'vdem_v2xnp_pres', 'vdem_v2xeg_eqaccess',\n",
    "    'vdem_v2x_diagacc', 'vdem_v2clrgunev', 'splag_vdem_v2x_libdem',\n",
    "    'splag_vdem_v2xcl_dmove', 'splag_vdem_v2x_accountability',\n",
    "    'splag_vdem_v2xpe_exlsocgr', 'splag_vdem_v2xcl_rol', 'wdi_sm_pop_netm',\n",
    "    'wdi_sp_dyn_imrt_in'\n",
    "]\n",
    "\n",
    "# 30 features that are drawn from the WDI as well as some conflict history indicators\n",
    "wdi = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'wdi_ag_lnd_frst_k2',\n",
    "    'wdi_dt_oda_odat_pc_zs', 'wdi_ms_mil_xpnd_gd_zs', 'wdi_ms_mil_xpnd_zs',\n",
    "    'wdi_nv_agr_totl_kd', 'wdi_nv_agr_totl_kn', 'wdi_ny_gdp_pcap_kd',\n",
    "    'wdi_sp_dyn_le00_in', 'wdi_se_prm_nenr', 'wdi_sh_sta_maln_zs', \n",
    "    'wdi_sh_sta_stnt_zs', 'wdi_sl_tlf_totl_fe_zs', 'wdi_sm_pop_refg_or', \n",
    "    'wdi_sm_pop_netm', 'wdi_sm_pop_totl_zs', 'wdi_sp_dyn_imrt_in', \n",
    "    'wdi_sh_dyn_mort_fe', 'wdi_sp_pop_1564_fe_zs', 'wdi_sp_pop_65up_fe_zs',\n",
    "    'wdi_sp_pop_grow', 'wdi_sp_urb_totl_in_zs',\n",
    "    'splag_wdi_sl_tlf_totl_fe_zs', 'splag_wdi_sm_pop_refg_or',\n",
    "    'splag_wdi_sm_pop_netm', 'splag_wdi_ag_lnd_frst_k2'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [01:10<07:02, 70.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [02:18<05:43, 68.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [03:30<04:41, 70.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [04:43<03:34, 71.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [05:45<02:16, 68.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [07:05<01:11, 71.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [08:13<00:00, 70.57s/it]\n"
     ]
    }
   ],
   "source": [
    "### prediction\n",
    "prediction_year = '2018' # 2019, 2020, 2021\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "\n",
    "#s_prediction_list = list(range(3, 15))\n",
    "s_prediction_list = list(range(3, 5))\n",
    "\n",
    "number_countries = len(country_list)\n",
    "\n",
    "\n",
    "pred_year_string = 'prediction_' + prediction_year\n",
    "# list to save the predictions for each country\n",
    "NNet_prediction_list = [{'country_id': country, pred_year_string: []} for country in country_list]\n",
    "\n",
    "\n",
    "### set hyperparameters for the prediction task\n",
    "## dataset split hyperparams\n",
    "rel_validation_size = 0.3 # percentual size of the validation set\n",
    "\n",
    "w_max = 24 # the maximal w (months to estimate the fatalities from) is set to e.g. 3 years (36 months)\n",
    "# BUT: If w_max=36 leads to a number of rolling windows < 1 in the validation dataset,\n",
    "# w_max is set to the maximal w, so that the number of rolling windows is >= 1\n",
    "# => this step is done below in the section for each country\n",
    "\n",
    "w_min = 2 # to calculate the w_max the w_min has to be set as well\n",
    "\n",
    "## neural net hyperparams\n",
    "batchSize = 1\n",
    "epochSize = 100\n",
    "learningRate = 0.1\n",
    "\n",
    "\n",
    "# loop through all countries\n",
    "for country_index in tqdm(range(number_countries)):\n",
    "    country = country_list[country_index]\n",
    "\n",
    "    ## HIER TRY Catch und dann nuller in liste des country\n",
    "    \"\"\" # check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "    if not check_last_featureMonth(prediction_country_id, dataset_index):\n",
    "        raise ValueError('last month is not contained in the data') \"\"\"\n",
    "\n",
    "\n",
    "    ## load datasets\n",
    "    feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "\n",
    "    # only FEATURE SUBSET\n",
    "    feature_data = feature_data.loc[:,wdi]\n",
    "\n",
    "    actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    ## remove months before the conflict trap (regime change)\n",
    "    # if the average number of fatalities per month in 6 months is above 10 and the fatalities of the starting month are > 0 \n",
    "    # the conflict trap starts and all obsservations before that month are dropped\n",
    "    # 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "    feature_data = drop_before_conflict_trap(feature_data, 10, 6, 76)\n",
    "\n",
    "    # numbers of months from the feature dataset\n",
    "    month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "    first_month = min(month_list_feature_data)\n",
    "    last_month = max(month_list_feature_data)\n",
    "    number_months_feature_data = len(month_list_feature_data) # number of months in the feature dataset\n",
    "\n",
    "    # loop over prediction horizons s\n",
    "    for s in s_prediction_list:\n",
    "\n",
    "        w_max = find_max_W(number_months_feature_data, w_min, w_max, rel_validation_size) # if not enough data w < w_max and w >= w_min\n",
    "        w = w_max\n",
    "\n",
    "        ### split data in train-, validation- and test-dataset\n",
    "        \"\"\" The data sizes are calculated in a way, that missing months are no problem. Additionaly due to the fact, that the data is sorted\n",
    "            regarding months, this step can be skipped. \"\"\"\n",
    "\n",
    "        # length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "        roll_window_len = rollingWindowLength(w)\n",
    "        n_train_months = number_train_months(number_months_feature_data, w, rel_validation_size)\n",
    "        n_valid_months = number_valid_months(number_months_feature_data, w, rel_validation_size)\n",
    "        n_test_months = w\n",
    "\n",
    "        month_list_train = month_list_feature_data[0:n_train_months]\n",
    "        month_list_valid = month_list_feature_data[n_train_months:(n_train_months+n_valid_months)]\n",
    "        month_list_test = month_list_feature_data[number_months_feature_data-n_test_months:]\n",
    "\n",
    "        ## training dataset------\n",
    "        X_train, Y_train = TrainValid_ArrayXY_split(n_train_months, w, month_list_train, feature_data, s)\n",
    "\n",
    "        ## validation dataset--------\n",
    "        X_validate, Y_validate = TrainValid_ArrayXY_split(n_valid_months, w, month_list_valid, feature_data, s)\n",
    "\n",
    "        ## test dataset-------\n",
    "        X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "        \n",
    "        ### prediction with the neural net\n",
    "        ## Define inputs with predefined shape\n",
    "        input_shape = (len(X_train[0]),)\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        ## define neural net\n",
    "        # Create an instance of the FeedForwardNN model\n",
    "        nameString = 'FFwdNN_country' + str(country) + '_s' + str(s)\n",
    "        model = FeedForwardNN(input_shape=inputs.shape, name=nameString)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=learningRate), loss=CRPSLoss())\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(X_train, Y_train, \n",
    "                            batch_size=batchSize, epochs=epochSize,\n",
    "                            validation_data=(X_validate, Y_validate),\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0, shuffle=False)\n",
    "\n",
    "        ## prediction\n",
    "        prediction = model.predict(X_test)\n",
    "        # round the prediction values to integers\n",
    "        empirical_distribution = np.round(np.sort(prediction[0])).astype(int)\n",
    "\n",
    "        # save the training and validation loss to generate plots\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(1, len(loss) + 1)\n",
    "\n",
    "        y_true = Y_test[0][0]\n",
    "        crps_prediction = pscore(empirical_distribution,y_true).compute()[0]\n",
    "\n",
    "\n",
    "        NNet_prediction_list[country_index][pred_year_string].append({'s':s, 'w':w, 'distribution':empirical_distribution, \n",
    "                                                                 'actual':y_true, 'CRPS':crps_prediction,\n",
    "                                                                 'loss':loss, 'val_loss':val_loss, 'epochs':epochs})\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s': 3,\n",
       "  'w': 6,\n",
       "  'distribution': array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,  56,  57,  57,  58,  59,  59,  60,  60,\n",
       "          60,  61,  61,  61,  61,  62,  62,  62,  62,  62,  63,  63,  63,\n",
       "          63,  64,  64,  64,  64,  64,  64,  65,  65,  65,  65,  65,  65,\n",
       "          66,  66,  66,  66,  66,  67,  67,  69,  70,  70,  71,  71,  71,\n",
       "          72,  73,  74,  74,  75,  75,  76,  76,  78,  79,  79,  80,  81,\n",
       "          81,  83,  83,  84,  85,  85,  85,  87,  89,  89,  89,  91,  91,\n",
       "          92,  93,  94,  94,  95, 100, 101, 106, 107, 110, 115, 119, 123,\n",
       "         128, 131, 136, 140, 144]),\n",
       "  'actual': 61.99999999999999,\n",
       "  'CRPS': 19.978750000000005,\n",
       "  'loss': [115.29875183105469,\n",
       "   107.10529327392578,\n",
       "   106.1467056274414,\n",
       "   99.9068603515625],\n",
       "  'val_loss': [22.196613311767578,\n",
       "   22.611835479736328,\n",
       "   23.050535202026367,\n",
       "   24.431657791137695],\n",
       "  'epochs': range(1, 5)},\n",
       " {'s': 4,\n",
       "  'w': 6,\n",
       "  'distribution': array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,  33,  52,  52,  53,  53,  54,\n",
       "          54,  55,  55,  55,  55,  56,  56,  56,  56,  56,  56,  57,  57,\n",
       "          57,  57,  58,  58,  58,  58,  59,  59,  59,  59,  59,  59,  59,\n",
       "          59,  59,  59,  60,  60,  60,  60,  61,  62,  63,  63,  63,  63,\n",
       "          63,  63,  64,  64,  64,  65,  65,  65,  65,  66,  66,  66,  66,\n",
       "          66,  66,  67,  67,  67,  67,  67,  67,  68,  68,  69,  69,  70,\n",
       "          72,  72,  72,  72,  73,  74,  74,  74,  75,  75,  75,  76,  77,\n",
       "          77,  78,  78,  78,  79,  79,  80,  80,  81,  81,  82,  82,  84,\n",
       "          84,  84,  84,  84,  85,  85,  86,  86,  87,  89,  89,  90,  91,\n",
       "          91,  92,  94,  95,  95,  96,  96,  97,  98,  99, 101, 101, 101,\n",
       "         102, 102, 103, 105, 107, 107, 108, 109, 111, 116, 118, 125, 129,\n",
       "         133, 136, 139, 148, 163, 167, 171, 179, 184, 201, 217, 227, 236,\n",
       "         247, 284, 297, 326, 350]),\n",
       "  'actual': 72.99999999999997,\n",
       "  'CRPS': 9.886824999999995,\n",
       "  'loss': [97.58729553222656,\n",
       "   89.41150665283203,\n",
       "   88.9227523803711,\n",
       "   81.96492767333984,\n",
       "   84.0574951171875,\n",
       "   80.46855926513672,\n",
       "   79.71337127685547],\n",
       "  'val_loss': [22.9631290435791,\n",
       "   25.217456817626953,\n",
       "   24.34530258178711,\n",
       "   20.02479362487793,\n",
       "   24.359607696533203,\n",
       "   23.131269454956055,\n",
       "   23.577396392822266],\n",
       "  'epochs': range(1, 8)}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNet_prediction_list[5][pred_year_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m val_loss \u001b[39m=\u001b[39m NNet_prediction_list[\u001b[39m3\u001b[39m][pred_year_string][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m epochs \u001b[39m=\u001b[39m NNet_prediction_list[\u001b[39m3\u001b[39m][pred_year_string][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m empirical_distribution \u001b[39m=\u001b[39m NNet_prediction_list[\u001b[39m3\u001b[39;49m][pred_year_string][\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'prediction'"
     ]
    }
   ],
   "source": [
    "loss = NNet_prediction_list[3][pred_year_string][0]['loss']\n",
    "val_loss = NNet_prediction_list[3][pred_year_string][0]['val_loss']\n",
    "epochs = NNet_prediction_list[3][pred_year_string][0]['epochs']\n",
    "\n",
    "empirical_distribution = NNet_prediction_list[3][pred_year_string][0]['prediction']\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def empirical_cdf(data):\n",
    "    n = len(data)\n",
    "    ecdf_values = np.arange(1, n + 1) / n\n",
    "    return ecdf_values\n",
    "\n",
    "# Berechne die empirische CDF\n",
    "ecdf = empirical_cdf(empirical_distribution)\n",
    "\n",
    "# Erstelle eine Figur mit 1 Zeile und 2 Spalten für die beiden Plots nebeneinander und kleiner\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "\n",
    "axes[0].plot(epochs, loss, 'y', label='Traing')\n",
    "axes[0].plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "axes[0].set_title('Training and validation loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plotte die empirische CDF im zweiten Subplot\n",
    "axes[1].step(empirical_distribution, ecdf, label='Empirical Density Function', color='green', linewidth=2)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('F(x)')\n",
    "axes[1].set_title('Empirical cdf')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()  # Optimiere den Abstand zwischen den Subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss for each entry:\n",
      "[1723.859774059431, 1673.428175189025, 1496.602015439313, 1313.3642731515408]\n",
      "Mean Validation Loss for each entry:\n",
      "[589.7917375830855, 504.70810147802484, 311.5503060402802, 288.2608816208771]\n"
     ]
    }
   ],
   "source": [
    "# Annahme: NNet_prediction_list enthält die Daten für jedes Land und jedes s\n",
    "\n",
    "# Initialisiere Listen mit Nullen für jeden Eintrag in loss, val_loss und crps\n",
    "mean_loss = [0.0] * len(NNet_prediction_list[0][pred_year_string][0]['loss'])\n",
    "mean_val_loss = [0.0] * len(NNet_prediction_list[0][pred_year_string][0]['val_loss'])\n",
    "\n",
    "# Summiere die loss-Werte, val_loss-Werte und crps-Werte für jedes Land auf\n",
    "for country_data in NNet_prediction_list:\n",
    "    for entry in country_data[pred_year_string]:\n",
    "        mean_loss = [mean + loss_value for mean, loss_value in zip(mean_loss, entry['loss'])]\n",
    "        mean_val_loss = [mean + val_loss_value for mean, val_loss_value in zip(mean_val_loss, entry['val_loss'])]\n",
    "\n",
    "# Berechne die Durchschnitte, indem du durch die Anzahl der Länder teilst\n",
    "mean_loss = [mean / len(NNet_prediction_list) for mean in mean_loss]\n",
    "mean_val_loss = [mean / len(NNet_prediction_list) for mean in mean_val_loss]\n",
    "\n",
    "print(\"Mean Loss for each entry:\")\n",
    "print(mean_loss)\n",
    "print(\"Mean Validation Loss for each entry:\")\n",
    "print(mean_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CRPS for s=5: None\n"
     ]
    }
   ],
   "source": [
    "s_to_calculate = 5  # Ersetze dies durch das gewünschte s\n",
    "\n",
    "crps_values = []\n",
    "\n",
    "for country_data in NNet_prediction_list:\n",
    "    country_crps = None\n",
    "    \n",
    "    for entry in country_data[pred_year_string]:\n",
    "        if entry['s'] == s_to_calculate:\n",
    "            country_crps = entry['CRPS']\n",
    "            break\n",
    "    \n",
    "    if country_crps is not None:\n",
    "        crps_values.append(country_crps)\n",
    "\n",
    "mean_crps = sum(crps_values) / len(crps_values) if crps_values else None\n",
    "\n",
    "print(f\"Mean CRPS for s={s_to_calculate}: {mean_crps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
