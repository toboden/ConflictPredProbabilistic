{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set seed so that the model outputs are reproducible\n",
    "After the kernel is restarted the same results are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(0)\n",
    "\n",
    "# This will make TensorFlow ops as deterministic as possible, but it will\n",
    "# affect the overall performance, so it's not enabled by default.\n",
    "# `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_Actuals(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# check if the last month of a country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "def check_last_featureMonth(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        raise ValueError('country does not have actuals')\n",
    "\n",
    "\n",
    "    # last month of the feature dataset\n",
    "    last_feature_month = country_feature_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[-1]\n",
    "\n",
    "    # first month of the actual dataset\n",
    "    first_actual_month = country_actual_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[0]\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (first_actual_month - 3) != last_feature_month:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "data = features_df_list[-1]['data']\n",
    "if 'gleditsch_ward' in data.columns:\n",
    "    data = data.drop(columns='gleditsch_ward') # column not necessary\n",
    "\n",
    "## Features without missing values\n",
    "columns_without_missing_values = data.columns[data.notna().all()]\n",
    "\n",
    "for i in range(len(features_df_list)):\n",
    "    data_set = features_df_list[i]['data']\n",
    "    features_df_list[i]['data'] = data_set[columns_without_missing_values]\n",
    "\n",
    "all_features = features_df_list[-1]['data'].columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different feature selection from views\n",
    "# 59 features that map the conflict history of a country\n",
    "conflict_history = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'ln_ged_sb_tlag_1',\n",
    "    'ln_ged_sb_tlag_2', 'ln_ged_sb_tlag_3', 'ln_ged_sb_tlag_4',\n",
    "    'ln_ged_sb_tlag_5', 'ln_ged_sb_tlag_6', 'ln_ged_sb_tsum_24',\n",
    "    'decay_ged_sb_100', 'decay_ged_sb_500', 'decay_ged_os_100',\n",
    "    'decay_ged_ns_5', 'decay_ged_ns_100', 'ln_ged_ns', 'ln_ged_os',\n",
    "    'ln_acled_sb', 'ln_acled_sb_count', 'ln_acled_os',\n",
    "    'ln_ged_os_tlag_1', 'decay_acled_sb_5', 'decay_acled_os_5',\n",
    "    'decay_acled_ns_5', 'splag_1_decay_ged_os_5',\n",
    "    'splag_1_decay_ged_ns_5'\n",
    "]\n",
    "# 59 features that are drawn from the Varieties of Democracy project\n",
    "vdem = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'vdem_v2x_delibdem',\n",
    "    'vdem_v2x_egaldem', 'vdem_v2x_libdem', 'vdem_v2x_libdem_48',\n",
    "    'vdem_v2x_partip', 'vdem_v2x_accountability',\n",
    "    'vdem_v2x_civlib', 'vdem_v2x_clphy', 'vdem_v2x_cspart',\n",
    "    'vdem_v2x_divparctrl', 'vdem_v2x_edcomp_thick', 'vdem_v2x_egal',\n",
    "    'vdem_v2x_execorr', 'vdem_v2x_frassoc_thick', 'vdem_v2x_gencs',\n",
    "    'vdem_v2x_gender', 'vdem_v2x_genpp', 'vdem_v2x_horacc',\n",
    "    'vdem_v2x_neopat', 'vdem_v2x_pubcorr', 'vdem_v2x_rule',\n",
    "    'vdem_v2x_veracc', 'vdem_v2x_freexp', 'vdem_v2xcl_acjst', \n",
    "    'vdem_v2xcl_dmove', 'vdem_v2xcl_prpty', 'vdem_v2xcl_rol', \n",
    "    'vdem_v2xcl_slave', 'vdem_v2xdl_delib', 'vdem_v2xeg_eqdr',\n",
    "    'vdem_v2xeg_eqprotec', 'vdem_v2xel_frefair', 'vdem_v2xel_regelec',\n",
    "    'vdem_v2xme_altinf', 'vdem_v2xnp_client', 'vdem_v2xnp_regcorr',\n",
    "    'vdem_v2xpe_exlecon', 'vdem_v2xpe_exlpol', 'vdem_v2xpe_exlgeo',\n",
    "    'vdem_v2xpe_exlgender', 'vdem_v2xpe_exlsocgr', 'vdem_v2xps_party',\n",
    "    'vdem_v2xcs_ccsi', 'vdem_v2xnp_pres', 'vdem_v2xeg_eqaccess',\n",
    "    'vdem_v2x_diagacc', 'vdem_v2clrgunev', 'splag_vdem_v2x_libdem',\n",
    "    'splag_vdem_v2xcl_dmove', 'splag_vdem_v2x_accountability',\n",
    "    'splag_vdem_v2xpe_exlsocgr', 'splag_vdem_v2xcl_rol', 'wdi_sm_pop_netm',\n",
    "    'wdi_sp_dyn_imrt_in'\n",
    "]\n",
    "\n",
    "# 30 features that are drawn from the WDI as well as some conflict history indicators\n",
    "wdi = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'wdi_ag_lnd_frst_k2',\n",
    "    'wdi_dt_oda_odat_pc_zs', 'wdi_ms_mil_xpnd_gd_zs', 'wdi_ms_mil_xpnd_zs',\n",
    "    'wdi_nv_agr_totl_kd', 'wdi_nv_agr_totl_kn', 'wdi_ny_gdp_pcap_kd',\n",
    "    'wdi_sp_dyn_le00_in', 'wdi_se_prm_nenr', 'wdi_sh_sta_maln_zs', \n",
    "    'wdi_sh_sta_stnt_zs', 'wdi_sl_tlf_totl_fe_zs', 'wdi_sm_pop_refg_or', \n",
    "    'wdi_sm_pop_netm', 'wdi_sm_pop_totl_zs', 'wdi_sp_dyn_imrt_in', \n",
    "    'wdi_sh_dyn_mort_fe', 'wdi_sp_pop_1564_fe_zs', 'wdi_sp_pop_65up_fe_zs',\n",
    "    'wdi_sp_pop_grow', 'wdi_sp_urb_totl_in_zs',\n",
    "    'splag_wdi_sl_tlf_totl_fe_zs', 'splag_wdi_sm_pop_refg_or',\n",
    "    'splag_wdi_sm_pop_netm', 'splag_wdi_ag_lnd_frst_k2'\n",
    "]\n",
    "\n",
    "ged = ['ged_sb']\n",
    "\n",
    "feature_subset_dict = {'conflict_history':conflict_history,\n",
    "                       'vdem':vdem,\n",
    "                       'wdi':wdi,\n",
    "                       'all':all_features,\n",
    "                       'ged':ged}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group data by country_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of the countries for which a prediction is requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_countrylist = os.path.join('..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "\n",
    "        # check if actuals exist for the country\n",
    "        if check_Actuals(country_id, 0):\n",
    "            # check if the last feature month is 3 months before the first actuals month\n",
    "            if not check_last_featureMonth(country_id, 0): \n",
    "                month_list.append([str(country_id) +' last month missing'])\n",
    "            else:\n",
    "                month_list.append([number_months_feature_data, country_id])\n",
    "        else:\n",
    "            month_list.append(str(country_id) + ' no actuals')\n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "Goal is to estimate the empirical distribution of the fatalities per month.\n",
    "### Definition of the CRPS loss function and the Feed forward Neural Network subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to set the number and partition of the neurons\n",
    "def number_neurons_allLayers(inputNeurons, outputNeurons):\n",
    "    return np.round(np.mean([inputNeurons, outputNeurons]))\n",
    "\n",
    "def split_neurons_3hiddenlayer(numberNeurons):\n",
    "\n",
    "    neuronsHiddenLayer1 = np.round(numberNeurons * 0.5) \n",
    "    neuronsHiddenLayer2 = np.round(numberNeurons * 0.3)\n",
    "    neuronsHiddenLayer3 = numberNeurons - neuronsHiddenLayer1 - neuronsHiddenLayer2\n",
    "\n",
    "    return int(neuronsHiddenLayer1), int(neuronsHiddenLayer2), int(neuronsHiddenLayer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)\n",
    "\n",
    "# Define custom ReLU activation function\n",
    "class ReLUTransform(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "# Define the Feed Forward Neural Network subclass\n",
    "class FeedForwardNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, name=\"FeedFwdNN\"):\n",
    "        super(FeedForwardNN, self).__init__(name=name)\n",
    "        \n",
    "        neurons_input = input_shape[-1]\n",
    "        neurons_output = 200\n",
    "        neurons_hidden_layer = number_neurons_allLayers(neurons_input,neurons_output)\n",
    "        #neuronsL1, neuronsL2, neuronsL3 = split_neurons_3hiddenlayer(neurons_hidden_layer)\n",
    "\n",
    "\n",
    "        self.hidden_layer1 = Dense(np.round(neurons_hidden_layer/2), activation='relu')\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.hidden_layer2 = Dense(np.round(neurons_hidden_layer/2), activation='relu')\n",
    "        self.untransformed_output = Dense(neurons_output)\n",
    "        self.final_output = Lambda(ReLUTransform())\n",
    "\n",
    "        self.model = self.build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.hidden_layer1(inputs)\n",
    "        x = self.dropout(x)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.untransformed_output(x)\n",
    "        y = self.final_output(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "numberNeurons='neuronshiddenlayer' \n",
    "numberHiddenLayers='2' \n",
    "drouputRate = 0.3\n",
    "# Definiere den EarlyStopping-Callback\n",
    "patience = 3\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the functions to perform the train-/validate-/test-split with rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "## conflict trap\n",
    "# drops all months before a starting conflict\n",
    "# defintion of a beginning conflict: fatalities(monthX) > 0 with mean(fatalities(window_size number months starting with monthX)) > threshold\n",
    "# (average fatalities per month in the starting half year are greater than the threshold)\n",
    "# iterates trough the dataset beginning with the first entry\n",
    "def drop_before_conflict_trap(data, threshold, window_size, minimal_data_size):\n",
    "    index_ged_sb = data.columns.get_loc('ged_sb')\n",
    "\n",
    "    start_index = 0\n",
    "    while start_index < len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "        window = data.iloc[start_index:start_index + window_size, index_ged_sb].to_list()\n",
    "        \n",
    "        if window[0] > 0 and sum(window) / window_size >= threshold:\n",
    "            break\n",
    "        else:\n",
    "            start_index += 1\n",
    "\n",
    "\n",
    "    if len(data) >= minimal_data_size:\n",
    "        # if there is no conflict trap do nothing\n",
    "        if start_index == len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "            return data\n",
    "        # if the truncation would result in a too small dataset prevent this\n",
    "        elif len(data.iloc[start_index:, :]) < minimal_data_size:\n",
    "            return data.iloc[-minimal_data_size:, :]\n",
    "        # drop every entry before the conflict trap \n",
    "        else:\n",
    "            return data.iloc[start_index:, :]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "\n",
    "## function used to calculate w_max, number of rolling windows etc.\n",
    "# length of a whole window (containing w input months and 12 acutal months)\n",
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "# number of months available for training (after removing the test months)\n",
    "def number_train_months(numberMonths_available, w):\n",
    "    #  all months feature data   -  test set input\n",
    "    return numberMonths_available - w\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "def find_max_W(numberMonths_available, w_min, w_max):\n",
    "    if number_rolling_windows(numberMonths_available, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "    # find the maximal w\n",
    "    max_W = w_max\n",
    "    number_months_train = number_train_months(numberMonths_available, max_W)\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_months_train, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while number_train_rollwindows_wmax == 0 and max_W > w_min:\n",
    "        max_W -= 1\n",
    "        number_months_train = number_train_months(numberMonths_available, max_W)\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_months_train, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "\n",
    "def month_lists_TrainTest(w_min, w_max, month_list_feature_data):\n",
    "    # numbers of months from the shortened feature dataset\n",
    "    number_months = len(month_list_feature_data)\n",
    "    \n",
    "    # find w_max (as mentioned above, if there are not enoug months, the w_max has to be < w_max)\n",
    "    w_max_local = find_max_W(number_months, w_min, w_max)\n",
    "\n",
    "    w = w_max_local\n",
    "\n",
    "    # length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "    n_train_months = number_train_months(number_months, w)\n",
    "\n",
    "    month_list_train = month_list_feature_data[0:n_train_months]\n",
    "    month_list_test = month_list_feature_data[-w:]\n",
    "\n",
    "    return month_list_train, month_list_test, w\n",
    "\n",
    "\n",
    "def TrainValid_ArrayXY_split(w, month_list, data_feature, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    train_months = len(month_list)\n",
    "\n",
    "    number_rolling_windows_train = number_rolling_windows(train_months, w)\n",
    "\n",
    "    for i in range(0, number_rolling_windows_train):\n",
    "        starting_month_features = month_list[i]\n",
    "\n",
    "        index_ending_month_features = i + w - 1\n",
    "        ending_month_features = month_list[index_ending_month_features]\n",
    "\n",
    "        starting_month_unrActuals = month_list[index_ending_month_features + 3]\n",
    "        ending_month_unrActuals = month_list[index_ending_month_features + 14]\n",
    "\n",
    "        window_features = data_feature.loc[slice(starting_month_features, ending_month_features), :] # excluding \"unreal\" actuals\n",
    "        window_actuals = data_feature.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "        normalized_window_features = preprocessing.normalize(window_features)\n",
    "        window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "        window_actual_array = np.array([window_actuals])\n",
    "\n",
    "        X.append(window_features_array)\n",
    "        Y.append(window_actual_array)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def Test_ArrayXY_split(month_list, data_feature, data_actual, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    starting_month_test = month_list[0]\n",
    "    ending_month_test = month_list[-1]\n",
    "\n",
    "    window_features_test = data_feature.loc[slice(starting_month_test, ending_month_test), :] # all w features to predict the fatalities\n",
    "    window_actuals_test = data_actual.iloc[s - 3].values # real actuals\n",
    "\n",
    "    normalized_window_features_test = preprocessing.normalize(window_features_test)\n",
    "    window_features_array_test = np.array([normalized_window_features_test.flatten()])[0]\n",
    "\n",
    "    window_actual_array_test = window_actuals_test\n",
    "\n",
    "    X.append(window_features_array_test)\n",
    "    Y.append(window_actual_array_test)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting month: 121\n",
      "Ending month: 490\n",
      " \n",
      " \n",
      "347\n",
      "w_max=5  w=5\n",
      "[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485]\n",
      "[486, 487, 488, 489, 490]\n",
      " \n",
      "(first, last) train month: (121,485)\n",
      "(first, last) test month: (486,490)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ## neural net hyperparams\\nbatchSize = 1\\nepochSize = 10\\nlearningRate = 0.1\\n\\n# Define inputs with predefined shape\\ninput_shape = (len(X_train[0]),)\\ninputs = Input(shape=input_shape)\\n#print(inputs.shape)\\n\\n# Print model summary\\n#model.summary()\\n\\n\\n# Create an instance of the FeedForwardNN model\\nmodel = FeedForwardNN(input_shape=inputs.shape, name=\\'FFwdNN_s3\\')\\n\\n# Compile the model\\nmodel.compile(optimizer=Adam(learning_rate=learningRate), loss=CRPSLoss())\\n\\nhistory = model.fit(X_train, Y_train, \\n                    batch_size=batchSize, epochs=epochSize,\\n                    callbacks=[early_stopping], #early_stopping\\n                    verbose=1, shuffle=False)\\n\\n\\nprint(\\'\\')\\nprint(\\'\\')\\n\\n# Evaluate the model on the test data using `evaluate`\\nprint(\"Evaluate on test data\")\\nresults = model.evaluate(X_test, Y_test, batch_size=batchSize)\\n\\n# Generate predictions (probabilities -- the output of the last layer)\\n# on new data using `predict`\\nprint(\"Generate prediction\")\\nprediction = model.predict(X_test)\\nprint(\"predictions shape:\", prediction.shape)\\nempirical_distribution = np.round(np.sort(prediction[0])).astype(int) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[28, 60, 65, 67, 96, 121, 133, 136, 145, 149, 218, 223, 245, 246]\n",
    "prediction_year = '2021'\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "mean_fatlities_per_month_threshold = 5 # threshold for the average number of fatalities per month (conflict trap detection)\n",
    "var_threshold = 0.05 # variance threshold for dropping columns\n",
    "feature_subset = 'wdi' # 'conflict_history', 'vdem', 'wdi', 'all',\n",
    "\n",
    "country = 60 #220  245\n",
    "\n",
    "# check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "if not check_last_featureMonth(country, dataset_index):\n",
    "    raise ValueError('last month is not contained in the data')\n",
    "\n",
    "### load and prepare datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "# only FEATURE SUBSET\n",
    "feature_data = feature_data.loc[:,feature_subset_dict[feature_subset]]\n",
    "\n",
    "## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "## remove months before the CONDFLICT TRAP (regime change)\n",
    "# if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "# the conflict trap starts and all obsservations before that month are dropped\n",
    "# 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "\n",
    "\n",
    "\n",
    "month_liste = feature_data.index.get_level_values('month_id').tolist()\n",
    "w_min = 1\n",
    "w_max = 5\n",
    "\n",
    "s = 8\n",
    "\n",
    "print('Starting month: ' + str(month_liste[0]))\n",
    "print('Ending month: ' + str(month_liste[-1]))\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "month_list_train, month_list_test, w = month_lists_TrainTest(w_min, w_max, month_liste)\n",
    "\n",
    "print(number_rolling_windows(len(month_list_train), w))\n",
    "print('w_max=' + str(w_max) + '  w=' + str(w))\n",
    "print(month_list_train)\n",
    "print(month_list_test)\n",
    "\n",
    "## training dataset------\n",
    "X_train, Y_train = TrainValid_ArrayXY_split(w, month_list_train, feature_data, s)\n",
    "#print('len x train input ' + str(len(X_train[0])))\n",
    "\n",
    "## test dataset-------\n",
    "X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "#print('len x test input ' + str(len(X_test[0])))\n",
    "print(' ')\n",
    "\n",
    "print('(first, last) train month: (' + str(month_list_train[0])+','+str(month_list_train[-1])+')')\n",
    "print('(first, last) test month: (' + str(month_list_test[0])+','+str(month_list_test[-1])+')')\n",
    "print('')\n",
    "\n",
    "\"\"\" ## neural net hyperparams\n",
    "batchSize = 1\n",
    "epochSize = 10\n",
    "learningRate = 0.1\n",
    "\n",
    "# Define inputs with predefined shape\n",
    "input_shape = (len(X_train[0]),)\n",
    "inputs = Input(shape=input_shape)\n",
    "#print(inputs.shape)\n",
    "\n",
    "# Print model summary\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# Create an instance of the FeedForwardNN model\n",
    "model = FeedForwardNN(input_shape=inputs.shape, name='FFwdNN_s3')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learningRate), loss=CRPSLoss())\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=batchSize, epochs=epochSize,\n",
    "                    callbacks=[early_stopping], #early_stopping\n",
    "                    verbose=1, shuffle=False)\n",
    "\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, Y_test, batch_size=batchSize)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate prediction\")\n",
    "prediction = model.predict(X_test)\n",
    "print(\"predictions shape:\", prediction.shape)\n",
    "empirical_distribution = np.round(np.sort(prediction[0])).astype(int) \"\"\"\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the future fatalites\n",
    "### Manipulate country list for the hyperparameter tuning\n",
    "Only countries that have months with fatalities greater than zero are relevant for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_fatalities_country_list = []\n",
    "countries_with_high_percentage_list = []\n",
    "someNonzero_fatalities_country_list = []\n",
    "\n",
    "for country_id in country_list:\n",
    "    feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "    # Berechnen des Prozentsatzes der Werte größer als 0 in der Spalte 'ged_sb'\n",
    "    positive_percentage = (feature_data['ged_sb'] > 0).mean() * 100\n",
    "\n",
    "    if (feature_data['ged_sb'] == 0).all():\n",
    "        zero_fatalities_country_list.append(country_id)\n",
    "    elif positive_percentage >= 60:\n",
    "        countries_with_high_percentage_list.append(country_id)\n",
    "    else:\n",
    "        someNonzero_fatalities_country_list.append(country_id)\n",
    "\n",
    "\n",
    "country_list = [x for x in country_list if x not in zero_fatalities_country_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = [60, 220]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A5F09C8E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A5C23B1870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [06:45<06:45, 405.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [11:26<00:00, 343.05s/it]\n"
     ]
    }
   ],
   "source": [
    "### prediction\n",
    "prediction_year = '2021' # 2019, 2020, 2021\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "\n",
    "s_prediction_list = list(range(3, 15))\n",
    "number_s = len(s_prediction_list)\n",
    "\n",
    "number_countries = len(country_list)\n",
    "\n",
    "pred_year_string = 'prediction_' + prediction_year\n",
    "\n",
    "# list to save the predictions for each country\n",
    "NNet_prediction_list = [{'country_id': country, pred_year_string: []} for country in country_list]\n",
    "\n",
    "\n",
    "### set hyperparameters for the prediction task\n",
    "var_threshold = 0.05 # variance threshold for dropping columns\n",
    "mean_fatlities_per_month_threshold = 5 # threshold for the average number of fatalities per month (conflict trap detection)\n",
    "\n",
    "w_max = 24 # the maximal w (months to estimate the fatalities from) is set to e.g. 3 years (36 months)\n",
    "w_min = 1 # to calculate the w_max the w_min has to be set as well\n",
    "\n",
    "## neural net hyperparams\n",
    "batchSize = 1\n",
    "epochSize = 100\n",
    "learningRate = 0.1\n",
    "\n",
    "feature_subset = 'wdi' # 'conflict_history', 'vdem', 'wdi', 'all'\n",
    "\n",
    "used_hyperparameter_dict = {'wMax': w_max, \n",
    "                            'conflictTrapThresh': mean_fatlities_per_month_threshold, 'features': feature_subset, \n",
    "                            'earlyStoppingPatience': patience, \n",
    "                            'learningRate': learningRate, \n",
    "                            'numberHiddenLayers': numberHiddenLayers, 'dropoutRate': drouputRate, \n",
    "                            'numberNeurons': numberNeurons}\n",
    "\n",
    "# loop through all countries\n",
    "for country_index in tqdm(range(number_countries)):\n",
    "    country = country_list[country_index]\n",
    "\n",
    "    ## HIER TRY Catch und dann nuller in liste des country\n",
    "    \"\"\" # check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "    if not check_last_featureMonth(prediction_country_id, dataset_index):\n",
    "        raise ValueError('last month is not contained in the data') \"\"\"\n",
    "\n",
    "\n",
    "    ### load and prepare datasets\n",
    "    feature_data = country_feature_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    # only FEATURE SUBSET\n",
    "    feature_data = feature_data.loc[:,feature_subset_dict[feature_subset]]\n",
    "\n",
    "    ## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "    columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "    feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "    actual_data = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    ## remove months before the CONDFLICT TRAP (regime change)\n",
    "    # if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "    # the conflict trap starts and all obsservations before that month are dropped\n",
    "    # 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "    feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "\n",
    "\n",
    "    ## final feature dataset months\n",
    "    # numbers of months from the feature dataset\n",
    "    month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "\n",
    "    NNet_prediction_list[country_index][pred_year_string].append({'s':[None for _ in range(number_s)], \n",
    "                                                                    'w':[None for _ in range(number_s)], \n",
    "                                                                    'distribution':[None for _ in range(number_s)],\n",
    "                                                                    'actual':[None for _ in range(number_s)],\n",
    "                                                                    'CRPS':[None for _ in range(number_s)], \n",
    "                                                                    'loss':[None for _ in range(number_s)], \n",
    "                                                                    'epochs':[None for _ in range(number_s)]})\n",
    "\n",
    "    for s in s_prediction_list:\n",
    "\n",
    "        month_list_train, month_list_test, w = month_lists_TrainTest(w_min, w_max, month_list_feature_data)\n",
    "\n",
    "        ## training dataset------\n",
    "        X_train, Y_train = TrainValid_ArrayXY_split(w, month_list_train, feature_data, s)\n",
    "\n",
    "        ## test dataset-------\n",
    "        X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data, s)\n",
    "\n",
    "        ### prediction with the neural net\n",
    "        ## Define inputs with predefined shape\n",
    "        input_shape = (len(X_train[0]),)\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        ## define neural net\n",
    "        # Create an instance of the FeedForwardNN model\n",
    "        nameString = 'FFwdNN_country' + str(country) + '_s' + str(s)\n",
    "        model = FeedForwardNN(input_shape=inputs.shape, name=nameString)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=learningRate), loss=CRPSLoss())\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(X_train, Y_train, \n",
    "                            batch_size=batchSize, epochs=epochSize,\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0, shuffle=False)\n",
    "\n",
    "        ## prediction\n",
    "        prediction = model.predict(X_test)\n",
    "        # round the prediction values to integers\n",
    "        empirical_distribution = np.round(np.sort(prediction[0])).astype(int)\n",
    "\n",
    "        # save the training and validation loss to generate plots\n",
    "        loss = history.history['loss']\n",
    "        epochs = range(1, len(loss) + 1)\n",
    "\n",
    "        y_true = Y_test[0][0]\n",
    "        crps_prediction = pscore(empirical_distribution,y_true).compute()[0]\n",
    "\n",
    "        NNet_prediction_list[country_index][pred_year_string][0]['s'][s-3] = s\n",
    "        NNet_prediction_list[country_index][pred_year_string][0]['w'][s-3] = w\n",
    "        NNet_prediction_list[country_index][pred_year_string][0]['distribution'][s-3] = empirical_distribution\n",
    "        NNet_prediction_list[country_index][pred_year_string][0]['actual'][s-3] = y_true\n",
    "        NNet_prediction_list[country_index][pred_year_string][0]['CRPS'][s-3] = crps_prediction\n",
    "        NNet_prediction_list[country_index][pred_year_string][0]['loss'][s-3] = loss\n",
    "        NNet_prediction_list[country_index][pred_year_string][0]['epochs'][s-3] = epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CRPS = 394.56340520833334 \\\\\n",
      "Parameters data prep:  \\\\wMax=24 conflictTrapThresh=5 features=wdi \\\\\n",
      "Parameters NN:  \\\\stoppingPatience=3 learningRate=0.1 numberHiddenLayers=2 numberNeurons=neuronshiddenlayer dropoutRate=0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crps_values_test = []\n",
    "for country_data in NNet_prediction_list:\n",
    "    country_crps_list = []\n",
    "    for s in range(0,number_s):\n",
    "        country_crps_list.append(country_data[pred_year_string][0]['CRPS'][s])\n",
    "    crps_values_test.append(np.mean(country_crps_list))\n",
    "\n",
    "mean_crps_test = np.mean(crps_values_test)\n",
    "\n",
    "\n",
    "print('Mean CRPS = ' + str(mean_crps_test) + ' \\\\'+'\\\\')\n",
    "print('Parameters data prep: ' + ' \\\\'+'\\\\' + 'wMax='+ str(used_hyperparameter_dict['wMax']) + ' conflictTrapThresh=' + str(used_hyperparameter_dict['conflictTrapThresh']) + ' features='+ used_hyperparameter_dict['features'] + ' \\\\'+'\\\\')\n",
    "print('Parameters NN: ' + ' \\\\'+'\\\\' + 'stoppingPatience=' + str(used_hyperparameter_dict['earlyStoppingPatience']) + ' learningRate='+str(used_hyperparameter_dict['learningRate'])+' numberHiddenLayers=' + used_hyperparameter_dict['numberHiddenLayers'] + ' numberNeurons=' + used_hyperparameter_dict['numberNeurons'] + ' dropoutRate=' + str(used_hyperparameter_dict['dropoutRate']))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "joblib_string = str(used_hyperparameter_dict['numberHiddenLayers']) + 'hiddenL' +  'wMax' + str(used_hyperparameter_dict['wMax']) + 'dropoutRate' + str(used_hyperparameter_dict['dropoutRate']) + '.joblib'\n",
    "\n",
    "#dump([NNet_prediction_list, used_hyperparameter_dict, country_list, pred_year_string],  joblib_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNet_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
