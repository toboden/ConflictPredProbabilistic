{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan's\n",
    "for featurelist in features_df_list:\n",
    "    is_na_series = featurelist['data'].isna().sum()\n",
    "\n",
    "    for i in range(len(is_na_series)):\n",
    "        if is_na_series[i] > 0 :\n",
    "            print(str(is_na_series.index[i]) + ': ' + str(is_na_series[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wichtig:\n",
    "analog zur baseline code implementieren, der abf채ngt, dass auch f체r jedes Land der letzte Monat vor Jan 2018 verf체gbar ist und insgesamt genug Monate vorhanden sind, um mit w_max den train, valid, test Split zu machen\n",
    "\n",
    "z.b. 246 viel zu wenig Monate f체r w = 48?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction task for country 223 and actual year 2018\n",
    "prediction_year = '2018'\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "prediction_country_id = 223\n",
    "\n",
    "## load datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(prediction_country_id)\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(prediction_country_id)\n",
    "\n",
    "# numbers of months from the feature dataset\n",
    "month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "first_month = month_list_feature_data[0]\n",
    "last_month = month_list_feature_data[-1]\n",
    "\n",
    "\n",
    "\n",
    "## hyperparameters\n",
    "w_max = 48 # maximum number of months used for one training run\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## split data in train-, validation- and test-dataset\n",
    "# length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "roll_estim_window_len = w_max + 2 + 12 \n",
    "\n",
    "# training dataset\n",
    "last_month_train = last_month - w_max - 12-2-w_max \n",
    "data_train = feature_data.loc[(slice(first_month, last_month_train), slice(None)), :] # including \"unreal\" actuals\n",
    "\n",
    "# validation dataset\n",
    "last_month_valid = last_month - w_max\n",
    "data_validate = feature_data.loc[(slice(last_month_train+1, last_month_valid), slice(None)), :] # including \"unreal\" actuals\n",
    "\n",
    "# test dataset\n",
    "data_test = feature_data.loc[(slice(last_month_valid+1, last_month), slice(None)), :] # no \"unreal\" actuals and real actuals not included as well\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 200)               64200     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2)                 402       \n",
      "                                                                 \n",
      " lambda_13 (Lambda)          (None, 2)                 0         \n",
      "                                                                 \n",
      " lambda_14 (Lambda)          (None, 10000)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64602 (252.35 KB)\n",
      "Trainable params: 64602 (252.35 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tf.compat.v2.enable_v2_behavior()\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from scipy.stats import nbinom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#final Dense layer has 2 units. n must be positive -> softplus activation function\n",
    "# p must be between 0 and 1 -> sigmoid activation function\n",
    "def negative_binomial_layer(x):\n",
    "    \"\"\"\n",
    "    Lambda function for generating negative binomial parameters\n",
    "    n and p from a Dense(2) output.\n",
    "    Assumes tensorflow 2 backend.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    outputs = Dense(2)(final_layer)\n",
    "    distribution_outputs = Lambda(negative_binomial_layer)(outputs)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        output tensor of Dense layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out_tensor : tf.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of dimensions of the input\n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(x, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "        \n",
    "    # Apply a softplus to make positive\n",
    "    n = tf.keras.activations.softplus(n)\n",
    "    \n",
    "    # Apply a sigmoid activation to bound between 0 and 1\n",
    "    p = tf.keras.activations.sigmoid(p)\n",
    "\n",
    "    # Join back together again\n",
    "    out_tensor = tf.concat((n, p), axis=num_dims-1)\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "\n",
    "def nBinom_sample_layer(x):\n",
    "    \"\"\"\n",
    "    Lambda function for generating samples from the\n",
    "    negative binomial parameters n and p from a tensor with dimension 2.\n",
    "    Assumes tensorflow 2 backend.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    outputs = tf.Tensor\n",
    "    emp_sample_ouput = Lambda(nBinom_quantile_layer)(outputs)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        output tf.Tensor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out_tensor : tf.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    # Get the number of dimensions of the input\n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(x, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "\n",
    "    # transform quantiles to tensor\n",
    "    #quantiles = tf.constant(quantiles)\n",
    "\n",
    "    negative_binomial_dist = tfd.NegativeBinomial(total_count=n, probs=p)\n",
    "\n",
    "    # calculate the quantiles\n",
    "    sample = negative_binomial_dist.sample(tf.constant(10000))\n",
    "\n",
    "    # Transpose to get the desired shape (1, 10000)\n",
    "    sample = tf.squeeze(sample, axis=-1)\n",
    "    sample = tf.transpose(sample)\n",
    "\n",
    "\n",
    "    \"\"\" quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "    quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "\n",
    "    # Convert n and p to scalar NumPy arrays\n",
    "    n = n.numpy()\n",
    "    p = p.numpy()\n",
    "\n",
    "\n",
    "    sample = tf.constant(nbinom.ppf(quantiles, n, p))\n",
    "    sample = tf.expand_dims(sample, axis=0) \"\"\"\n",
    "\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def negative_binomial_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Negative binomial loss function.\n",
    "    Assumes tensorflow backend.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf.Tensor\n",
    "        Ground truth values of predicted variable.\n",
    "    y_pred : tf.Tensor\n",
    "        n and p values of predicted distribution.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nll : tf.Tensor\n",
    "        Negative log likelihood.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(y_pred, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "    \n",
    "    # Calculate the negative log likelihood\n",
    "    nll = (\n",
    "        tf.math.lgamma(n) \n",
    "        + tf.math.lgamma(y_true + 1)\n",
    "        - tf.math.lgamma(n + y_true)\n",
    "        - n * tf.math.log(p)\n",
    "        - y_true * tf.math.log(1 - p)\n",
    "    )                  \n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "number_features = 10\n",
    "len_features = 32\n",
    "\n",
    "input_shape = (number_features*len_features,) # Number of used features   10 * 32\n",
    "# z.B.\n",
    "\"\"\" [\n",
    "  [1, 2, 3, ..., 10],   # Datenpunkt 1 mit 10 features\n",
    "  [11, 12, 13, ..., 20],  # Datenpunkt 2 mit 10 features\n",
    "  ...\n",
    "  [311, 312, 313, ..., 320]  # Datenpunkt 32 mit 10 features\n",
    "] \"\"\"\n",
    "\n",
    "\n",
    "# Define inputs with predefined shape\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# print(inputs.shape) -> (None, 10, 32) no Batch size defined (more flexible)\n",
    "\n",
    "hidden_layer1 = Dense(200, activation='relu')(inputs) \n",
    "# Dense Layer: the 10 neurons in the dense layer get their source of input data \n",
    "# from all the other neurons of the previous layer of the network (= fully connected layer)\n",
    "#hidden_layer2 = Dense(8, activation='relu')(hidden_layer1) \n",
    "\n",
    "# Predict the parameters of a negative binomial distribution\n",
    "output_s3 = Dense(2)(hidden_layer1) # neurons for n and p\n",
    "dist_output_s3 = Lambda(negative_binomial_layer)(output_s3) # n and p are transformed, so that they fulfill the constraints\n",
    "\n",
    "sample_output_s3 = Lambda(nBinom_sample_layer)(dist_output_s3) # generate 999 quantiles from the distribution (needed for the crps loss)\n",
    "\n",
    "# Construct model\n",
    "model = Model(inputs=inputs, outputs=sample_output_s3)\n",
    "#model = Model(inputs=inputs, outputs=dist_output_s3)\n",
    "\n",
    "# Compile the model with the desired optimizer, loss function, etc.\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=CRPSLoss())\n",
    "#model.compile(optimizer=Adam(learning_rate=0.001), loss=negative_binomial_loss)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1084, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1229, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1191, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_16/kernel:0' shape=(320, 200) dtype=float32>), (None, <tf.Variable 'dense_16/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_17/kernel:0' shape=(200, 2) dtype=float32>), (None, <tf.Variable 'dense_17/bias:0' shape=(2,) dtype=float32>)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m x_train  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([used_features_norm\u001b[39m.\u001b[39mflatten()])\n\u001b[0;32m      7\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m70.0\u001b[39m])\n\u001b[1;32m----> 9\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train)   \u001b[39m#, batch_size=64, epochs=2\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m\"\"\" test_scores = model.evaluate(x_test, y_test, verbose=2)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mprint(\"Test loss:\", test_scores[0])\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint(\"Test accuracy:\", test_scores[1]) \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_8q3xsrc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1084, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1229, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1191, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"c:\\Users\\Tobias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_16/kernel:0' shape=(320, 200) dtype=float32>), (None, <tf.Variable 'dense_16/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_17/kernel:0' shape=(200, 2) dtype=float32>), (None, <tf.Variable 'dense_17/bias:0' shape=(2,) dtype=float32>)).\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "used_features = feature_data.iloc[:,2:12].tail(32)\n",
    "used_features_norm = preprocessing.normalize(used_features)\n",
    "\n",
    "x_train  = np.array([used_features_norm.flatten()])\n",
    "y_train = np.array([70.0])\n",
    "\n",
    "history = model.fit(x_train, y_train)   #, batch_size=64, epochs=2\n",
    "\n",
    "\"\"\" test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1]) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" tensorflow_probability.python.internal import samplers\\n\\n\\n\\ndef _sample_n(self, n, seed=None):\\n    # Here we use the fact that if:\\n    # lam ~ Gamma(concentration=total_count, rate=(1-probs)/probs)\\n    # then X ~ Poisson(lam) is Negative Binomially distributed.\\n    logits = self._logits_parameter_no_checks()\\n    gamma_seed, poisson_seed = samplers.split_seed(\\n        seed, salt='NegativeBinomial')\\n    # TODO(b/152785714): For some reason switching to gamma_lib.random_gamma\\n    # makes tests time out. Note: observed similar in jax_transformation_test.\\n    rate = samplers.gamma(\\n        shape=[n],\\n        alpha=self.total_count,\\n        beta=tf.math.exp(-logits),\\n        dtype=self.dtype,\\n        seed=gamma_seed)\\n    return samplers.poisson(\\n        shape=[], lam=rate, dtype=self.dtype, seed=poisson_seed) \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" tensorflow_probability.python.internal import samplers\n",
    "\n",
    "\n",
    "\n",
    "def _sample_n(self, n, seed=None):\n",
    "    # Here we use the fact that if:\n",
    "    # lam ~ Gamma(concentration=total_count, rate=(1-probs)/probs)\n",
    "    # then X ~ Poisson(lam) is Negative Binomially distributed.\n",
    "    logits = self._logits_parameter_no_checks()\n",
    "    gamma_seed, poisson_seed = samplers.split_seed(\n",
    "        seed, salt='NegativeBinomial')\n",
    "    # TODO(b/152785714): For some reason switching to gamma_lib.random_gamma\n",
    "    # makes tests time out. Note: observed similar in jax_transformation_test.\n",
    "    rate = samplers.gamma(\n",
    "        shape=[n],\n",
    "        alpha=self.total_count,\n",
    "        beta=tf.math.exp(-logits),\n",
    "        dtype=self.dtype,\n",
    "        seed=gamma_seed)\n",
    "    return samplers.poisson(\n",
    "        shape=[], lam=rate, dtype=self.dtype, seed=poisson_seed) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2.5788898 0.5986877], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.\n",
      "   4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.  6.  6.  6.  6.\n",
      "   6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.\n",
      "   7.  8.  8.  8.  8.  8.  9.  9. 10.]], shape=(1, 999), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Beispielhafter Tensor mit Werten f체r n und p\n",
    "n_values = 2.5  # Beispielwerte f체r n\n",
    "p_values = 0.4  # Beispielwerte f체r p\n",
    "\n",
    "# Zusammenf체hren der Werte in einen Tensor\n",
    "input_tensor = tf.constant([n_values, p_values])\n",
    "\n",
    "# Ausgabe des erstellten Tensors\n",
    "output = negative_binomial_layer(input_tensor)\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "def nBinom_quantile_layer(x):\n",
    "    \"\"\"\n",
    "    Lambda function for generating samples from the\n",
    "    negative binomial parameters n and p from a tensor with dimension 2.\n",
    "    Assumes tensorflow 2 backend.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    outputs = tf.Tensor\n",
    "    emp_sample_ouput = Lambda(nBinom_quantile_layer)(outputs)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        output tf.Tensor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out_tensor : tf.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "    quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "\n",
    "    # Get the number of dimensions of the input\n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(x, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "\n",
    "    # transform quantiles to tensor\n",
    "    #quantiles = tf.constant(quantiles)\n",
    "\n",
    "    \"\"\" negative_binomial_dist = tfd.NegativeBinomial(total_count=n, probs=p)\n",
    "\n",
    "    # calculate the quantiles\n",
    "    sample = negative_binomial_dist.sample(10000)\n",
    "\n",
    "    sample = tf.transpose(sample) \"\"\"\n",
    "\n",
    "    # Convert n and p to scalar NumPy arrays\n",
    "    n = n.numpy()\n",
    "    p = p.numpy()\n",
    "\n",
    "\n",
    "    sample = tf.constant(nbinom.ppf(quantiles, n, p))\n",
    "    sample = tf.expand_dims(sample, axis=0)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample = nBinom_quantile_layer(output)\n",
    "\n",
    "\n",
    "\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([2997.39435461])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps(3000.0,sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v2.enable_v2_behavior()\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "negative_binomial_dist = tfd.NegativeBinomial(total_count=2, probs=0.2)\n",
    "\n",
    "# calculate the quantiles\n",
    "negative_binomial_dist.sample(tf.constant(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
