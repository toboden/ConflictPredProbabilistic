{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set seed so that the model outputs are reproducible\n",
    "After the kernel is restarted the same results are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "seed = 0\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# This will make TensorFlow ops as deterministic as possible, but it will\n",
    "# affect the overall performance, so it's not enabled by default.\n",
    "# `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_Actuals(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# check if the last month of a country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "def check_last_featureMonth(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        raise ValueError('country does not have actuals')\n",
    "\n",
    "\n",
    "    # last month of the feature dataset\n",
    "    last_feature_month = country_feature_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[-1]\n",
    "\n",
    "    # first month of the actual dataset\n",
    "    first_actual_month = country_actual_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[0]\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (first_actual_month - 3) != last_feature_month:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "data = features_df_list[-1]['data']\n",
    "if 'gleditsch_ward' in data.columns:\n",
    "    data = data.drop(columns='gleditsch_ward') # column not necessary\n",
    "\n",
    "## Features without missing values\n",
    "columns_without_missing_values = data.columns[data.notna().all()]\n",
    "\n",
    "for i in range(len(features_df_list)):\n",
    "    data_set = features_df_list[i]['data']\n",
    "    features_df_list[i]['data'] = data_set[columns_without_missing_values]\n",
    "\n",
    "# last dataset contains all other datasets (because of concat) \n",
    "all_features = features_df_list[-1]['data'].columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different feature selection from views\n",
    "# 59 features that map the conflict history of a country\n",
    "conflict_history = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'ged_sb_tlag_1',\n",
    "    'ged_sb_tlag_2', 'ged_sb_tlag_3', 'ged_sb_tlag_4',\n",
    "    'ged_sb_tlag_5', 'ged_sb_tlag_6', 'ged_sb_tsum_24',\n",
    "    'decay_ged_sb_100', 'decay_ged_sb_500', 'decay_ged_os_100',\n",
    "    'decay_ged_ns_5', 'decay_ged_ns_100', 'ged_ns', 'ged_os',\n",
    "    'acled_sb', 'acled_sb_count', 'acled_os',\n",
    "    'ged_os_tlag_1', 'decay_acled_sb_5', 'decay_acled_os_5',\n",
    "    'decay_acled_ns_5', 'splag_1_decay_ged_os_5',\n",
    "    'splag_1_decay_ged_ns_5'\n",
    "]\n",
    "\n",
    "\n",
    "# 59 features that are drawn from the Varieties of Democracy project\n",
    "vdem = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'vdem_v2x_delibdem',\n",
    "    'vdem_v2x_egaldem', 'vdem_v2x_libdem', 'vdem_v2x_libdem_48',\n",
    "    'vdem_v2x_partip', 'vdem_v2x_accountability',\n",
    "    'vdem_v2x_civlib', 'vdem_v2x_clphy', 'vdem_v2x_cspart',\n",
    "    'vdem_v2x_divparctrl', 'vdem_v2x_edcomp_thick', 'vdem_v2x_egal',\n",
    "    'vdem_v2x_execorr', 'vdem_v2x_frassoc_thick', 'vdem_v2x_gencs',\n",
    "    'vdem_v2x_gender', 'vdem_v2x_genpp', 'vdem_v2x_horacc',\n",
    "    'vdem_v2x_neopat', 'vdem_v2x_pubcorr', 'vdem_v2x_rule',\n",
    "    'vdem_v2x_veracc', 'vdem_v2x_freexp', 'vdem_v2xcl_acjst', \n",
    "    'vdem_v2xcl_dmove', 'vdem_v2xcl_prpty', 'vdem_v2xcl_rol', \n",
    "    'vdem_v2xcl_slave', 'vdem_v2xdl_delib', 'vdem_v2xeg_eqdr',\n",
    "    'vdem_v2xeg_eqprotec', 'vdem_v2xel_frefair', 'vdem_v2xel_regelec',\n",
    "    'vdem_v2xme_altinf', 'vdem_v2xnp_client', 'vdem_v2xnp_regcorr',\n",
    "    'vdem_v2xpe_exlecon', 'vdem_v2xpe_exlpol', 'vdem_v2xpe_exlgeo',\n",
    "    'vdem_v2xpe_exlgender', 'vdem_v2xpe_exlsocgr', 'vdem_v2xps_party',\n",
    "    'vdem_v2xcs_ccsi', 'vdem_v2xnp_pres', 'vdem_v2xeg_eqaccess',\n",
    "    'vdem_v2x_diagacc', 'vdem_v2clrgunev', 'splag_vdem_v2x_libdem',\n",
    "    'splag_vdem_v2xcl_dmove', 'splag_vdem_v2x_accountability',\n",
    "    'splag_vdem_v2xpe_exlsocgr', 'splag_vdem_v2xcl_rol', 'wdi_sm_pop_netm',\n",
    "    'wdi_sp_dyn_imrt_in'\n",
    "]\n",
    "\n",
    "# 30 features that are drawn from the WDI as well as some conflict history indicators\n",
    "wdi = [\n",
    "    'ged_sb', 'decay_ged_sb_5', 'decay_ged_os_5',\n",
    "    'splag_1_decay_ged_sb_5', 'wdi_sp_pop_totl', 'wdi_ag_lnd_frst_k2',\n",
    "    'wdi_dt_oda_odat_pc_zs', 'wdi_ms_mil_xpnd_gd_zs', 'wdi_ms_mil_xpnd_zs',\n",
    "    'wdi_nv_agr_totl_kd', 'wdi_nv_agr_totl_kn', 'wdi_ny_gdp_pcap_kd',\n",
    "    'wdi_sp_dyn_le00_in', 'wdi_se_prm_nenr', 'wdi_sh_sta_maln_zs', \n",
    "    'wdi_sh_sta_stnt_zs', 'wdi_sl_tlf_totl_fe_zs', 'wdi_sm_pop_refg_or', \n",
    "    'wdi_sm_pop_netm', 'wdi_sm_pop_totl_zs', 'wdi_sp_dyn_imrt_in', \n",
    "    'wdi_sh_dyn_mort_fe', 'wdi_sp_pop_1564_fe_zs', 'wdi_sp_pop_65up_fe_zs',\n",
    "    'wdi_sp_pop_grow', 'wdi_sp_urb_totl_in_zs',\n",
    "    'splag_wdi_sl_tlf_totl_fe_zs', 'splag_wdi_sm_pop_refg_or',\n",
    "    'splag_wdi_sm_pop_netm', 'splag_wdi_ag_lnd_frst_k2'\n",
    "]\n",
    "\n",
    "ged = ['ged_sb']\n",
    "\n",
    "feature_subset_dict = {'conflict_history':conflict_history,\n",
    "                       'vdem':vdem,\n",
    "                       'wdi':wdi,\n",
    "                       'all':all_features,\n",
    "                       'ged':ged}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group data by country_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of the countries for which a prediction is requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_countrylist = os.path.join('..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data_views = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data_views.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "\n",
    "        # check if actuals exist for the country\n",
    "        if check_Actuals(country_id, 0):\n",
    "            # check if the last feature month is 3 months before the first actuals month\n",
    "            if not check_last_featureMonth(country_id, 0): \n",
    "                month_list.append([str(country_id) +' last month missing'])\n",
    "            else:\n",
    "                month_list.append([number_months_feature_data, country_id])\n",
    "        else:\n",
    "            month_list.append(str(country_id) + ' no actuals')\n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "Goal is to estimate the empirical distribution of the fatalities per month.\n",
    "### Definition of the CRPS loss function and the Feed forward Neural Network subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import nbinom\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# log likelihood loss function\n",
    "def log_l_nbinom(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Negative binomial loss function.\n",
    "    Assumes tensorflow backend.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf.Tensor\n",
    "        Ground truth values of predicted variable.\n",
    "    y_pred : tf.Tensor\n",
    "        n and p values of predicted distribution.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nll : tf.Tensor\n",
    "        Negative log likelihood.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(y_pred, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "    \n",
    "    # Calculate the negative log likelihood\n",
    "    nll = (\n",
    "        tf.math.lgamma(n) \n",
    "        + tf.math.lgamma(y_true + 1)\n",
    "        - tf.math.lgamma(n + y_true)\n",
    "        - n * tf.math.log(p)\n",
    "        - y_true * tf.math.log(1 - p)\n",
    "    )                  \n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "class NegBinLoss(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        return log_l_nbinom(y_true, y_pred)\n",
    "\n",
    "\n",
    "def negative_binomial_layer(x):\n",
    "    \"\"\"\n",
    "    Lambda function for generating negative binomial parameters\n",
    "    n and p from a Dense(2) output.\n",
    "    Assumes tensorflow 2 backend.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    outputs = Dense(2)(final_layer)\n",
    "    distribution_outputs = Lambda(negative_binomial_layer)(outputs)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        output tensor of Dense layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out_tensor : tf.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of dimensions of the input\n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(x, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "        \n",
    "    # Apply a softplus to make positive\n",
    "    n = tf.keras.activations.softplus(n)\n",
    "    \n",
    "    # Apply a sigmoid activation to bound between 0 and 1\n",
    "    p = tf.keras.activations.sigmoid(p)\n",
    "\n",
    "    # Join back together again\n",
    "    out_tensor = tf.concat((n, p), axis=num_dims-1)\n",
    "\n",
    "    #print(out_tensor)\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "# number of neurons per hidden layer (equally spaced)\n",
    "def get_numberNeurons_per_hiddenlayer(numberHiddenLayers, numberNeurons):\n",
    "\n",
    "    neurons_per_HL = np.round(numberNeurons/numberHiddenLayers)\n",
    "\n",
    "    if neurons_per_HL <= 0:\n",
    "        raise ValueError('Number of neurons per hidden layer less than one.')\n",
    "\n",
    "    return neurons_per_HL\n",
    "\n",
    "# Define the Feed Forward Neural Network subclass\n",
    "class FeedForwardNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, name=\"FeedFwdNN_neg_bin\", neurons_output = 2, number_hidden_layers=1, \n",
    "                 number_neurons=10, dropout_rate=None):\n",
    "        super(FeedForwardNN, self).__init__(name=name)\n",
    "\n",
    "        number_neurons_per_hlayer = get_numberNeurons_per_hiddenlayer(number_hidden_layers, number_neurons)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "\n",
    "        self.hidden_layers.append(Dense(number_neurons_per_hlayer, activation='relu'))\n",
    "        if dropout_rate is not None:\n",
    "            self.hidden_layers.append(Dropout(dropout_rate))\n",
    "\n",
    "        if number_hidden_layers > 1:\n",
    "            for _ in range(number_hidden_layers - 1):\n",
    "                self.hidden_layers.append(Dense(number_neurons_per_hlayer, activation='relu'))\n",
    "\n",
    "        self.untransformed_output = Dense(2) #Dense(neurons_output)\n",
    "\n",
    "        self.model = self.build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.untransformed_output(x)\n",
    "        #print(x[0,0])\n",
    "        y = Lambda(negative_binomial_layer)(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the functions to perform the train-/test-split with rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "## conflict trap\n",
    "# drops all months before a starting conflict\n",
    "# defintion of a beginning conflict: fatalities(monthX) > 0 with mean(fatalities(window_size number months starting with monthX)) > threshold\n",
    "# (average fatalities per month in the starting half year are greater than the threshold)\n",
    "# iterates trough the dataset beginning with the first entry\n",
    "def drop_before_conflict_trap(data, threshold, window_size, minimal_data_size):\n",
    "    index_ged_sb = data.columns.get_loc('ged_sb')\n",
    "\n",
    "    start_index = 0\n",
    "    while start_index < len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "        window = data.iloc[start_index:start_index + window_size, index_ged_sb].to_list()\n",
    "        \n",
    "        if window[0] > 0 and sum(window) / window_size >= threshold:\n",
    "            break\n",
    "        else:\n",
    "            start_index += 1\n",
    "\n",
    "\n",
    "    if len(data) >= minimal_data_size:\n",
    "        # if there is no conflict trap do nothing\n",
    "        if start_index == len(data.iloc[:,index_ged_sb]) - window_size + 1:\n",
    "            return data\n",
    "        # if the truncation would result in a too small dataset prevent this\n",
    "        elif len(data.iloc[start_index:, :]) < minimal_data_size:\n",
    "            return data.iloc[-minimal_data_size:, :]\n",
    "        # drop every entry before the conflict trap \n",
    "        else:\n",
    "            return data.iloc[start_index:, :]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "\n",
    "## function used to calculate w_max, number of rolling windows etc.\n",
    "# length of a whole window (containing w input months and 12 acutal months)\n",
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "# number of months available for training (after removing the test months)\n",
    "def number_train_months(numberMonths_available, w):\n",
    "    #  all months feature data   -  test set input\n",
    "    return numberMonths_available - w\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "def find_max_W(numberMonths_available, w_min, w_max):\n",
    "    if number_rolling_windows(numberMonths_available, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "    # find the maximal w\n",
    "    max_W = w_max\n",
    "    number_months_train = number_train_months(numberMonths_available, max_W)\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_months_train, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while number_train_rollwindows_wmax == 0 and max_W > w_min:\n",
    "        max_W -= 1\n",
    "        number_months_train = number_train_months(numberMonths_available, max_W)\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_months_train, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "\n",
    "def month_lists_TrainTest(w_min, w_max, month_list_feature_data):\n",
    "    # numbers of months from the shortened feature dataset\n",
    "    number_months = len(month_list_feature_data)\n",
    "    \n",
    "    # find w_max (as mentioned above, if there are not enoug months, the w_max has to be < w_max)\n",
    "    w_max_local = find_max_W(number_months, w_min, w_max)\n",
    "\n",
    "    w = w_max_local\n",
    "\n",
    "    # length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "    n_train_months = number_train_months(number_months, w)\n",
    "\n",
    "    month_list_train = month_list_feature_data[0:n_train_months]\n",
    "    month_list_test = month_list_feature_data[-w:]\n",
    "\n",
    "    return month_list_train, month_list_test, w\n",
    "\n",
    "\n",
    "def Train_ArrayXY_split(w, month_list, data_feature, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    train_months = len(month_list)\n",
    "\n",
    "    number_rolling_windows_train = number_rolling_windows(train_months, w)\n",
    "\n",
    "    for i in range(0, number_rolling_windows_train):\n",
    "        starting_month_features = month_list[i]\n",
    "\n",
    "        index_ending_month_features = i + w - 1\n",
    "        ending_month_features = month_list[index_ending_month_features]\n",
    "\n",
    "        starting_month_unrActuals = month_list[index_ending_month_features + 3]\n",
    "        ending_month_unrActuals = month_list[index_ending_month_features + 14]\n",
    "\n",
    "        window_features = data_feature.loc[slice(starting_month_features, ending_month_features), :] # excluding \"unreal\" actuals\n",
    "        window_actuals = data_feature.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "        normalized_window_features = preprocessing.normalize(window_features)\n",
    "        window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "        window_actual_array = np.array([window_actuals])\n",
    "\n",
    "        X.append(window_features_array)\n",
    "        Y.append(window_actual_array)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def Test_ArrayXY_split(month_list, data_feature, data_actual, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    starting_month_test = month_list[0]\n",
    "    ending_month_test = month_list[-1]\n",
    "\n",
    "    window_features_test = data_feature.loc[slice(starting_month_test, ending_month_test), :] # all w features to predict the fatalities\n",
    "    window_actuals_test = data_actual.iloc[s - 3].values # real actuals\n",
    "\n",
    "    normalized_window_features_test = preprocessing.normalize(window_features_test)\n",
    "    window_features_array_test = np.array([normalized_window_features_test.flatten()])[0]\n",
    "\n",
    "    window_actual_array_test = window_actuals_test\n",
    "\n",
    "    X.append(window_features_array_test)\n",
    "    Y.append(window_actual_array_test)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the future fatalites\n",
    "### Set the year to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_year = '2021' # 2019, 2020, 2021\n",
    "dataset_index = actual_years.index(prediction_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate country list for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_fatalities_country_list = []\n",
    "countries_with_high_percentage_list = []\n",
    "someNonzero_fatalities_country_list = []\n",
    "\n",
    "for country_id in country_list:\n",
    "    feature_data_all = country_feature_group_list[dataset_index].get_group(country_id)\n",
    "\n",
    "    # Berechnen des Prozentsatzes der Werte größer als 0 in der Spalte 'ged_sb'\n",
    "    positive_percentage = (feature_data_all['ged_sb'] > 0).mean() * 100\n",
    "\n",
    "    if (feature_data_all['ged_sb'] == 0).all():\n",
    "        zero_fatalities_country_list.append(country_id)\n",
    "    elif positive_percentage >= 60:\n",
    "        countries_with_high_percentage_list.append(country_id)\n",
    "    else:\n",
    "        someNonzero_fatalities_country_list.append(country_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numberNeurons_all_layers(inputNeurons, outputNeurons, lam):\n",
    "    upper_bound = max(inputNeurons, outputNeurons)\n",
    "    lower_bound = min(inputNeurons, outputNeurons)\n",
    "\n",
    "    return np.round(lam * (upper_bound - lower_bound) + lower_bound)\n",
    "\n",
    "\n",
    "def evaluate_model(hyperParameters, s, feature_dataset, actuals_dataset):\n",
    "\n",
    "    ## fixed paramters--------\n",
    "    w_min= 1\n",
    "    number_output_neurons = 2\n",
    "    var_threshold = 0.05\n",
    "    mean_fatlities_per_month_threshold = 5\n",
    "    #--------------\n",
    "\n",
    "    ## prepare feature dataset\n",
    "    # only FEATURE SUBSET\n",
    "    feature_data = feature_dataset.loc[:,feature_subset_dict[hyperParameters['featureSubset']]]\n",
    "\n",
    "    ## Drop features with NEAR ZERO VARIANCE (but dont drop 'ged_sb' -> needed for conflict trap detection)\n",
    "    columns_to_keep = [col for col in feature_data.columns if (col == 'ged_sb') or (feature_data[col].var() >= var_threshold)]\n",
    "    feature_data = feature_data[columns_to_keep]\n",
    "\n",
    "    ## remove months before the CONDFLICT TRAP (regime change)\n",
    "    # if the average number of fatalities per month in 6 months is above 'mean_fatlities_per_month_threshold' and the fatalities of the starting month are > 0 \n",
    "    # the conflict trap starts and all obsservations before that month are dropped\n",
    "    # 76 is the minimal length of the dataframe (refers to the minimal size of the data for all countries -> country_id 246 len = 76) \n",
    "    feature_data = drop_before_conflict_trap(feature_data, mean_fatlities_per_month_threshold, 6, 76)\n",
    "    \n",
    "    month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "\n",
    "\n",
    "    ### data split\n",
    "    month_list_train, month_list_test, w = month_lists_TrainTest(w_min, hyperParameters['wmax'], month_list_feature_data)\n",
    "\n",
    "    ## training dataset------\n",
    "    X_train, Y_train = Train_ArrayXY_split(w, month_list_train, feature_data, s)\n",
    "\n",
    "    ## test dataset-------\n",
    "    X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actuals_dataset, s)\n",
    "\n",
    "    ### prediction with the neural net\n",
    "    ## Define inputs with predefined shape\n",
    "    input_shape = (len(X_train[0]),)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # overwrite the old w\n",
    "    hyperParameters['w'] = w\n",
    "\n",
    "    ## define neural net\n",
    "    # Create an instance of the FeedForwardNN model\n",
    "    nameString = 'FFwdNN_neg_bin_s' + str(s)\n",
    "    model = FeedForwardNN(input_shape=inputs.shape, name=nameString, neurons_output = number_output_neurons,\n",
    "                          number_hidden_layers=hyperParameters['numbHiddenL'], \n",
    "                          number_neurons=hyperParameters['numbNeurons'], \n",
    "                          dropout_rate=hyperParameters['dropoutrate'])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=hyperParameters['lr']), loss=NegBinLoss())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train, Y_train, \n",
    "                        batch_size=hyperParameters['batch_size'], epochs=hyperParameters['epochs'],\n",
    "                        verbose=0, shuffle=False)\n",
    "\n",
    "    ## prediction\n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    # calculation of quantiles\n",
    "    quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "    quantiles = np.round(quantiles, 3)\n",
    "\n",
    "    \n",
    "    n,p = prediction[0]\n",
    "\n",
    "    nan_count = False\n",
    "    if math.isnan(n) or math.isnan(p):\n",
    "        nan_count = True\n",
    "\n",
    "    empirical_distribution = nbinom.ppf(quantiles, n, p)\n",
    "\n",
    "    # save the training loss to generate plots\n",
    "    loss = history.history['loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    y_true = Y_test[0][0]\n",
    "    crps_prediction = pscore(empirical_distribution,y_true).compute()[0]\n",
    "\n",
    "    return loss, epochs, y_true, crps_prediction, empirical_distribution, nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list = []\n",
    "\n",
    "### prediction\n",
    "s_prediction_list = list(range(3, 15))\n",
    "number_s = len(s_prediction_list)\n",
    "number_countries = len(country_list)\n",
    "\n",
    "pred_year_string = 'prediction_' + prediction_year\n",
    "\n",
    "# list to save the predictions for each country\n",
    "NNet_prediction_list = [{'country_id': country, pred_year_string: []} for country in country_list]\n",
    "\n",
    "# loop through all countries\n",
    "for country_index in tqdm(range(number_countries)):\n",
    "    country = country_list[country_index]\n",
    "\n",
    "    ## load datasets\n",
    "    features = country_feature_group_list[dataset_index].get_group(country)\n",
    "    actuals = country_actual_group_list[dataset_index].get_group(country)\n",
    "\n",
    "    hyperparams = None\n",
    "\n",
    "    if country in zero_fatalities_country_list:\n",
    "\n",
    "        distribution = np.array([0]*999)\n",
    "\n",
    "        NNet_prediction_list[country_index][pred_year_string].append({'s':[None for _ in range(number_s)],\n",
    "                                                                  'distribution':[distribution for _ in range(number_s)],\n",
    "                                                                  'actual':[None for _ in range(number_s)],\n",
    "                                                                  'CRPS':[None for _ in range(number_s)], \n",
    "                                                                  'loss':[None for _ in range(number_s)], \n",
    "                                                                  'epochs':None,\n",
    "                                                                  'hyperparams':None})\n",
    "    else:\n",
    "\n",
    "        ## fixed hyperparams\n",
    "        hyperparams = {'wmax': 12, 'numbHiddenL': 2, 'relNeurons': 0.0, 'lr': 0.0001, \n",
    "                            'featureSubset': 'conflict_history', 'batch_size': 4, 'dropoutrate': 0.3, \n",
    "                            'epochs': 100, 'numbNeurons': 20, 'w': 12}\n",
    "        \n",
    "        NNet_prediction_list[country_index][pred_year_string].append({'s':[None for _ in range(number_s)],\n",
    "                                                                  'distribution':[None for _ in range(number_s)],\n",
    "                                                                  'actual':[None for _ in range(number_s)],\n",
    "                                                                  'CRPS':[None for _ in range(number_s)], \n",
    "                                                                  'loss':[None for _ in range(number_s)], \n",
    "                                                                  'epochs':None,\n",
    "                                                                  'hyperparams':hyperparams})\n",
    "\n",
    "    for s in s_prediction_list:\n",
    "\n",
    "        if country in zero_fatalities_country_list:\n",
    "\n",
    "            model_y_true = actuals.iloc[s - 3].values[0]\n",
    "            model_crps_prediction = pscore(distribution,model_y_true).compute()[0]\n",
    "\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['s'][s-3] = s\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['actual'][s-3] = model_y_true\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['CRPS'][s-3] = model_crps_prediction\n",
    "\n",
    "        else:\n",
    "            hyper_params = hyperparams\n",
    "            model_loss, model_epochs, model_y_true, model_crps_prediction, distribution, nan_count = evaluate_model(hyper_params, s, features, actuals)\n",
    "\n",
    "            if nan_count is True:\n",
    "                nan_list.append({'country':country, 's':s})\n",
    "                distribution = np.array([0]*999)\n",
    "\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['s'][s-3] = s\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['distribution'][s-3] = distribution\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['actual'][s-3] = model_y_true\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['CRPS'][s-3] = model_crps_prediction\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['loss'][s-3] = model_loss\n",
    "            NNet_prediction_list[country_index][pred_year_string][0]['epochs'] = model_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "joblib_string = 'FinalTask2_NNNegBin_' + str(prediction_year) + '_HyperparamFixed' + '.joblib'\n",
    "\n",
    "dump([NNet_prediction_list, country_list, pred_year_string, seed, zero_fatalities_country_list, nan_list],  joblib_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_values_test = []\n",
    "for country_data in NNet_prediction_list:\n",
    "    if country_data['country_id'] not in zero_fatalities_country_list:\n",
    "        country_crps_list = []\n",
    "        for s in range(0,number_s):\n",
    "            country_crps_list.append(country_data[pred_year_string][0]['CRPS'][s])\n",
    "        crps_values_test.append(np.mean(country_crps_list))\n",
    "\n",
    "mean_crps_test = np.mean(crps_values_test)\n",
    "\n",
    "\n",
    "print('Mean CRPS = ' + str(mean_crps_test) + ' \\\\'+'\\\\')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
