{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_Actuals(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# check if the last month of a country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "def check_last_featureMonth(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        raise ValueError('country does not have actuals')\n",
    "\n",
    "\n",
    "    # last month of the feature dataset\n",
    "    last_feature_month = country_feature_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[-1]\n",
    "\n",
    "    # first month of the actual dataset\n",
    "    first_actual_month = country_actual_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[0]\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (first_actual_month - 3) != last_feature_month:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LÃ¤nder aussortieren\n",
    "die nicht gefordert sind und\n",
    "*  die keine actuals haben\n",
    "*  die zu wenig Beobachtungen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[76, 245],\n",
       " [76, 246],\n",
       " [116, 232],\n",
       " [116, 233],\n",
       " [137, 231],\n",
       " [186, 176],\n",
       " [186, 209],\n",
       " [209, 124],\n",
       " [209, 131],\n",
       " [261, 242],\n",
       " [277, 144],\n",
       " [294, 56],\n",
       " [294, 57],\n",
       " [298, 98],\n",
       " [298, 102],\n",
       " [307, 83],\n",
       " [307, 84],\n",
       " [307, 103],\n",
       " [311, 63],\n",
       " [311, 65],\n",
       " [311, 117],\n",
       " [311, 125],\n",
       " [311, 126],\n",
       " [312, 86],\n",
       " [313, 122],\n",
       " [314, 92],\n",
       " [314, 111],\n",
       " [314, 113],\n",
       " [314, 114],\n",
       " [314, 137],\n",
       " [315, 110],\n",
       " [315, 115],\n",
       " [315, 123],\n",
       " [315, 134],\n",
       " [325, 184],\n",
       " [332, 163],\n",
       " [332, 170],\n",
       " [334, 1],\n",
       " [334, 2],\n",
       " [334, 3],\n",
       " [334, 4],\n",
       " [334, 5],\n",
       " [334, 6],\n",
       " [334, 7],\n",
       " [334, 8],\n",
       " [334, 9],\n",
       " [334, 10],\n",
       " [334, 11],\n",
       " [334, 12],\n",
       " [334, 13],\n",
       " [334, 14],\n",
       " [334, 16],\n",
       " [334, 17],\n",
       " [334, 18],\n",
       " [334, 19],\n",
       " [334, 20],\n",
       " [334, 21],\n",
       " [334, 22],\n",
       " [334, 23],\n",
       " [334, 24],\n",
       " [334, 25],\n",
       " [334, 26],\n",
       " [334, 27],\n",
       " [334, 28],\n",
       " [334, 29],\n",
       " [334, 30],\n",
       " [334, 31],\n",
       " [334, 32],\n",
       " [334, 33],\n",
       " [334, 34],\n",
       " [334, 35],\n",
       " [334, 36],\n",
       " [334, 37],\n",
       " [334, 38],\n",
       " [334, 39],\n",
       " [334, 40],\n",
       " [334, 41],\n",
       " [334, 42],\n",
       " [334, 43],\n",
       " [334, 45],\n",
       " [334, 46],\n",
       " [334, 47],\n",
       " [334, 48],\n",
       " [334, 49],\n",
       " [334, 50],\n",
       " [334, 52],\n",
       " [334, 53],\n",
       " [334, 54],\n",
       " [334, 55],\n",
       " [334, 58],\n",
       " [334, 60],\n",
       " [334, 62],\n",
       " [334, 64],\n",
       " [334, 66],\n",
       " [334, 67],\n",
       " [334, 69],\n",
       " [334, 70],\n",
       " [334, 73],\n",
       " [334, 74],\n",
       " [334, 76],\n",
       " [334, 77],\n",
       " [334, 78],\n",
       " [334, 79],\n",
       " [334, 80],\n",
       " [334, 81],\n",
       " [334, 82],\n",
       " [334, 85],\n",
       " [334, 87],\n",
       " [334, 89],\n",
       " [334, 90],\n",
       " [334, 93],\n",
       " [334, 94],\n",
       " [334, 96],\n",
       " [334, 97],\n",
       " [334, 99],\n",
       " [334, 100],\n",
       " [334, 101],\n",
       " [334, 104],\n",
       " [334, 105],\n",
       " [334, 107],\n",
       " [334, 108],\n",
       " [334, 109],\n",
       " [334, 112],\n",
       " [334, 116],\n",
       " [334, 118],\n",
       " [334, 119],\n",
       " [334, 120],\n",
       " [334, 121],\n",
       " [334, 127],\n",
       " [334, 128],\n",
       " [334, 129],\n",
       " [334, 130],\n",
       " [334, 132],\n",
       " [334, 133],\n",
       " [334, 135],\n",
       " [334, 136],\n",
       " [334, 138],\n",
       " [334, 139],\n",
       " [334, 140],\n",
       " [334, 142],\n",
       " [334, 143],\n",
       " [334, 145],\n",
       " [334, 146],\n",
       " [334, 147],\n",
       " [334, 148],\n",
       " [334, 149],\n",
       " [334, 150],\n",
       " [334, 151],\n",
       " [334, 152],\n",
       " [334, 153],\n",
       " [334, 154],\n",
       " [334, 155],\n",
       " [334, 156],\n",
       " [334, 157],\n",
       " [334, 158],\n",
       " [334, 159],\n",
       " [334, 160],\n",
       " [334, 161],\n",
       " [334, 162],\n",
       " [334, 164],\n",
       " [334, 165],\n",
       " [334, 166],\n",
       " [334, 167],\n",
       " [334, 168],\n",
       " [334, 169],\n",
       " [334, 171],\n",
       " [334, 172],\n",
       " [334, 173],\n",
       " [334, 174],\n",
       " [334, 177],\n",
       " [334, 178],\n",
       " [334, 179],\n",
       " [334, 180],\n",
       " [334, 181],\n",
       " [334, 182],\n",
       " [334, 183],\n",
       " [334, 198],\n",
       " [334, 199],\n",
       " [334, 205],\n",
       " [334, 206],\n",
       " [334, 213],\n",
       " [334, 214],\n",
       " [334, 218],\n",
       " [334, 220],\n",
       " [334, 222],\n",
       " [334, 223],\n",
       " [334, 234],\n",
       " [334, 235],\n",
       " [334, 237],\n",
       " [334, 243],\n",
       " [334, 244]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_path_countrylist = os.path.join('..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "\n",
    "        if check_Actuals(country_id, 0):\n",
    "            if not check_last_featureMonth(country_id, 0): \n",
    "                month_list.append([str(country_id) +' last month missing'])\n",
    "            else:\n",
    "                month_list.append([number_months_feature_data, country_id])\n",
    "        else:\n",
    "            month_list.append(str(country_id) + ' no actuals')\n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()\n",
    "month_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for possible Nan's in all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan's\n",
    "for featurelist in features_df_list:\n",
    "    is_na_series = featurelist['data'].isna().sum()\n",
    "\n",
    "    for i in range(len(is_na_series)):\n",
    "        if is_na_series[i] > 0 :\n",
    "            print(str(is_na_series.index[i]) + ': ' + str(is_na_series[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward Neural Net\n",
    "Goal is to estimate the empirical distribution of the fatalities per month.\n",
    "### Definition of the CRPS loss function and the Feed forward Neural Network subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)\n",
    "\n",
    "# Define custom ReLU activation function\n",
    "class ReLUTransform(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "# Define the Feed Forward Neural Network subclass\n",
    "class FeedForwardNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, name=\"FeedFwdNN\"):\n",
    "        super(FeedForwardNN, self).__init__(name=name)\n",
    "        \n",
    "        self.hidden_layer1 = Dense(4, activation='relu')\n",
    "        #self.hidden_layer2 = Dense(4, activation='relu')\n",
    "        self.untransformed_output = Dense(200)\n",
    "        self.final_output = Lambda(ReLUTransform())\n",
    "\n",
    "        self.model = self.build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.hidden_layer1(inputs)\n",
    "        #x = self.hidden_layer2(x)\n",
    "        x = self.untransformed_output(x)\n",
    "        y = self.final_output(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create the rolling window dataframes per country and window w\n",
    "The dataset for each country are split in train-, validate- and test-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "### function used to calculate w_max, number of rolling windows etc.\n",
    "# length of a whole window (containing w input months and 12 acutal months)\n",
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "def number_valid_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    number_train_valid_months = numberMonths_available - w\n",
    "    number_valid_months = math.floor(number_train_valid_months * relative_validation_size)\n",
    "\n",
    "    return number_valid_months\n",
    "\n",
    "# number of months available for training (after removing the validation and test months)\n",
    "def number_train_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    valid_months = number_valid_months(numberMonths_available, w, relative_validation_size)\n",
    "\n",
    "    #  all months feature data   -  validate set    -   test set input\n",
    "    return numberMonths_available - valid_months - w\n",
    "\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "def find_max_W(numberMonths_available, w_min, w_max, relative_validation_size):\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one validation window\n",
    "    number_valid_months_wmin = number_valid_months(numberMonths_available, w_min, relative_validation_size)\n",
    "    if number_rolling_windows(number_valid_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one validation window with w_min = ' + str(w_min))\n",
    "\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one train window\n",
    "    number_train_months_wmin = number_train_months(numberMonths_available, w_min, relative_validation_size)\n",
    "    if number_rolling_windows(number_train_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "\n",
    "    # find the maximal w\n",
    "    max_W = w_max\n",
    "    number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "    number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while (number_valid_rollwindows_wmax == 0 or number_train_rollwindows_wmax == 0) and max_W > w_min:\n",
    "        max_W -= 1\n",
    "        number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "        number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "\n",
    "def TrainValid_ArrayXY_split(train_months, w, month_list, data_feature):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    number_rolling_windows_train = number_rolling_windows(train_months, w)\n",
    "\n",
    "    for i in range(0, number_rolling_windows_train):\n",
    "        starting_month_features = month_list[i]\n",
    "\n",
    "        index_ending_month_features = i + w - 1\n",
    "        ending_month_features = month_list[index_ending_month_features]\n",
    "\n",
    "        starting_month_unrActuals = month_list[index_ending_month_features + 3]\n",
    "        ending_month_unrActuals = month_list[index_ending_month_features + 14]\n",
    "\n",
    "        window_features = data_feature.loc[slice(starting_month_features, ending_month_features), :] # 'ged_sb':'ged_sb_tlag_4'  ||excluding \"unreal\" actuals\n",
    "        window_actuals = data_feature.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "        normalized_window_features = preprocessing.normalize(window_features)\n",
    "        window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "        window_actual_array = np.array([window_actuals])\n",
    "\n",
    "        X.append(window_features_array)\n",
    "        Y.append(window_actual_array)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def Test_ArrayXY_split(month_list, data_feature, data_actual):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    starting_month_test = month_list[0]\n",
    "    ending_month_test = month_list[-1]\n",
    "\n",
    "    window_features_test = data_feature.loc[slice(starting_month_test, ending_month_test), :] # all w features to predict the fatalities\n",
    "    window_actuals_test = data_actual.iloc[s - 3].values # real actuals\n",
    "\n",
    "    normalized_window_features_test = preprocessing.normalize(window_features_test)\n",
    "    window_features_array_test = np.array([normalized_window_features_test.flatten()])[0]\n",
    "\n",
    "    window_actual_array_test = window_actuals_test\n",
    "\n",
    "    X.append(window_features_array_test)\n",
    "    Y.append(window_actual_array_test)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_prediction_list = list(range(3, 15))\n",
    "\n",
    "number_countries = len(country_list)\n",
    "number_dataframes = len(actual_years)\n",
    "\n",
    "# list to save the predictions for each country\n",
    "NNet_prediction_list = [{'country_id': country, 'prediction': {'2018': [], \n",
    "                                                                   '2019': [], \n",
    "                                                                   '2020': [], \n",
    "                                                                   '2021': []}} for country in country_list]\n",
    "\n",
    "\n",
    "# loop through all countries\n",
    "for index in range(number_countries):\n",
    "    country = country_list[index]\n",
    "\n",
    "    # list to store the predictions for each year temporally\n",
    "    NNet_prediction = [[] for _ in range(number_dataframes)]\n",
    "    \n",
    "    # loop through datasets\n",
    "    for i in range(number_dataframes):\n",
    "\n",
    "\n",
    "        features = country_feature_group_list[i].get_group(country) # features of country in dataset i\n",
    "        actuals = country_actual_group_list[i].get_group(country) # actuals of country in dataset i\n",
    "\n",
    "        data_year = actual_years[i]\n",
    "\n",
    "        # loop over countries\n",
    "        for index in range(number_countries):\n",
    "            country = country_list[index]\n",
    "\n",
    "            # loop over prediction horizons s\n",
    "            for s in s_prediction_list:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # loop over w's\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                NNet_prediction_list[j][index]['prediction'][data_year].append({'s':s, 'w':w,\n",
    "                                                                                    'fatalities':fit['fatalities'],\n",
    "                                                                                    'actual':true_obs,\n",
    "                                                                                    'CRPS':crps})\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction task for year 2018\n",
    "prediction_year = '2018'\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "s = 14 # month to predict element out of [3,14]\n",
    "\n",
    "rel_validation_size = 0.3 # percentual size of the validation set\n",
    "\n",
    "# the maximal w (months to estimate the fatalities from) is set to e.g. 3 years (36 months)\n",
    "w_max = 36 \n",
    "# BUT: If w_max=36 leads to a number of rolling windows < 1 in the validation dataset,\n",
    "# w_max is set to the maximal w, so that the number of rolling windows is >= 1\n",
    "# => this step is done below in the section for each country\n",
    "\n",
    "# to calculate the w_max the w_min has to be set as well\n",
    "w_min = 2\n",
    "\n",
    "\n",
    "## country 223\n",
    "prediction_country_id = 245 #220\n",
    "\n",
    "# check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "if not check_last_featureMonth(prediction_country_id, dataset_index):\n",
    "    raise ValueError('last month is not contained in the data')\n",
    "\n",
    "## load datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(prediction_country_id)\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(prediction_country_id)\n",
    "\n",
    "## drop the column 'gleditsch_ward'\n",
    "feature_data = feature_data.drop(columns='gleditsch_ward')\n",
    "\n",
    "\n",
    "# numbers of months from the feature dataset\n",
    "month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "first_month = min(month_list_feature_data)\n",
    "last_month = max(month_list_feature_data)\n",
    "number_months_feature_data = len(month_list_feature_data) # number of months in the feature dataset\n",
    "\n",
    "# find w_max (as mentioned above, if there are not enoug months, the w_max has to be < 36)\n",
    "w_max = find_max_W(number_months_feature_data, w_min, w_max, rel_validation_size)\n",
    "w_list = list(range(w_min, w_max+1))\n",
    "\n",
    "w = w_list[-1] ###################\n",
    "\n",
    "### split data in train-, validation- and test-dataset\n",
    "\"\"\" The data sizes are calculated in a way, that missing months are no problem. Additionaly due to the fact, that the data is sorted\n",
    "    regarding months, this step can be skipped. \"\"\"\n",
    "\n",
    "# length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "roll_window_len = rollingWindowLength(w)\n",
    "n_train_months = number_train_months(number_months_feature_data, w, rel_validation_size)\n",
    "n_valid_months = number_valid_months(number_months_feature_data, w, rel_validation_size)\n",
    "n_test_months = w\n",
    "\n",
    "month_list_train = month_list_feature_data[0:n_train_months]\n",
    "month_list_valid = month_list_feature_data[n_train_months:(n_train_months+n_valid_months)]\n",
    "month_list_test = month_list_feature_data[number_months_feature_data-n_test_months:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## training dataset------\n",
    "X_train, Y_train = TrainValid_ArrayXY_split(n_train_months, w, month_list_train, feature_data)\n",
    "\n",
    "## validation dataset--------\n",
    "X_validate, Y_validate = TrainValid_ArrayXY_split(n_valid_months, w, month_list_valid, feature_data)\n",
    "\n",
    "## test dataset-------\n",
    "X_test, Y_test = Test_ArrayXY_split(month_list_test, feature_data, actual_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
