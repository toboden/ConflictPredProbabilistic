{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan's\n",
    "for featurelist in features_df_list:\n",
    "    is_na_series = featurelist['data'].isna().sum()\n",
    "\n",
    "    for i in range(len(is_na_series)):\n",
    "        if is_na_series[i] > 0 :\n",
    "            print(str(is_na_series.index[i]) + ': ' + str(is_na_series[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# crps loss function \n",
    "def crps(y_true, S):\n",
    "    \"\"\"\n",
    "    Computes continuous ranked probability score:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf tensor of shape (BATCH_SIZE, 1)\n",
    "        True values.\n",
    "    S : tf tensor of shape (BATCH_SIZE, N_SAMPLES)\n",
    "        Predictive samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf tensor of shape (BATCH_SIZE,)\n",
    "        Scores.\n",
    "\n",
    "    \"\"\"\n",
    "    beta=1\n",
    "    n_samples = S.shape[-1]\n",
    "    def expected_dist(diff, beta):\n",
    "        return K.sum(K.pow(K.sqrt(K.square(diff)+K.epsilon()), beta),axis=-1) #axis = -1: last dimension <=> N_SAMPLES\n",
    "    es_1 = expected_dist(y_true - S, beta)\n",
    "    es_2 = 0\n",
    "    for i in range(n_samples):\n",
    "        es_2 = es_2 + expected_dist(K.expand_dims(S[:,i]) - S, beta)\n",
    "    return es_1/n_samples - es_2/(2*n_samples**2)\n",
    "\n",
    "class CRPSLoss(Loss):\n",
    "    def call(self, y_true, S):\n",
    "        return crps(y_true, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction task for country 223 and actual year 2018\n",
    "prediction_year = '2018'\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "prediction_country_id = 223\n",
    "\n",
    "## load datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(prediction_country_id)\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(prediction_country_id)\n",
    "\n",
    "# numbers of months from the feature dataset\n",
    "month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "first_month = month_list_feature_data[0]\n",
    "last_month = month_list_feature_data[-1]\n",
    "\n",
    "\n",
    "\n",
    "## hyperparameters\n",
    "w_max = 48 # maximum number of months used for one training run\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## split data in train-, validation- and test-dataset\n",
    "# length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "roll_estim_window_len = w_max + 2 + 12 \n",
    "\n",
    "# training dataset\n",
    "last_month_train = last_month - w_max - 12-2-w_max \n",
    "data_train = feature_data.loc[(slice(first_month, last_month_train), slice(None)), :] # including \"unreal\" actuals\n",
    "\n",
    "# validation dataset\n",
    "last_month_valid = last_month - w_max\n",
    "data_validate = feature_data.loc[(slice(last_month_train+1, last_month_valid), slice(None)), :] # including \"unreal\" actuals\n",
    "\n",
    "# test dataset\n",
    "data_test = feature_data.loc[(slice(last_month_valid+1, last_month), slice(None)), :] # no \"unreal\" actuals and real actuals not included as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                6420      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 200)               4200      \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 200)               0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Total params: 10620 (41.48 KB)\n",
      "Trainable params: 10620 (41.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tf.compat.v2.enable_v2_behavior()\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from scipy.stats import nbinom\n",
    "\n",
    "# Funktion fÃ¼r die ReLU-Transformation\n",
    "def relu_transform(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "number_features = 10\n",
    "len_features = 32\n",
    "\n",
    "input_shape = (number_features*len_features,) # Number of used features   10 * 32\n",
    "# z.B.\n",
    "\"\"\" [\n",
    "  [1, 2, 3, ..., 10],   # Datenpunkt 1 mit 10 features\n",
    "  [11, 12, 13, ..., 20],  # Datenpunkt 2 mit 10 features\n",
    "  ...\n",
    "  [311, 312, 313, ..., 320]  # Datenpunkt 32 mit 10 features\n",
    "] \"\"\"\n",
    "\n",
    "\n",
    "# Define inputs with predefined shape\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# print(inputs.shape) -> (None, 10, 32) no Batch size defined (more flexible)\n",
    "\n",
    "hidden_layer1 = Dense(20, activation='relu')(inputs) \n",
    "# Dense Layer: the 10 neurons in the dense layer get their source of input data \n",
    "# from all the other neurons of the previous layer of the network (= fully connected layer)\n",
    "#hidden_layer2 = Dense(8, activation='relu')(hidden_layer1) \n",
    "\n",
    "# Predict the parameters of a negative binomial distribution\n",
    "output_s3 = Dense(200)(hidden_layer1) # neurons for n and p\n",
    "sample_output_s3 = Lambda(relu_transform)(output_s3) # n and p are transformed, so that they fulfill the constraints\n",
    "\n",
    "# Construct model\n",
    "model = Model(inputs=inputs, outputs=sample_output_s3)\n",
    "\n",
    "# Compile the model with the desired optimizer, loss function, etc.\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=CRPSLoss())\n",
    "#model.compile(optimizer=Adam(learning_rate=0.001), loss=negative_binomial_loss)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 51s 51s/step - loss: 69.8993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' test_scores = model.evaluate(x_test, y_test, verbose=2)\\nprint(\"Test loss:\", test_scores[0])\\nprint(\"Test accuracy:\", test_scores[1]) '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "used_features = feature_data.iloc[:,2:12].tail(32)\n",
    "used_features_norm = preprocessing.normalize(used_features)\n",
    "\n",
    "x_train  = np.array([used_features_norm.flatten()])\n",
    "y_train = np.array([70.0])\n",
    "\n",
    "history = model.fit(x_train, y_train)   #, batch_size=64, epochs=2\n",
    "\n",
    "\"\"\" test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
