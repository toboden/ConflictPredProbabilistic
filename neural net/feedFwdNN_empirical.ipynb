{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_Actuals(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# check if the last month of a country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "def check_last_featureMonth(country_id, dataindex):\n",
    "    # Check if the country_id exists in actual dataset\n",
    "    if country_id not in country_actual_group_list[dataindex].groups.keys():\n",
    "        raise ValueError('country does not have actuals')\n",
    "\n",
    "\n",
    "    # last month of the feature dataset\n",
    "    last_feature_month = country_feature_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[-1]\n",
    "\n",
    "    # first month of the actual dataset\n",
    "    first_actual_month = country_actual_group_list[dataindex].get_group(country_id).index.get_level_values('month_id').unique().tolist()[0]\n",
    "\n",
    "    # if the last month of the feature dataset in the country does not match the first of the actuals return false\n",
    "    if (first_actual_month - 3) != last_feature_month:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# create the feature- and actuals-data list\n",
    "# set the feature and actuals year lists\n",
    "feature_years = ['2017','2018','2019','2020']\n",
    "actual_years = ['2018','2019','2020','2021']\n",
    "\n",
    "actuals_df_list = []\n",
    "features_df_list = []\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for i in range(len(feature_years)):\n",
    "    # relative paths to the parquet files\n",
    "    relative_path_features = os.path.join('..', 'data', 'cm_features_to_oct' + feature_years[i] + '.parquet')\n",
    "    relative_path_actuals = os.path.join('..', 'data', 'cm_actuals_' + actual_years[i] + '.parquet')\n",
    "\n",
    "    path_features = os.path.join(current_dir, relative_path_features)\n",
    "    path_actuals = os.path.join(current_dir, relative_path_actuals)\n",
    "\n",
    "    # append datasets to the lists\n",
    "    actuals_df_list.append({'year':actual_years[i], 'data':pd.read_parquet(path_actuals, engine='pyarrow')})\n",
    "    features_df_list.append({'year':feature_years[i], 'data':pd.read_parquet(path_features, engine='pyarrow')})\n",
    "\n",
    "# concat the feature datasets, so that every data contains the observations starting with january 1990\n",
    "for i in range(1,len(features_df_list)):\n",
    "    features_df_list[i]['data'] = pd.concat([features_df_list[i-1]['data'], features_df_list[i]['data']])\n",
    "\n",
    "country_list = sorted(features_df_list[3]['data'].index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all four datasets\n",
    "country_feature_group_list = []\n",
    "country_actual_group_list = []\n",
    "# fill list \n",
    "for i in range(len(features_df_list)):\n",
    "    country_feature_group_list.append(features_df_list[i]['data'].groupby('country_id'))\n",
    "    country_actual_group_list.append(actuals_df_list[i]['data'].groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LÃ¤nder aussortieren\n",
    "die nicht gefordert sind und\n",
    "*  die keine actuals haben\n",
    "*  die zu wenig Beobachtungen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_countrylist = os.path.join('..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "\n",
    "        if check_Actuals(country_id, 0):\n",
    "            if not check_last_featureMonth(country_id, 0): \n",
    "                month_list.append([str(country_id) +' last month missing'])\n",
    "            else:\n",
    "                month_list.append([number_months_feature_data, country_id])\n",
    "        else:\n",
    "            month_list.append(str(country_id) + ' no actuals')\n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()\n",
    "month_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for possible Nan's in all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan's\n",
    "for featurelist in features_df_list:\n",
    "    is_na_series = featurelist['data'].isna().sum()\n",
    "\n",
    "    for i in range(len(is_na_series)):\n",
    "        if is_na_series[i] > 0 :\n",
    "            print(str(is_na_series.index[i]) + ': ' + str(is_na_series[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "### function used to calculate w_max, number of rolling windows etc.\n",
    "# length of a whole window (containing w input months and 12 acutal months)\n",
    "def rollingWindowLength(w):\n",
    "    return w + 2 + 12\n",
    "\n",
    "def number_valid_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    number_train_valid_months = numberMonths_available - w\n",
    "    number_valid_months = math.floor(number_train_valid_months * relative_validation_size)\n",
    "\n",
    "    return number_valid_months\n",
    "\n",
    "# number of months available for training (after removing the validation and test months)\n",
    "def number_train_months(numberMonths_available, w, relative_validation_size):\n",
    "\n",
    "    valid_months = number_valid_months(numberMonths_available, w, relative_validation_size)\n",
    "\n",
    "    #  all months feature data   -  validate set    -   test set input\n",
    "    return numberMonths_available - valid_months - w\n",
    "\n",
    "\n",
    "def number_rolling_windows(numberMonths_available, w):\n",
    "    return max(0,numberMonths_available - rollingWindowLength(w) + 1)\n",
    "\n",
    "\n",
    "\n",
    "def find_max_W(numberMonths_available, w_min, w_max, relative_validation_size):\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one validation window\n",
    "    number_valid_months_wmin = number_valid_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_valid_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one validation window with w_min = ' + str(w_min))\n",
    "\n",
    "    # with \"w_min\" and the \"relative_validation_size\" there has to be at least one train window\n",
    "    number_train_months_wmin = number_train_months(numberMonths_available, w_min, relative_validation_size)\n",
    "\n",
    "    if number_rolling_windows(number_train_months_wmin, w_min) == 0:\n",
    "        raise ValueError('not enough months for one training window with w_min = ' + str(w_min))\n",
    "\n",
    "\n",
    "    max_W = w_max\n",
    "    number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "    number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "    number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    # calculate w_max so that the number of rolling windows for the validation set is >= 1\n",
    "    # and that\n",
    "    # the number of rolling windows for the train set is >= 1\n",
    "    while number_valid_rollwindows_wmax == 0 or number_train_rollwindows_wmax == 0:\n",
    "        max_W -= 1\n",
    "        number_valid_months_wmax = number_valid_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_valid_rollwindows_wmax = number_rolling_windows(number_valid_months_wmax, max_W)\n",
    "\n",
    "        number_train_months_wmax = number_train_months(numberMonths_available, max_W, relative_validation_size)\n",
    "        number_train_rollwindows_wmax = number_rolling_windows(number_train_months_wmax, max_W)\n",
    "\n",
    "    return max_W\n",
    "\n",
    "### prediction task for year 2018\n",
    "prediction_year = '2018'\n",
    "dataset_index = actual_years.index(prediction_year)\n",
    "s = 14 # month to predict element out of [3,14]\n",
    "\n",
    "rel_validation_size = 0.3 # percentual size of the validation set\n",
    "\n",
    "# the maximal w (months to estimate the fatalities from) is set to e.g. 3 years (36 months)\n",
    "w_max = 36 \n",
    "# BUT: If w_max=36 leads to a number of rolling windows < 1 in the validation dataset,\n",
    "# w_max is set to the maximal w, so that the number of rolling windows is >= 1\n",
    "# => this step is done below in the section for each country\n",
    "\n",
    "# to calculate the w_max the w_min has to be set as well\n",
    "w_min = 2\n",
    "\n",
    "\n",
    "## country 223\n",
    "prediction_country_id = 245 #220\n",
    "\n",
    "# check if the last month of the country in the feature dataset is 3 months before the first month that has to be predicted\n",
    "if not check_last_featureMonth(prediction_country_id, dataset_index):\n",
    "    raise ValueError('last month is not contained in the data')\n",
    "\n",
    "## load datasets\n",
    "feature_data = country_feature_group_list[dataset_index].get_group(prediction_country_id)\n",
    "actual_data = country_actual_group_list[dataset_index].get_group(prediction_country_id)\n",
    "\n",
    "feature_data = feature_data.drop(columns='gleditsch_ward')\n",
    "\n",
    "\n",
    "# numbers of months from the feature dataset\n",
    "month_list_feature_data = feature_data.index.get_level_values('month_id').tolist()\n",
    "first_month = min(month_list_feature_data)\n",
    "last_month = max(month_list_feature_data)\n",
    "number_months_feature_data = len(month_list_feature_data) # number of months in the feature dataset\n",
    "\n",
    "# find w_max (as mentioned above, if there are not enoug months, the w_max has to be < 36)\n",
    "w_max = find_max_W(number_months_feature_data, w_min, w_max, rel_validation_size)\n",
    "\n",
    "w_list = range(w_min, w_max+1)\n",
    "\n",
    "### split data in train-, validation- and test-dataset\n",
    "\"\"\" The data sizes are calculated in a way, that missing months are no problem. Additionaly due to the fact, that the data is sorted\n",
    "    regarding months, this step can be skipped. \"\"\"\n",
    "\n",
    "# length of the maximum rolling window and the used \"unreal\" acutals starting 3 months after the last used month\n",
    "roll_window_len = rollingWindowLength(w)\n",
    "n_train_months = number_train_months(number_months_feature_data, w, rel_validation_size)\n",
    "n_valid_months = number_valid_months(number_months_feature_data, w, rel_validation_size)\n",
    "n_test_months = w\n",
    "\n",
    "month_list_train = month_list_feature_data[0:n_train_months]\n",
    "month_list_valid = month_list_feature_data[n_train_months:(n_train_months+n_valid_months)]\n",
    "month_list_test = month_list_feature_data[number_months_feature_data-n_test_months:]\n",
    "\n",
    "## training dataset------\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "number_rolling_windows_train = number_rolling_windows(n_train_months, w)\n",
    "\n",
    "for i in range(0, number_rolling_windows_train):\n",
    "    starting_month_features = month_list_train[i]\n",
    "\n",
    "    index_ending_month_features = i + w - 1\n",
    "    ending_month_features = month_list_train[index_ending_month_features]\n",
    "\n",
    "    starting_month_unrActuals = month_list_train[index_ending_month_features + 3]\n",
    "    ending_month_unrActuals = month_list_train[index_ending_month_features + 14]\n",
    "\n",
    "    window_features = feature_data.loc[slice(starting_month_features, ending_month_features), :] # 'ged_sb':'ged_sb_tlag_4'  ||excluding \"unreal\" actuals\n",
    "    window_actuals = feature_data.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "    normalized_window_features = preprocessing.normalize(window_features)\n",
    "    window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "    window_actual_array = np.array([window_actuals])\n",
    "\n",
    "    X_train.append(window_features_array)\n",
    "    Y_train.append(window_actual_array)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "\n",
    "\n",
    "## validation dataset--------\n",
    "X_validate = []\n",
    "Y_validate = []\n",
    "\n",
    "number_rolling_windows_validate = number_rolling_windows(n_valid_months, w)\n",
    "\n",
    "for i in range(0, number_rolling_windows_validate):\n",
    "    starting_month_features = month_list_valid[i]\n",
    "\n",
    "    index_ending_month_features = i + w - 1\n",
    "    ending_month_features = month_list_valid[index_ending_month_features]\n",
    "\n",
    "    starting_month_unrActuals = month_list_valid[index_ending_month_features + 3]\n",
    "    ending_month_unrActuals = month_list_valid[index_ending_month_features + 14]\n",
    "\n",
    "    window_features = feature_data.loc[slice(starting_month_features, ending_month_features), :] # excluding \"unreal\" actuals\n",
    "    window_actuals = feature_data.loc[slice(starting_month_unrActuals, ending_month_unrActuals), 'ged_sb'].iloc[s - 3] # \"unreal\" actuals\n",
    "\n",
    "\n",
    "    normalized_window_features = preprocessing.normalize(window_features)\n",
    "    window_features_array = np.array([normalized_window_features.flatten()])[0]\n",
    "\n",
    "    window_actual_array = np.array([window_actuals])\n",
    "\n",
    "    X_validate.append(window_features_array)\n",
    "    Y_validate.append(window_actual_array)\n",
    "\n",
    "X_validate = np.array(X_validate)\n",
    "Y_validate = np.array(Y_validate)\n",
    "\n",
    "## test dataset-------\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "starting_month_test = month_list_test[0]\n",
    "ending_month_test = month_list_test[-1]\n",
    "\n",
    "window_features_test = feature_data.loc[slice(starting_month_test, ending_month_test), :] # all w features to predict the fatalities\n",
    "window_actuals_test = actual_data.iloc[s - 3].values # real actuals\n",
    "\n",
    "normalized_window_features_test = preprocessing.normalize(window_features_test)\n",
    "window_features_array_test = np.array([normalized_window_features_test.flatten()])[0]\n",
    "\n",
    "window_actual_array_test = window_actuals_test\n",
    "\n",
    "X_test.append(window_features_array_test)\n",
    "Y_test.append(window_actual_array_test)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "\n",
    "print('(first, last) train month: (' + str(month_list_train[0])+','+str(month_list_train[-1])+')')\n",
    "print('(first, last) validate month: (' + str(month_list_valid[0])+','+str(month_list_valid[-1])+')')\n",
    "print('(first, last) test month: (' + str(month_list_test[0])+','+str(month_list_test[-1])+')')\n",
    "print('train + valid + test = ' + str(n_train_months + n_valid_months +n_test_months) + ' != ' + str(number_months_feature_data))\n",
    "\n",
    "\n",
    "print('')\n",
    "print('# rolling w train: ' + str(number_rolling_windows_train) + '      # rolling w validate: ' + str(number_rolling_windows_validate))\n",
    "print('w_max = ' + str(w_max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
